{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid navigation for different input representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "colab = False\n",
    "\n",
    "if colab:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    !git clone https://github.com/deepmind/pycolab.git\n",
    "    !git clone https://github.com/nicoladainese96/RelationalDeepRL.git\n",
    "    !pip install pycolab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "if colab: \n",
    "    import sys\n",
    "    sys.path.insert(0, 'RelationalDeepRL')\n",
    "\n",
    "from RelationalModule import ActorCritic, ControlActorCritic, CoordActorCritic, OheActorCritic\n",
    "from Utils import train_agent_sandbox as train\n",
    "from Utils import test_env\n",
    "from Utils import utils\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'Utils.test_env' from '/home/nicola/Nicola_unipd/MasterThesis/RelationalDeepRL/Utils/test_env.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(test_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(losses, x='', y='', t='', xlog=False, ylog=False):\n",
    "    \"\"\"Generic plotting function\"\"\"\n",
    "    episodes = np.arange(len(losses)) + 1\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.plot(episodes, losses)\n",
    "    plt.xlabel(x, fontsize=16)\n",
    "    plt.ylabel(y, fontsize=16)\n",
    "    plt.title(t, fontsize=16)\n",
    "    if xlog:\n",
    "        plt.xscale('log')\n",
    "    if ylog:\n",
    "        plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coordinate State Representation\n",
    "\n",
    "Simplest representation to learn, already providing high-level features.\n",
    "\n",
    "state = (x_agent, y_agent, x_goal, y_goal, near_borders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable parameters\n",
    "X = 10\n",
    "Y = 10\n",
    "initial = [0,0]\n",
    "goal = [6,7]\n",
    "\n",
    "# All game parameters\n",
    "game_params = dict(x=X, y=Y, initial=initial, goal=goal, max_steps=100, \n",
    "                   greyscale_state=False, return_coord=True, R0=-0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HPs = dict(observation_space = 5, lr=0.0005, gamma=0.9, TD=True, \n",
    "                 twin=True, tau=0.1, n_steps=40, H=1e-1, hiddens=[64,32,16])\n",
    "\n",
    "if colab:\n",
    "    HPs['device'] = 'cuda'\n",
    "else:\n",
    "    HPs['device'] = 'cpu'\n",
    "    \n",
    "# Relational Agent\n",
    "agent = CoordActorCritic.CoordA2C(action_space = 4, **HPs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# see if everything works before starting the real training cycle\n",
    "env = test_env.Sandbox(**game_params)\n",
    "rewards, log_probs, distributions, states, done, bootstrap = train.play_episode(agent, env, max_steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"rewards: \", rewards.shape)\n",
    "print(\"log_probs: \", len(log_probs))\n",
    "print(\"distributions: \", len(distributions))\n",
    "print(\"states: \", states.shape)\n",
    "print(\"done: \", done.shape)\n",
    "print(\"bootstrap: \", bootstrap.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"rewards: \\n\", rewards)\n",
    "print(\"log_probs: \\n\", log_probs)\n",
    "print(\"distributions: \\n\", distributions)\n",
    "print(\"states: \\n\", states)\n",
    "print(\"done: \\n\", done)\n",
    "print(\"bootstrap: \\n\", bootstrap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "agent.update(rewards, log_probs, distributions, states, done, bootstrap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training cycle\n",
    "\n",
    "Reward: $R_0$ for each step except the one that ends the episode going to the goal. No penalty agains walls.\n",
    "\n",
    "Now one should use a random agent and an optimal one to see what is the window of rewards better than random and less or equal to the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_STEPS = 100\n",
    "game_params = dict(x=X, y=Y, initial=[0,0], goal=[9,9], max_steps=MAX_STEPS, \n",
    "                   greyscale_state=False, return_coord=True, R0=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "results = train.train_sandbox(agent, game_params, n_episodes = 5000,\n",
    "                              max_steps=MAX_STEPS, return_agent=True, random_init=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "score, asymptotic_score, asymptotic_std, trained_agent, time_profile, losses, steps_to_solve = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "n_epochs = np.arange(100, len(score))\n",
    "average_score = np.array([np.mean(score[i:i+100]) for i in range(len(score)-100)])\n",
    "plt.plot(n_epochs, average_score, alpha=0.9)\n",
    "plt.title(\"Performance\", fontsize=16)\n",
    "plt.xlabel(\"Number of epochs\", fontsize=16)\n",
    "plt.ylabel(\"Total reward\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "n_epochs = np.arange(100, len(score))\n",
    "average_steps = np.array([np.mean(steps_to_solve[i:i+100]) for i in range(len(score)-100)])\n",
    "plt.plot(n_epochs, average_steps, alpha=0.9)\n",
    "plt.title(\"Performance\", fontsize=16)\n",
    "plt.xlabel(\"Number of epochs\", fontsize=16)\n",
    "plt.ylabel(\"Steps to solve\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "n_epochs = np.arange(len(score))\n",
    "plt.plot(n_epochs, losses['critic_losses'], alpha=0.9, label='critic loss')\n",
    "plt.title(\"Performance\", fontsize=16)\n",
    "plt.xlabel(\"Number of epochs\", fontsize=16)\n",
    "plt.ylabel(\"Loss\", fontsize=16)\n",
    "plt.legend(fontsize=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "n_epochs = np.arange(len(score))\n",
    "plt.plot(n_epochs, losses['actor_losses'], alpha=0.9, label='actor loss')\n",
    "plt.plot(n_epochs, np.array(losses['entropies']), alpha=0.9, label='entropy term')\n",
    "plt.title(\"Performance\", fontsize=16)\n",
    "plt.xlabel(\"Number of epochs\", fontsize=16)\n",
    "plt.ylabel(\"Total reward\", fontsize=16)\n",
    "plt.legend(fontsize=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "n_epochs = np.arange(len(score))\n",
    "#plt.plot(n_epochs, losses['actor_losses'], alpha=0.9, label='actor loss')\n",
    "plt.plot(n_epochs, -np.array(losses['entropies']), alpha=0.9, label='entropy term')\n",
    "plt.title(\"Entropy\", fontsize=16)\n",
    "plt.xlabel(\"Number of epochs\", fontsize=16)\n",
    "plt.ylabel(\"Entropy loss\", fontsize=16)\n",
    "plt.legend(fontsize=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to read and interpret:** Entropy is higher for policies that are highly stochastic and zero for those that are deterministic. The spikes toward smaller entropies are probably due to strong feedbacks (either positive or negative ones). If we have |A| possible actions, the maximum value of the entropy is:\n",
    "$$S = \\sum_{a=1}^4 - \\frac{1}{|A|}log\\left( \\frac{1}{|A|} \\right) = - log\\left( \\frac{1}{|A|} \\right) = +log(|A|) \\approx 1.4 $$\n",
    "Given that the actor updates have order of the hundred, we should use a coupling constant much higher that the one currently used $(10^{-2})$. The other problem is that scaling the reward would leave the entropy unchanged, thus if the actor loss changes of many orders of magnitude during training, we must be aware that entropy regularization might become a real problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = test_env.Sandbox(**game_params)\n",
    "R = 0\n",
    "test_episodes = 100\n",
    "max_steps = 30\n",
    "tot_steps = 0\n",
    "\n",
    "for i in range(test_episodes):\n",
    "    rewards, log_probs, distributions, states, done, bootstrap = train.play_episode(trained_agent, env, max_steps=max_steps)\n",
    "    #print(\"Solved in %d steps\"%len(rewards))\n",
    "    R += rewards.sum()\n",
    "    tot_steps += len(rewards)\n",
    "print(\"Average reward: \", R/test_episodes)\n",
    "print(\"Steps to solve: \", tot_steps/test_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "%matplotlib notebook\n",
    "\n",
    "# Variable parameters\n",
    "X = 10\n",
    "Y = 10\n",
    "initial = [0,9]\n",
    "goal = [5,4]\n",
    "\n",
    "# All game parameters\n",
    "#game_params = dict(x=X, y=Y, initial=initial, goal=goal, max_steps=100, greyscale_state=True)\n",
    "game_params = dict(x=X, y=Y, initial=initial, goal=goal, max_steps=100, \n",
    "                   greyscale_state=False, return_coord=True)\n",
    "\n",
    "env = test_env.Sandbox(**game_params)\n",
    "#utils.render(trained_agent, env, save=False)\n",
    "render(trained_agent, env, save=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OneHotEncoded State Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable parameters\n",
    "X = 10\n",
    "Y = 10\n",
    "initial = [0,0]\n",
    "goal = [6,7]\n",
    "MAX_STEPS = 100\n",
    "\n",
    "game_params = dict(x=X, y=Y, initial=initial, goal=goal, max_steps=MAX_STEPS, \n",
    "                   greyscale_state=True, return_ohe=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HPs = dict(lr=0.0005, gamma=0.9, TD=True, twin=True, tau=0.1, n_steps=40, H=1e-1)\n",
    "\n",
    "if colab:\n",
    "    HPs['device'] = 'cuda'\n",
    "else:\n",
    "    HPs['device'] = 'cpu'\n",
    "    \n",
    "# Relational Agent\n",
    "agent = OheActorCritic.OheA2C(action_space = 4, map_size=X, **HPs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# see if everything works before starting the real training cycle\n",
    "env = test_env.Sandbox(**game_params)\n",
    "rewards, log_probs, distributions, states, done, bootstrap = train.play_episode(agent, env, max_steps=MAX_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"rewards: \", rewards.shape)\n",
    "print(\"log_probs: \", len(log_probs))\n",
    "print(\"distributions: \", len(distributions))\n",
    "print(\"states: \", states.shape)\n",
    "print(\"done: \", done.shape)\n",
    "print(\"bootstrap: \", bootstrap.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "results = train.train_sandbox(agent, game_params, n_episodes = 5000,\n",
    "                              max_steps=MAX_STEPS, return_agent=True, random_init=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score, asymptotic_score, asymptotic_std, trained_agent, time_profile, losses, steps_to_solve = results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reloading partially trained agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dir = \"Results/Sandbox/\"\n",
    "queries = ['ohe', 'sandbox']\n",
    "train_dict = utils.load_session(load_dir, queries)\n",
    "\n",
    "game_params = train_dict['game_params']\n",
    "HPs = train_dict['HPs']\n",
    "score = train_dict['score']\n",
    "keywords = train_dict['keywords']\n",
    "ID = keywords[-1]\n",
    "\n",
    "# Load agent \n",
    "if colab:\n",
    "    trained_agent = torch.load(load_dir+\"agent_\"+ID)\n",
    "else:\n",
    "    trained_agent = torch.load(load_dir+\"agent_\"+ID, map_location=torch.device('cpu'))\n",
    "    trained_agent.device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "results = train.train_sandbox(agent, game_params, n_episodes = 5000,\n",
    "                              max_steps=MAX_STEPS, return_agent=True, random_init=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score, asymptotic_score, asymptotic_std, trained_agent, time_profile, losses, steps_to_solve = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "%matplotlib notebook\n",
    "\n",
    "# Variable parameters\n",
    "\n",
    "X = 10\n",
    "Y = 10\n",
    "initial = [0,0]\n",
    "goal = [9,9]\n",
    "MAX_STEPS = 100\n",
    "\n",
    "game_params = dict(x=X, y=Y, initial=initial, goal=goal, max_steps=MAX_STEPS, \n",
    "                   greyscale_state=True, return_ohe=True)\n",
    "\n",
    "env = test_env.Sandbox(**game_params)\n",
    "utils.render(trained_agent, env, save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save = True\n",
    "keywords = ['control', '10by10',str(len(score))+\"-episodes\",\"200-steps\",\"NQBK-pt2\"] # example\n",
    "\n",
    "if colab and save:\n",
    "    %cd ~\n",
    "    parent_dir = \"/content/gdrive/My Drive/Colab Notebooks/\"\n",
    "    save_dir  = \"RelationalTrained/\"\n",
    "    %cd \"{parent_dir}\"\n",
    "    !mkdir \"{save_dir}\"\n",
    "    ID = utils.save_session(save_dir, keywords, game_params, HPs, score)\n",
    "    torch.save(trained_agent, save_dir+\"agent_\"+ID)\n",
    "elif save:\n",
    "    save_dir = 'Results/Sandbox/'\n",
    "    ID = utils.save_session(save_dir, keywords, game_params, HPs, score)\n",
    "    torch.save(trained_agent, save_dir+\"agent_\"+ID)\n",
    "else:\n",
    "    print(\"Nothing saved\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relational agent with OHE state representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable parameters\n",
    "X = 10\n",
    "Y = 10\n",
    "initial = [0,0]\n",
    "goal = [6,7]\n",
    "MAX_STEPS = 200\n",
    "\n",
    "game_params = dict(x=X, y=Y, initial=initial, goal=goal, max_steps=MAX_STEPS, \n",
    "                   greyscale_state=True, return_ohe=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "HPs = dict(lr=0.0005, gamma=0.9, TD=True, twin=True, tau=0.1, n_steps=40, H=1e-1)\n",
    "\n",
    "if colab:\n",
    "    HPs['device'] = 'cuda'\n",
    "else:\n",
    "    HPs['device'] = 'cpu'\n",
    "    \n",
    "# Relational Agent\n",
    "agent = ActorCritic.BoxWorldA2C(action_space = 4, **HPs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# see if everything works before starting the real training cycle\n",
    "env = test_env.Sandbox(**game_params)\n",
    "rewards, log_probs, distributions, states, done, bootstrap = train.play_episode(agent, env, max_steps=MAX_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards:  (200,)\n",
      "log_probs:  200\n",
      "distributions:  200\n",
      "states:  (201, 3, 12, 12)\n",
      "done:  (200,)\n",
      "bootstrap:  (200,)\n"
     ]
    }
   ],
   "source": [
    "print(\"rewards: \", rewards.shape)\n",
    "print(\"log_probs: \", len(log_probs))\n",
    "print(\"distributions: \", len(distributions))\n",
    "print(\"states: \", states.shape)\n",
    "print(\"done: \", done.shape)\n",
    "print(\"bootstrap: \", bootstrap.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.12117810547351837, -0.11549694091081619, -0.026653388515114784)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.update(rewards, log_probs, distributions, states, done, bootstrap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "results = train.train_sandbox(agent, game_params, n_episodes = 15000,\n",
    "                              max_steps=MAX_STEPS, return_agent=True, random_init=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score, asymptotic_score, asymptotic_std, trained_agent, time_profile, losses, steps_to_solve = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "n_epochs = np.arange(100, len(score))\n",
    "average_steps = np.array([np.mean(steps_to_solve[i:i+100]) for i in range(len(score)-100)])\n",
    "plt.plot(n_epochs, average_steps, alpha=0.9)\n",
    "plt.title(\"Performance\", fontsize=16)\n",
    "plt.xlabel(\"Number of epochs\", fontsize=16)\n",
    "plt.ylabel(\"Steps to solve\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save = True\n",
    "keywords = ['relational','ohe', '10by10',str(len(score))+\"-episodes\",\"200-steps\"] # example\n",
    "\n",
    "if colab and save:\n",
    "    %cd ~\n",
    "    parent_dir = \"/content/gdrive/My Drive/Colab Notebooks/\"\n",
    "    save_dir  = \"RelationalTrained/\"\n",
    "    %cd \"{parent_dir}\"\n",
    "    !mkdir \"{save_dir}\"\n",
    "    ID = utils.save_session(save_dir, keywords, game_params, HPs, score, steps_to_solve)\n",
    "    torch.save(trained_agent, save_dir+\"agent_\"+ID)\n",
    "elif save:\n",
    "    save_dir = 'Results/Sandbox/'\n",
    "    ID = utils.save_session(save_dir, keywords, game_params, HPs, score, steps_to_solve)\n",
    "    torch.save(trained_agent, save_dir+\"agent_\"+ID)\n",
    "else:\n",
    "    print(\"Nothing saved\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddied State Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable parameters\n",
    "X = 10\n",
    "Y = 10\n",
    "initial = [0,0]\n",
    "goal = [6,7]\n",
    "\n",
    "game_params = dict(x=X, y=Y, initial=initial, goal=goal, max_steps=100, greyscale_state=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scripted critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "from torch.distributions import Categorical\n",
    "\n",
    "from RelationalModule.AC_networks import ControlCritic #custom module\n",
    "\n",
    "debug = False \n",
    "\n",
    "class CriticAgentTD():\n",
    "    \"\"\"\n",
    "    Advantage Actor-Critic RL agent for BoxWorld environment described in the paper\n",
    "    Relational Deep Reinforcement Learning.\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    * Always uses 2 separate networks for the critic,one that learns from new experience \n",
    "      (student/critic) and the other one (critic_target/teacher)that is more conservative \n",
    "      and whose weights are updated through an exponential moving average of the weights \n",
    "      of the critic, i.e.\n",
    "          target.params = (1-tau)*target.params + tau* critic.params\n",
    "    * In the case of Monte Carlo estimation the critic_target is never used\n",
    "    * Possible to use twin networks for the critic and the critic target for improved \n",
    "      stability. Critic target is used for updates of both the actor and the critic and\n",
    "      its output is the minimum between the predictions of its two internal networks.\n",
    "      \n",
    "    \"\"\" \n",
    "    \n",
    "    def __init__(self, lr, gamma, TD=True, twin=False, tau = 1., \n",
    "                 H=1e-2, n_steps = 1, device='cpu', **box_net_args):\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.TD = TD\n",
    "        self.twin = twin \n",
    "        self.tau = tau\n",
    "        self.H = H\n",
    "        self.n_steps = n_steps\n",
    "        \n",
    "        self.critic = ControlCritic(twin, **box_net_args)\n",
    "        \n",
    "        if self.TD:\n",
    "            self.critic_trg = ControlCritic(twin, target=True, **box_net_args)\n",
    "\n",
    "            # Init critic target identical to critic\n",
    "            for trg_params, params in zip(self.critic_trg.parameters(), self.critic.parameters()):\n",
    "                trg_params.data.copy_(params.data)\n",
    "            \n",
    "        self.critic_optim = torch.optim.Adam(self.critic.parameters(), lr=lr)\n",
    "        \n",
    "        self.device = device \n",
    "        self.critic.to(self.device)\n",
    "        if self.TD:\n",
    "            self.critic_trg.to(self.device)\n",
    "        \n",
    "        if debug:\n",
    "            print(\"=\"*10 +\" A2C HyperParameters \"+\"=\"*10)\n",
    "            print(\"Discount factor: \", self.gamma)\n",
    "            print(\"Learning rate: \", self.lr)\n",
    "            print(\"Temporal Difference learning: \", self.TD)\n",
    "            print(\"Twin networks: \", self.twin)\n",
    "            print(\"Update critic target factor: \", self.tau)\n",
    "            if self.TD:\n",
    "                print(\"n_steps for TD: \", self.n_steps)\n",
    "            print(\"Device used: \", self.device)\n",
    "            print(\"\\n\\n\"+\"=\"*10 +\" A2C Architecture \"+\"=\"*10)\n",
    "            print(\"Critic architecture: \\n\",self.critic)\n",
    "            print(\"Critic target architecture: \")\n",
    "            if self.TD:\n",
    "                print(self.critic_trg)\n",
    "            else:\n",
    "                print(\"Not used\")\n",
    "    \n",
    "    def update_TD(self, rewards, states, done, bootstrap=None):   \n",
    "        \n",
    "        ### Compute n-steps rewards, states, discount factors and done mask ###\n",
    "        \n",
    "        n_step_rewards = self.compute_n_step_rewards(rewards)\n",
    "        if debug:\n",
    "            print(\"n_step_rewards.shape: \", n_step_rewards.shape)\n",
    "            print(\"rewards.shape: \", rewards.shape)\n",
    "            print(\"n_step_rewards: \", n_step_rewards)\n",
    "            print(\"rewards: \", rewards)\n",
    "            print(\"bootstrap: \", bootstrap)\n",
    "                \n",
    "        if bootstrap is not None:\n",
    "            done[bootstrap] = False \n",
    "        if debug:\n",
    "            print(\"done.shape: (before n_steps)\", done.shape)\n",
    "            print(\"done: (before n_steps)\", done)\n",
    "        \n",
    "        old_states = torch.tensor(states[:-1].astype(int)).to(self.device)\n",
    "\n",
    "        new_states, Gamma_V, done = self.compute_n_step_states(states, done)\n",
    "        new_states = torch.tensor(new_states.astype(int)).to(self.device)\n",
    "\n",
    "        if debug:\n",
    "            print(\"done.shape: (after n_steps)\", done.shape)\n",
    "            print(\"Gamma_V.shape: \", Gamma_V.shape)\n",
    "            print(\"done: (after n_steps)\", done)\n",
    "            print(\"Gamma_V: \", Gamma_V)\n",
    "            print(\"old_states.shape: \", old_states.shape)\n",
    "            print(\"new_states.shape: \", new_states.shape)\n",
    "            \n",
    "        ### Wrap variables into tensors ###\n",
    "        \n",
    "        done = torch.LongTensor(done.astype(int)).to(self.device)\n",
    "    \n",
    "        n_step_rewards = torch.tensor(n_step_rewards).float().to(self.device)\n",
    "        Gamma_V = torch.tensor(Gamma_V).float().to(self.device)\n",
    "        \n",
    "        ### Update critic and then actor ###\n",
    "        critic_loss = self.update_critic_TD(n_step_rewards, new_states, old_states, done, Gamma_V)\n",
    "        #actor_loss, entropy = self.update_actor_TD(n_step_rewards, log_probs, distributions, new_states, old_states, done, Gamma_V)\n",
    "        \n",
    "        return critic_loss#, actor_loss, entropy\n",
    "    \n",
    "    def update_critic_TD(self, n_step_rewards, new_states, old_states, done, Gamma_V):\n",
    "        \n",
    "        # Compute loss \n",
    "        if debug: print(\"Updating critic...\")\n",
    "        with torch.no_grad():\n",
    "            V_trg = self.critic_trg(new_states).squeeze()\n",
    "            if debug:\n",
    "                print(\"V_trg.shape (after critic): \", V_trg.shape)\n",
    "            V_trg = (1-done)*Gamma_V*V_trg + n_step_rewards\n",
    "            if debug:\n",
    "                print(\"V_trg.shape (after sum): \", V_trg.shape)\n",
    "            V_trg = V_trg.squeeze()\n",
    "            if debug:\n",
    "                print(\"V_trg.shape (after squeeze): \", V_trg.shape)\n",
    "                print(\"V_trg.shape (after squeeze): \", V_trg)\n",
    "            \n",
    "        if self.twin:\n",
    "            V1, V2 = self.critic(old_states)\n",
    "            if debug:\n",
    "                print(\"V1.shape: \", V1.squeeze().shape)\n",
    "                print(\"V1: \", V1)\n",
    "            loss1 = 0.5*F.mse_loss(V1.squeeze(), V_trg)\n",
    "            loss2 = 0.5*F.mse_loss(V2.squeeze(), V_trg)\n",
    "            loss = loss1 + loss2\n",
    "        else:\n",
    "            V = self.critic(old_states).squeeze()\n",
    "            if debug: \n",
    "                print(\"V.shape: \",  V.shape)\n",
    "                print(\"V: \",  V)\n",
    "            loss = F.mse_loss(V, V_trg)\n",
    "        \n",
    "        # Backpropagate and update\n",
    "        \n",
    "        self.critic_optim.zero_grad()\n",
    "        loss.backward()\n",
    "        self.critic_optim.step()\n",
    "        \n",
    "        # Update critic_target: (1-tau)*old + tau*new\n",
    "        \n",
    "        for trg_params, params in zip(self.critic_trg.parameters(), self.critic.parameters()):\n",
    "                trg_params.data.copy_((1.-self.tau)*trg_params.data + self.tau*params.data)\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    def compute_n_step_rewards(self, rewards):\n",
    "        \"\"\"\n",
    "        Computes n-steps discounted reward padding with zeros the last elements of the trajectory.\n",
    "        This means that the rewards considered are AT MOST n, but can be less for the last n-1 elements.\n",
    "        \"\"\"\n",
    "        T = len(rewards)\n",
    "        \n",
    "        # concatenate n_steps zeros to the rewards -> they do not change the cumsum\n",
    "        r = np.concatenate((rewards,[0 for _ in range(self.n_steps)])) \n",
    "        \n",
    "        Gamma = np.array([self.gamma**i for i in range(r.shape[0])])\n",
    "        \n",
    "        # reverse everything to use cumsum in right order, then reverse again\n",
    "        Gt = np.cumsum(r[::-1]*Gamma[::-1])[::-1]\n",
    "        \n",
    "        G_nstep = Gt[:T] - Gt[self.n_steps:] # compute n-steps discounted return\n",
    "        \n",
    "        Gamma = Gamma[:T]\n",
    "        \n",
    "        assert len(G_nstep) == T, \"Something went wrong computing n-steps reward\"\n",
    "        \n",
    "        n_steps_r = G_nstep / Gamma\n",
    "        \n",
    "        return n_steps_r\n",
    "    \n",
    "    def compute_n_step_states(self, states, done):\n",
    "        \"\"\"\n",
    "        Computes n-steps target states (to be used by the critic as target values together with the\n",
    "        n-steps discounted reward). For last n-1 elements the target state is the last one available.\n",
    "        Adjusts also the `done` mask used for disabling the bootstrapping in the case of terminal states\n",
    "        and returns Gamma_V, that are the discount factors for the target state-values, since they are \n",
    "        n-steps away (except for the last n-1 states, whose discount is adjusted accordingly).\n",
    "        \n",
    "        Return\n",
    "        ------\n",
    "        new_states, Gamma_V, done: arrays with first dimension = len(states)-1\n",
    "        \"\"\"\n",
    "        \n",
    "        # Compute indexes for (at most) n-step away states \n",
    "        \n",
    "        n_step_idx = np.arange(len(states)-1) + self.n_steps\n",
    "        diff = n_step_idx - len(states) + 1\n",
    "        mask = (diff > 0)\n",
    "        n_step_idx[mask] = len(states) - 1\n",
    "        \n",
    "        # Compute new states\n",
    "        \n",
    "        new_states = states[n_step_idx]\n",
    "        \n",
    "        # Compute discount factors\n",
    "        \n",
    "        pw = np.array([self.n_steps for _ in range(len(new_states))])\n",
    "        pw[mask] = self.n_steps - diff[mask]\n",
    "        Gamma_V = self.gamma**pw\n",
    "        \n",
    "        # Adjust done mask\n",
    "        \n",
    "        mask = (diff >= 0)\n",
    "        done[mask] = done[-1]\n",
    "        \n",
    "        return new_states, Gamma_V, done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_optimal(env):\n",
    "    state = env.reset(random_init = True)\n",
    "    \n",
    "    rewards = []\n",
    "    done = []\n",
    "    states = [state]\n",
    "    bootstrap = []\n",
    "    \n",
    "    while True:\n",
    "        action = env.get_optimal_action()\n",
    "        \n",
    "        new_state, reward, terminal, info = env.step(action) \n",
    "        states.append(new_state)\n",
    "        rewards.append(reward)\n",
    "        done.append(terminal)\n",
    "        \n",
    "        if terminal is True and 'TimeLimit.truncated' in info:\n",
    "            bootstrap.append(True)\n",
    "        else:\n",
    "            bootstrap.append(False) \n",
    "            \n",
    "        if terminal:\n",
    "            break\n",
    "            \n",
    "        state = new_state\n",
    "    \n",
    "    return np.array(rewards), np.array(states), np.array(done), np.array(bootstrap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(test_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_critic_TD(HPs, n_episodes = 100):\n",
    "    \n",
    "    # Create environment\n",
    "    env = test_env.Sandbox(10, 10, [0,0], [9,9], max_steps=50)\n",
    "    \n",
    "    critic = CriticAgentTD(**HPs)\n",
    "    \n",
    "    losses = []\n",
    "    for e in range(n_episodes):\n",
    "        rewards, states, done, bootstrap = play_optimal(env)\n",
    "        loss = critic.update_TD(rewards, states, done, bootstrap)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        if (e+1)%10 == 0:\n",
    "            print(\"Episode %d - loss: %.4f\"%(e+1, np.mean(losses[-10:])))\n",
    "\n",
    "    return critic, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HPs = dict(lr=0.0005, gamma=0.99, TD=True, twin=True, tau=0.01, n_steps=1,\n",
    "               H=1., vocab_size = 4, n_dim=3, n_features=16, n_heads=4, linear_size=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "critic, losses = train_critic_TD(HPs, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = \"Number of episodes\"\n",
    "y = \"Squared error of the prediction [log]\"\n",
    "t = \"Critic trained through optimal policy and TD\"\n",
    "plot_loss(losses, x, y, t, ylog=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "MAP_SIZE = 10 + 2 # because of the border\n",
    "goal = (9+1,9+1) # because of the border\n",
    "state = np.full([MAP_SIZE,MAP_SIZE], 0)\n",
    "values = np.full([MAP_SIZE,MAP_SIZE], 0).astype(float)\n",
    "state[goal] = 2\n",
    "\n",
    "for s in range(MAP_SIZE**2):\n",
    "    x, y = s//MAP_SIZE, s%MAP_SIZE\n",
    "    new_state = copy.deepcopy(state)\n",
    "    new_state[x,y] = 1\n",
    "    new_state = torch.LongTensor(new_state).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        V1, V2 = critic.critic(new_state)\n",
    "        V = torch.min(V1,V2)\n",
    "    values[x,y] = V.squeeze().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_real_value_map(env, gamma):\n",
    "    distances = np.zeros((env.boundary[0], env.boundary[1] ))\n",
    "    for x in range(env.boundary[0]):\n",
    "        for y in range(env.boundary[1]):\n",
    "            d = env.dist_to_goal([x,y])\n",
    "            distances[x,y] = d\n",
    "    distances = distances.flatten() -1\n",
    "    real_values = gamma**distances\n",
    "    real_values[-1] = 1\n",
    "    return real_values.reshape(env.boundary[0], env.boundary[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_map(critic, env):\n",
    "    MAP_SIZE = env.boundary[0] \n",
    "    goal = (env.goal[0]+1, env.goal[1]+1)\n",
    "    gamma = critic.gamma\n",
    "    \n",
    "    state = np.full([MAP_SIZE+2,MAP_SIZE+2], 0)\n",
    "    values = np.full([MAP_SIZE,MAP_SIZE], 0).astype(float)\n",
    "    state[goal] = 2\n",
    "    # adding borders\n",
    "    state[0,:] = 3\n",
    "    state[-1,:] = 3\n",
    "    state[:,0] = 3\n",
    "    state[:,-1] = 3\n",
    "    \n",
    "    for s in range(MAP_SIZE**2):\n",
    "        x, y = s//MAP_SIZE, s%MAP_SIZE\n",
    "        new_state = copy.deepcopy(state)\n",
    "        new_state[x+1,y+1] = 1\n",
    "        new_state = torch.LongTensor(new_state).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            V1, V2 = critic.critic(new_state)\n",
    "            V = torch.min(V1,V2)\n",
    "        values[x,y] = V.squeeze().numpy()\n",
    "    \n",
    "    plt.figure(figsize=(15,6))\n",
    "\n",
    "    plt.subplot(121)\n",
    "    values[-1,-1] = 1\n",
    "    plt.imshow(values)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.title(\"Value map\", fontsize=16)\n",
    "    plt.colorbar()\n",
    "    \n",
    "    real_values = get_real_value_map(env, gamma)\n",
    "    plt.subplot(122)\n",
    "    plt.imshow(real_values)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.title(\"Real values\", fontsize=16)\n",
    "    plt.colorbar()\n",
    "    \n",
    "    plt.figure(figsize=(7,6))\n",
    "    diff = values-real_values\n",
    "    diff[-1,-1] = 0\n",
    "    plt.imshow(diff)\n",
    "    #plt.imshow(real_values)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.title(\"Difference from real value\", fontsize=16)\n",
    "    plt.colorbar()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = test_env.Sandbox(10, 10, [0,0], [9,9], max_steps=50)\n",
    "value_map(critic, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scripted actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "from torch.distributions import Categorical\n",
    "\n",
    "from RelationalModule.AC_networks import ControlActor\n",
    "\n",
    "debug = False\n",
    "\n",
    "class ActorAgentTD():\n",
    "    \"\"\"\n",
    "    Advantage Actor-Critic RL agent for BoxWorld environment described in the paper\n",
    "    Relational Deep Reinforcement Learning.\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    * Always uses 2 separate networks for the critic,one that learns from new experience \n",
    "      (student/critic) and the other one (critic_target/teacher)that is more conservative \n",
    "      and whose weights are updated through an exponential moving average of the weights \n",
    "      of the critic, i.e.\n",
    "          target.params = (1-tau)*target.params + tau* critic.params\n",
    "    * In the case of Monte Carlo estimation the critic_target is never used\n",
    "    * Possible to use twin networks for the critic and the critic target for improved \n",
    "      stability. Critic target is used for updates of both the actor and the critic and\n",
    "      its output is the minimum between the predictions of its two internal networks.\n",
    "      \n",
    "    \"\"\" \n",
    "    \n",
    "    def __init__(self, action_space, lr, gamma, TD=True, twin=False, tau = 1., \n",
    "                 H=1e-2, n_steps = 1, device='cpu', **box_net_args):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        action_space: int\n",
    "            Number of (discrete) possible actions to take\n",
    "        lr: float in [0,1]\n",
    "            Learning rate\n",
    "        gamma: float in [0,1]\n",
    "            Discount factor\n",
    "        TD: bool (default=True)\n",
    "            If True, uses Temporal Difference for the critic's estimates\n",
    "            Otherwise uses Monte Carlo estimation\n",
    "        twin: bool (default=False)\n",
    "            Enables twin networks both for critic and critic_target\n",
    "        tau: float in [0,1] (default = 1.)\n",
    "            Regulates how fast the critic_target gets updates, i.e. what percentage of the weights\n",
    "            inherits from the critic. If tau=1., critic and critic_target are identical \n",
    "            at every step, if tau=0. critic_target is unchangable. \n",
    "            As a default this feature is disabled setting tau = 1, but if one wants to use it a good\n",
    "            empirical value is 0.005.\n",
    "        H: float (default 1e-2)\n",
    "            Entropy multiplicative factor in actor's loss\n",
    "        n_steps: int (default=1)\n",
    "            Number of steps considered in TD update\n",
    "        device: str in {'cpu','cuda'}\n",
    "            Select if training agent with cpu or gpu. \n",
    "            FIXME: At the moment is gpu is present, it MUST use the gpu.\n",
    "        **box_net_args: dict (optional)\n",
    "            Dictionary of {'key':value} pairs valid for BoxWorldNet.\n",
    "            Valid keys:\n",
    "                in_channels: int (default 3)\n",
    "                    Number of channels of the input image (e.g. 3 for RGB)\n",
    "                n_kernels: int (default 24)\n",
    "                    Number of features extracted for each pixel\n",
    "                vocab_size: int (default 116)\n",
    "                    Range of integer values of the raw pixels\n",
    "                n_dim: int (default 3)\n",
    "                    Embedding dimension for each pixel channel (1 channel for greyscale, \n",
    "                    3 for RGB)\n",
    "                n_features: int (default 256)\n",
    "                    Number of linearly projected features after positional encoding.\n",
    "                    This is the number of features used during the Multi-Headed Attention\n",
    "                    (MHA) blocks\n",
    "                n_heads: int (default 4)\n",
    "                    Number of heades in each MHA block\n",
    "                n_attn_modules: int (default 2)\n",
    "                    Number of MHA blocks\n",
    "                n_linears: int (default 4)\n",
    "                    Number of fully-connected layers after the FeaturewiseMaxPool layer\n",
    "        \"\"\"\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.n_actions = action_space\n",
    "        self.TD = TD\n",
    "        self.twin = twin \n",
    "        self.tau = tau\n",
    "        self.H = H\n",
    "        self.n_steps = n_steps\n",
    "        \n",
    "        self.actor = ControlActor(action_space, **box_net_args)\n",
    "       \n",
    "\n",
    "        self.actor_optim = torch.optim.Adam(self.actor.parameters(), lr=lr)\n",
    "            \n",
    "     \n",
    "        self.device = device \n",
    "        self.actor.to(self.device) \n",
    "        \n",
    "    def get_action(self, state, return_log=False):\n",
    "        log_probs = self.forward(state)\n",
    "        dist = torch.exp(log_probs)\n",
    "        probs = Categorical(dist)\n",
    "        action =  probs.sample().item()\n",
    "        if return_log:\n",
    "            return action, log_probs.view(-1)[action], dist\n",
    "        else:\n",
    "            return action\n",
    "    \n",
    "    def forward(self, state):\n",
    "        \"\"\"\n",
    "        Makes a tensor out of a numpy array state and then forward\n",
    "        it with the actor network.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        state: array of int\n",
    "            Shape (episode_len, in_channels, lin_size, lin_size)\n",
    "            Or    (in_channels, lin_size, lin_size)\n",
    "        \"\"\"\n",
    "        state = torch.from_numpy(state.astype(int)).to(self.device)\n",
    "        log_probs = self.actor(state)\n",
    "        return log_probs\n",
    "    \n",
    "    def critic(self, state, env):\n",
    "        \"\"\"Implements optimal critic given the sandbox environment.\"\"\"\n",
    "        AGENT_COLOR = 1\n",
    "        distances = []\n",
    "        for s in state:\n",
    "            #print(\"s: \", s)\n",
    "            shape = s.shape\n",
    "            #print(\"shape: \", shape)\n",
    "            s = s.numpy().reshape(shape[1], shape[2])\n",
    "            x, y = np.where(s==AGENT_COLOR)\n",
    "            #print(\"x and y: \", x, y)\n",
    "            s_decoded = [x, y]\n",
    "            #print(\"s_decoded: \", s_decoded)\n",
    "            #print(\"\\n\\tState %d decoded in (%d, %d)\"%(s, s_decoded[0], s_decoded[1]))\n",
    "            d = env.dist_to_goal(s_decoded)\n",
    "            #print(\"\\tGoal in (%d,%d)\"%(env.goal[0], env.goal[1]))\n",
    "            #print(\"\\tDistance from goal: \", d)\n",
    "            distances.append(d)\n",
    "        distances = np.array(distances) -1\n",
    "        #print(\"distances.shape: \", distances.shape)\n",
    "        #print(\"Distances: \", distances)\n",
    "        #print(\"gamma*dist: \", (self.gamma**distances).shape)\n",
    "        values = torch.tensor(self.gamma**distances).float()\n",
    "        \n",
    "        #print(\"Values: \", values,'\\n')\n",
    "        return values\n",
    "    \n",
    "    def update_TD(self, env, rewards, log_probs, distributions, states, done, bootstrap=None):   \n",
    "        \n",
    "        ### Compute n-steps rewards, states, discount factors and done mask ###\n",
    "        \n",
    "        n_step_rewards = self.compute_n_step_rewards(rewards)\n",
    "        if debug:\n",
    "            print(\"n_step_rewards.shape: \", n_step_rewards.shape)\n",
    "            print(\"rewards.shape: \", rewards.shape)\n",
    "            print(\"n_step_rewards: \", n_step_rewards)\n",
    "            print(\"rewards: \", rewards)\n",
    "            #print(\"bootstrap: \", bootstrap)\n",
    "                \n",
    "        if bootstrap is not None:\n",
    "            done[bootstrap] = False \n",
    "        #if debug:\n",
    "            #print(\"done.shape: (before n_steps)\", done.shape)\n",
    "            #print(\"done: (before n_steps)\", done)\n",
    "        \n",
    "        old_states = torch.tensor(states[:-1].astype(int)).to(self.device)\n",
    "\n",
    "        new_states, Gamma_V, done = self.compute_n_step_states(states, done)\n",
    "        new_states = torch.tensor(new_states.astype(int)).to(self.device)\n",
    "\n",
    "        if debug:\n",
    "            print(\"done.shape: (after n_steps)\", done.shape)\n",
    "            print(\"Gamma_V.shape: \", Gamma_V.shape)\n",
    "            print(\"done: (after n_steps)\", done)\n",
    "            print(\"Gamma_V: \", Gamma_V)\n",
    "            print(\"old_states.shape: \", old_states.shape)\n",
    "            print(\"new_states.shape: \", new_states.shape)\n",
    "            \n",
    "        ### Wrap variables into tensors ###\n",
    "        \n",
    "        done = torch.LongTensor(done.astype(int)).to(self.device)\n",
    "        #if debug: print(\"log_probs: \", log_probs)\n",
    "        log_probs = torch.stack(log_probs).to(self.device)\n",
    "        #if debug: print(\"log_probs: \", log_probs)\n",
    "        distributions = torch.stack(distributions, axis=1).to(self.device)\n",
    "        # Regularize terms with 0 probability (if that happens)\n",
    "        mask = (distributions == 0).nonzero()\n",
    "        distributions[:,mask] = 1e-5\n",
    "        \n",
    "        n_step_rewards = torch.tensor(n_step_rewards).float().to(self.device)\n",
    "        Gamma_V = torch.tensor(Gamma_V).float().to(self.device)\n",
    "        \n",
    "        ### Update critic and then actor ###\n",
    "        #critic_loss = self.update_critic_TD(n_step_rewards, new_states, old_states, done, Gamma_V)\n",
    "        actor_loss = self.update_actor_TD(env, n_step_rewards, log_probs, distributions, new_states, old_states, done, Gamma_V)\n",
    "        \n",
    "        return actor_loss\n",
    "    \n",
    "\n",
    "    def update_actor_TD(self, env, n_step_rewards, log_probs, distributions, new_states, old_states, done, Gamma_V):\n",
    "        \n",
    "        # Compute gradient \n",
    "        if debug: print(\"Updating actor...\")\n",
    "        with torch.no_grad():\n",
    "            V_pred = self.critic(old_states, env).squeeze()\n",
    "            V_trg = self.critic(new_states, env).squeeze()\n",
    "            V_trg = (1-done)*Gamma_V*V_trg + n_step_rewards\n",
    "            \n",
    "        A = V_trg - V_pred\n",
    "        policy_gradient = - log_probs*A\n",
    "        if debug:\n",
    "            print(\"V_trg.shape: \",V_trg.shape)\n",
    "            print(\"V_trg: \", V_trg)\n",
    "            print(\"V_pred.shape: \",V_pred.shape)\n",
    "            print(\"V_pred: \", V_pred)\n",
    "            print(\"A.shape: \", A.shape)\n",
    "            print(\"A: \", A)\n",
    "            print(\"policy_gradient.shape: \", policy_gradient.shape)\n",
    "            print(\"policy_gradient: \", policy_gradient)\n",
    "        policy_grad = torch.mean(policy_gradient)\n",
    "        if debug: print(\"policy_grad: \", policy_grad)\n",
    "        entropy = self.H*torch.mean(distributions*torch.log(distributions))\n",
    "        loss = policy_grad + entropy\n",
    "        if debug: print(\"Actor loss: \", loss)\n",
    "        \n",
    "        # Backpropagate and update\n",
    "    \n",
    "        self.actor_optim.zero_grad()\n",
    "        loss.backward()\n",
    "        self.actor_optim.step()\n",
    "        \n",
    "        return policy_grad.item()\n",
    "    \n",
    "    def compute_n_step_rewards(self, rewards):\n",
    "        \"\"\"\n",
    "        Computes n-steps discounted reward padding with zeros the last elements of the trajectory.\n",
    "        This means that the rewards considered are AT MOST n, but can be less for the last n-1 elements.\n",
    "        \"\"\"\n",
    "        T = len(rewards)\n",
    "        \n",
    "        # concatenate n_steps zeros to the rewards -> they do not change the cumsum\n",
    "        r = np.concatenate((rewards,[0 for _ in range(self.n_steps)])) \n",
    "        \n",
    "        Gamma = np.array([self.gamma**i for i in range(r.shape[0])])\n",
    "        \n",
    "        # reverse everything to use cumsum in right order, then reverse again\n",
    "        Gt = np.cumsum(r[::-1]*Gamma[::-1])[::-1]\n",
    "        \n",
    "        G_nstep = Gt[:T] - Gt[self.n_steps:] # compute n-steps discounted return\n",
    "        \n",
    "        Gamma = Gamma[:T]\n",
    "        \n",
    "        assert len(G_nstep) == T, \"Something went wrong computing n-steps reward\"\n",
    "        \n",
    "        n_steps_r = G_nstep / Gamma\n",
    "        \n",
    "        return n_steps_r\n",
    "    \n",
    "    def compute_n_step_states(self, states, done):\n",
    "        \"\"\"\n",
    "        Computes n-steps target states (to be used by the critic as target values together with the\n",
    "        n-steps discounted reward). For last n-1 elements the target state is the last one available.\n",
    "        Adjusts also the `done` mask used for disabling the bootstrapping in the case of terminal states\n",
    "        and returns Gamma_V, that are the discount factors for the target state-values, since they are \n",
    "        n-steps away (except for the last n-1 states, whose discount is adjusted accordingly).\n",
    "        \n",
    "        Return\n",
    "        ------\n",
    "        new_states, Gamma_V, done: arrays with first dimension = len(states)-1\n",
    "        \"\"\"\n",
    "        \n",
    "        # Compute indexes for (at most) n-step away states \n",
    "        \n",
    "        n_step_idx = np.arange(len(states)-1) + self.n_steps\n",
    "        diff = n_step_idx - len(states) + 1\n",
    "        mask = (diff > 0)\n",
    "        n_step_idx[mask] = len(states) - 1\n",
    "        \n",
    "        # Compute new states\n",
    "        \n",
    "        new_states = states[n_step_idx]\n",
    "        \n",
    "        # Compute discount factors\n",
    "        \n",
    "        pw = np.array([self.n_steps for _ in range(len(new_states))])\n",
    "        pw[mask] = self.n_steps - diff[mask]\n",
    "        Gamma_V = self.gamma**pw\n",
    "        \n",
    "        # Adjust done mask\n",
    "        \n",
    "        mask = (diff >= 0)\n",
    "        done[mask] = done[-1]\n",
    "        \n",
    "        return new_states, Gamma_V, done\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_episode(agent, env):\n",
    "    # Reset environment (start of an episode)\n",
    "    state = env.reset(random_init=True)\n",
    "    \n",
    "    states = [state]\n",
    "    rewards = []\n",
    "    log_probs = []\n",
    "    distributions = []\n",
    "    done = []\n",
    "    bootstrap = []\n",
    "    \n",
    "        \n",
    "        \n",
    "    while True:\n",
    "        action, log_prob, distr = agent.get_action(state, return_log=True)\n",
    "        new_state, reward, terminal, info = env.step(action) # gym standard step's output\n",
    "        \n",
    "\n",
    "        states.append(new_state)\n",
    "        rewards.append(reward)\n",
    "        log_probs.append(log_prob)\n",
    "        distributions.append(distr)\n",
    "        done.append(terminal)\n",
    "        \n",
    "        if terminal is True and 'TimeLimit.truncated' in info:\n",
    "            bootstrap.append(True)\n",
    "        else:\n",
    "            bootstrap.append(False) \n",
    "            \n",
    "        if terminal:\n",
    "            break\n",
    "            \n",
    "        state = new_state\n",
    "       \n",
    "    return  np.array(rewards), log_probs, distributions, np.array(states), np.array(done), np.array(bootstrap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_actor_TD(actor, env, n_episodes = 100):\n",
    "    losses = []\n",
    "    performances = []\n",
    "    steps_to_solve = []\n",
    "    for e in range(n_episodes):\n",
    "        rewards, log_probs, distributions, states, done, bootstrap = play_episode(actor, env)\n",
    "        loss = actor.update_TD(env, rewards, log_probs, distributions, states, done, bootstrap)\n",
    "        losses.append(loss)\n",
    "        performances.append(rewards.sum())\n",
    "        steps_to_solve.append(len(rewards))\n",
    "        if (e+1)%10 == 0:\n",
    "            print(\"Episode %d - reward: %.2f -  steps to solve: %.2f\"%(e+1, np.mean(performances[-10:]), np.mean(steps_to_solve[-10:])))\n",
    "\n",
    "    return actor, performances, losses, steps_to_solve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HPs = dict(lr=0.0003, gamma=0.99, TD=True, twin=True, tau=1., n_steps=1,\n",
    "               H=1., vocab_size = 4, n_dim=3, n_features=64, n_heads=4, linear_size=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = test_env.Sandbox(5, 5, [0,0], [4,4], max_steps=50)\n",
    "action_space = 4\n",
    "actor = ActorAgentTD(action_space, **HPs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "actor_TD, score, actor_loss_TD, steps_to_solve = train_actor_TD(actor, env, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = \"Number of episodes\"\n",
    "y = \"Actor loss\"\n",
    "t = \"Actor trained through optimal critic\"\n",
    "plot_loss(actor_loss_TD, x, y, t, ylog=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = \"Number of episodes\"\n",
    "y = \"Total reward\"\n",
    "t = \"Actor trained through optimal critic\"\n",
    "window = 500\n",
    "moving_score =  np.array([np.mean(score[i:i+window]) for i in range(len(score)-window)])\n",
    "plot_loss(moving_score, x, y, t, ylog=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "utils.render(actor_TD, x=5, y=5, goal=[4,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save = False\n",
    "keywords = ['relational', 'sandbox',str(len(score))+\"-episodes\",\"100-steps\"] # example\n",
    "\n",
    "if colab and save:\n",
    "    %cd ~\n",
    "    parent_dir = \"/content/gdrive/My Drive/Colab Notebooks/\"\n",
    "    save_dir  = \"RelationalTrained/\"\n",
    "    %cd \"{parent_dir}\"\n",
    "    !mkdir \"{save_dir}\"\n",
    "    ID = utils.save_session(save_dir, keywords, game_params, HPs, score)\n",
    "    torch.save(trained_agent, save_dir+\"agent_\"+ID)\n",
    "elif save:\n",
    "    save_dir = 'Results/Sandbox/'\n",
    "    ID = utils.save_session(save_dir, keywords, game_params, HPs, score)\n",
    "    torch.save(trained_agent, save_dir+\"agent_\"+ID)\n",
    "else:\n",
    "    print(\"Nothing saved\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
