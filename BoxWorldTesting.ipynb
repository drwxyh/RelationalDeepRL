{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "colab_type": "code",
    "id": "R9EYGzHsgtK6",
    "outputId": "b8434c21-12f3-4b79-e71b-07e5cf0864ef"
   },
   "outputs": [],
   "source": [
    "colab = False\n",
    "\n",
    "if colab:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    !git clone https://github.com/deepmind/pycolab.git\n",
    "    !git clone https://github.com/nicoladainese96/RelationalModule.git\n",
    "    !pip install pycolab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YhQ0STsMfTkc"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from RelationalModule import ActorCritic, ControlActorCritic\n",
    "from RelationalModule import train_agent as train\n",
    "from RelationalModule import utils\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "6iwtqRC6fTkx",
    "outputId": "35629e0e-d823-4c64-a249-975db24775f1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'RelationalModule.train_agent' from '/home/nicola/Nicola_unipd/MasterThesis/RelationalDeepRL/RelationalModule/train_agent.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bec0WsZRfTlG"
   },
   "outputs": [],
   "source": [
    "GRID_SIZE = 7\n",
    "game_params = dict(grid_size=GRID_SIZE,\n",
    "                solution_length=[0],\n",
    "                num_forward = [0], # number of distractors\n",
    "                num_backward=[0], # just set to 0 for now\n",
    "                branch_length=1, # length of forward distractors\n",
    "                max_num_steps = 120\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "HDHaeLwhfTlT",
    "outputId": "cd556298-557f-4d17-e299-37ac9f9432f0",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device:  cpu\n",
      "========== A2C HyperParameters ==========\n",
      "Discount factor:  0.99\n",
      "Learning rate:  0.003\n",
      "Action space:  4\n",
      "Temporal Difference learning:  True\n",
      "Twin networks:  True\n",
      "Update critic target factor:  0.2\n",
      "n_steps for TD:  40\n",
      "Device used:  cpu\n",
      "\n",
      "\n",
      "========== A2C Architecture ==========\n",
      "Actor architecture: \n",
      " BoxWorldActor(\n",
      "  (boxnet): BoxWorldNet(\n",
      "    (net): Sequential(\n",
      "      (0): ExtractEntities(\n",
      "        (embed): Embedding(117, 12)\n",
      "        (net): Sequential(\n",
      "          (0): Conv2d(12, 48, kernel_size=(2, 2), stride=(1, 1))\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(48, 96, kernel_size=(2, 2), stride=(1, 1))\n",
      "          (3): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (1): RelationalModule(\n",
      "        (net): Sequential(\n",
      "          (0): PositionalEncoding(\n",
      "            (projection): Linear(in_features=98, out_features=64, bias=True)\n",
      "          )\n",
      "          (1): AttentionBlock(\n",
      "            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (attn): MultiheadAttention(\n",
      "              (out_proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "            )\n",
      "            (ff): PositionwiseFeedForward(\n",
      "              (w_1): Linear(in_features=64, out_features=64, bias=True)\n",
      "              (w_2): Linear(in_features=64, out_features=64, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): AttentionBlock(\n",
      "            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (attn): MultiheadAttention(\n",
      "              (out_proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "            )\n",
      "            (ff): PositionwiseFeedForward(\n",
      "              (w_1): Linear(in_features=64, out_features=64, bias=True)\n",
      "              (w_2): Linear(in_features=64, out_features=64, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): FeaturewiseProjection(\n",
      "        (norm): LayerNorm((49,), eps=1e-05, elementwise_affine=True)\n",
      "        (projection): Linear(in_features=49, out_features=1, bias=True)\n",
      "      )\n",
      "      (3): ResidualLayer(\n",
      "        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (w1): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (w2): Linear(in_features=64, out_features=64, bias=True)\n",
      "      )\n",
      "      (4): ResidualLayer(\n",
      "        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (w1): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (w2): Linear(in_features=64, out_features=64, bias=True)\n",
      "      )\n",
      "      (5): ResidualLayer(\n",
      "        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (w1): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (w2): Linear(in_features=64, out_features=64, bias=True)\n",
      "      )\n",
      "      (6): ResidualLayer(\n",
      "        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (w1): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (w2): Linear(in_features=64, out_features=64, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (linear): Linear(in_features=64, out_features=4, bias=True)\n",
      ")\n",
      "Critic architecture: \n",
      " BoxWorldCritic(\n",
      "  (net1): BoxWorldBasicCritic(\n",
      "    (boxnet): BoxWorldNet(\n",
      "      (net): Sequential(\n",
      "        (0): ExtractEntities(\n",
      "          (embed): Embedding(117, 12)\n",
      "          (net): Sequential(\n",
      "            (0): Conv2d(12, 48, kernel_size=(2, 2), stride=(1, 1))\n",
      "            (1): ReLU()\n",
      "            (2): Conv2d(48, 96, kernel_size=(2, 2), stride=(1, 1))\n",
      "            (3): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (1): RelationalModule(\n",
      "          (net): Sequential(\n",
      "            (0): PositionalEncoding(\n",
      "              (projection): Linear(in_features=98, out_features=64, bias=True)\n",
      "            )\n",
      "            (1): AttentionBlock(\n",
      "              (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (attn): MultiheadAttention(\n",
      "                (out_proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "              )\n",
      "              (ff): PositionwiseFeedForward(\n",
      "                (w_1): Linear(in_features=64, out_features=64, bias=True)\n",
      "                (w_2): Linear(in_features=64, out_features=64, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (2): AttentionBlock(\n",
      "              (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (attn): MultiheadAttention(\n",
      "                (out_proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "              )\n",
      "              (ff): PositionwiseFeedForward(\n",
      "                (w_1): Linear(in_features=64, out_features=64, bias=True)\n",
      "                (w_2): Linear(in_features=64, out_features=64, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): FeaturewiseProjection(\n",
      "          (norm): LayerNorm((49,), eps=1e-05, elementwise_affine=True)\n",
      "          (projection): Linear(in_features=49, out_features=1, bias=True)\n",
      "        )\n",
      "        (3): ResidualLayer(\n",
      "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (w1): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (w2): Linear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (4): ResidualLayer(\n",
      "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (w1): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (w2): Linear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (5): ResidualLayer(\n",
      "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (w1): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (w2): Linear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (6): ResidualLayer(\n",
      "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (w1): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (w2): Linear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (linear): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      "  (net2): BoxWorldBasicCritic(\n",
      "    (boxnet): BoxWorldNet(\n",
      "      (net): Sequential(\n",
      "        (0): ExtractEntities(\n",
      "          (embed): Embedding(117, 12)\n",
      "          (net): Sequential(\n",
      "            (0): Conv2d(12, 48, kernel_size=(2, 2), stride=(1, 1))\n",
      "            (1): ReLU()\n",
      "            (2): Conv2d(48, 96, kernel_size=(2, 2), stride=(1, 1))\n",
      "            (3): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (1): RelationalModule(\n",
      "          (net): Sequential(\n",
      "            (0): PositionalEncoding(\n",
      "              (projection): Linear(in_features=98, out_features=64, bias=True)\n",
      "            )\n",
      "            (1): AttentionBlock(\n",
      "              (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (attn): MultiheadAttention(\n",
      "                (out_proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "              )\n",
      "              (ff): PositionwiseFeedForward(\n",
      "                (w_1): Linear(in_features=64, out_features=64, bias=True)\n",
      "                (w_2): Linear(in_features=64, out_features=64, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (2): AttentionBlock(\n",
      "              (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (attn): MultiheadAttention(\n",
      "                (out_proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "              )\n",
      "              (ff): PositionwiseFeedForward(\n",
      "                (w_1): Linear(in_features=64, out_features=64, bias=True)\n",
      "                (w_2): Linear(in_features=64, out_features=64, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): FeaturewiseProjection(\n",
      "          (norm): LayerNorm((49,), eps=1e-05, elementwise_affine=True)\n",
      "          (projection): Linear(in_features=49, out_features=1, bias=True)\n",
      "        )\n",
      "        (3): ResidualLayer(\n",
      "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (w1): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (w2): Linear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (4): ResidualLayer(\n",
      "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (w1): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (w2): Linear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (5): ResidualLayer(\n",
      "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (w1): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (w2): Linear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (6): ResidualLayer(\n",
      "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (w1): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (w2): Linear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (linear): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Critic target architecture: \n",
      "BoxWorldCritic(\n",
      "  (net1): BoxWorldBasicCritic(\n",
      "    (boxnet): BoxWorldNet(\n",
      "      (net): Sequential(\n",
      "        (0): ExtractEntities(\n",
      "          (embed): Embedding(117, 12)\n",
      "          (net): Sequential(\n",
      "            (0): Conv2d(12, 48, kernel_size=(2, 2), stride=(1, 1))\n",
      "            (1): ReLU()\n",
      "            (2): Conv2d(48, 96, kernel_size=(2, 2), stride=(1, 1))\n",
      "            (3): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (1): RelationalModule(\n",
      "          (net): Sequential(\n",
      "            (0): PositionalEncoding(\n",
      "              (projection): Linear(in_features=98, out_features=64, bias=True)\n",
      "            )\n",
      "            (1): AttentionBlock(\n",
      "              (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (attn): MultiheadAttention(\n",
      "                (out_proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "              )\n",
      "              (ff): PositionwiseFeedForward(\n",
      "                (w_1): Linear(in_features=64, out_features=64, bias=True)\n",
      "                (w_2): Linear(in_features=64, out_features=64, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (2): AttentionBlock(\n",
      "              (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (attn): MultiheadAttention(\n",
      "                (out_proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "              )\n",
      "              (ff): PositionwiseFeedForward(\n",
      "                (w_1): Linear(in_features=64, out_features=64, bias=True)\n",
      "                (w_2): Linear(in_features=64, out_features=64, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): FeaturewiseProjection(\n",
      "          (norm): LayerNorm((49,), eps=1e-05, elementwise_affine=True)\n",
      "          (projection): Linear(in_features=49, out_features=1, bias=True)\n",
      "        )\n",
      "        (3): ResidualLayer(\n",
      "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (w1): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (w2): Linear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (4): ResidualLayer(\n",
      "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (w1): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (w2): Linear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (5): ResidualLayer(\n",
      "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (w1): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (w2): Linear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (6): ResidualLayer(\n",
      "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (w1): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (w2): Linear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (linear): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      "  (net2): BoxWorldBasicCritic(\n",
      "    (boxnet): BoxWorldNet(\n",
      "      (net): Sequential(\n",
      "        (0): ExtractEntities(\n",
      "          (embed): Embedding(117, 12)\n",
      "          (net): Sequential(\n",
      "            (0): Conv2d(12, 48, kernel_size=(2, 2), stride=(1, 1))\n",
      "            (1): ReLU()\n",
      "            (2): Conv2d(48, 96, kernel_size=(2, 2), stride=(1, 1))\n",
      "            (3): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (1): RelationalModule(\n",
      "          (net): Sequential(\n",
      "            (0): PositionalEncoding(\n",
      "              (projection): Linear(in_features=98, out_features=64, bias=True)\n",
      "            )\n",
      "            (1): AttentionBlock(\n",
      "              (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (attn): MultiheadAttention(\n",
      "                (out_proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "              )\n",
      "              (ff): PositionwiseFeedForward(\n",
      "                (w_1): Linear(in_features=64, out_features=64, bias=True)\n",
      "                (w_2): Linear(in_features=64, out_features=64, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (2): AttentionBlock(\n",
      "              (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (attn): MultiheadAttention(\n",
      "                (out_proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "              )\n",
      "              (ff): PositionwiseFeedForward(\n",
      "                (w_1): Linear(in_features=64, out_features=64, bias=True)\n",
      "                (w_2): Linear(in_features=64, out_features=64, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): FeaturewiseProjection(\n",
      "          (norm): LayerNorm((49,), eps=1e-05, elementwise_affine=True)\n",
      "          (projection): Linear(in_features=49, out_features=1, bias=True)\n",
      "        )\n",
      "        (3): ResidualLayer(\n",
      "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (w1): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (w2): Linear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (4): ResidualLayer(\n",
      "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (w1): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (w2): Linear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (5): ResidualLayer(\n",
      "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (w1): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (w2): Linear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (6): ResidualLayer(\n",
      "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (w1): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (w2): Linear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (linear): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "========== A2C HyperParameters ==========\n",
      "Discount factor:  0.99\n",
      "Learning rate:  0.003\n",
      "Action space:  4\n",
      "Temporal Difference learning:  True\n",
      "Twin networks:  True\n",
      "Update critic target factor:  0.2\n",
      "n_steps for TD:  40\n",
      "Device used:  cpu\n",
      "\n",
      "\n",
      "========== A2C Architecture ==========\n",
      "Actor architecture: \n",
      " ControlActor(\n",
      "  (control_net): ControlNet(\n",
      "    (embed): Embedding(117, 3)\n",
      "    (net): Sequential(\n",
      "      (0): Linear(in_features=243, out_features=768, bias=True)\n",
      "      (1): ResidualLayer(\n",
      "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (w1): Linear(in_features=768, out_features=256, bias=True)\n",
      "        (w2): Linear(in_features=256, out_features=768, bias=True)\n",
      "      )\n",
      "      (2): Linear(in_features=768, out_features=256, bias=True)\n",
      "      (3): ResidualLayer(\n",
      "        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (w1): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (w2): Linear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (linear): Linear(in_features=256, out_features=4, bias=True)\n",
      ")\n",
      "Critic architecture: \n",
      " ControlCritic(\n",
      "  (net1): ControlBasicCritic(\n",
      "    (control_net): ControlNet(\n",
      "      (embed): Embedding(117, 3)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=243, out_features=768, bias=True)\n",
      "        (1): ResidualLayer(\n",
      "          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (w1): Linear(in_features=768, out_features=256, bias=True)\n",
      "          (w2): Linear(in_features=256, out_features=768, bias=True)\n",
      "        )\n",
      "        (2): Linear(in_features=768, out_features=256, bias=True)\n",
      "        (3): ResidualLayer(\n",
      "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (w1): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (w2): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (linear): Linear(in_features=256, out_features=1, bias=True)\n",
      "  )\n",
      "  (net2): ControlBasicCritic(\n",
      "    (control_net): ControlNet(\n",
      "      (embed): Embedding(117, 3)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=243, out_features=768, bias=True)\n",
      "        (1): ResidualLayer(\n",
      "          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (w1): Linear(in_features=768, out_features=256, bias=True)\n",
      "          (w2): Linear(in_features=256, out_features=768, bias=True)\n",
      "        )\n",
      "        (2): Linear(in_features=768, out_features=256, bias=True)\n",
      "        (3): ResidualLayer(\n",
      "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (w1): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (w2): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (linear): Linear(in_features=256, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Critic target architecture: \n",
      "ControlCritic(\n",
      "  (net1): ControlBasicCritic(\n",
      "    (control_net): ControlNet(\n",
      "      (embed): Embedding(117, 3)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=243, out_features=768, bias=True)\n",
      "        (1): ResidualLayer(\n",
      "          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (w1): Linear(in_features=768, out_features=256, bias=True)\n",
      "          (w2): Linear(in_features=256, out_features=768, bias=True)\n",
      "        )\n",
      "        (2): Linear(in_features=768, out_features=256, bias=True)\n",
      "        (3): ResidualLayer(\n",
      "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (w1): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (w2): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (linear): Linear(in_features=256, out_features=1, bias=True)\n",
      "  )\n",
      "  (net2): ControlBasicCritic(\n",
      "    (control_net): ControlNet(\n",
      "      (embed): Embedding(117, 3)\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=243, out_features=768, bias=True)\n",
      "        (1): ResidualLayer(\n",
      "          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (w1): Linear(in_features=768, out_features=256, bias=True)\n",
      "          (w2): Linear(in_features=256, out_features=768, bias=True)\n",
      "        )\n",
      "        (2): Linear(in_features=768, out_features=256, bias=True)\n",
      "        (3): ResidualLayer(\n",
      "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (w1): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (w2): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (linear): Linear(in_features=256, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "HPs = dict(action_space=4, lr=0.003, gamma=0.99, TD=True, twin=True, tau=0.2, n_steps=40,\n",
    "           n_kernels=96, vocab_size = 117, n_dim=12, n_features=64, n_heads=4, n_attn_modules=2, \n",
    "           n_linears=4, max_pool=False, linear_size=GRID_SIZE+2)\n",
    "if colab:\n",
    "    HPs['device'] = 'cuda'\n",
    "else:\n",
    "    HPs['device'] = 'cpu'\n",
    "\n",
    "print('device: ', HPs['device'])  \n",
    "\n",
    "# Relational Agent\n",
    "agent = ActorCritic.BoxWorldA2C(**HPs)\n",
    "\n",
    "control_HPs = dict(action_space=4, lr=0.003, gamma=0.99, TD=True, twin=True, tau=0.2, n_steps=40, linear_size=GRID_SIZE+2)\n",
    "           \n",
    "# Control Agent\n",
    "control_agent = ControlActorCritic.ControlA2C(**control_HPs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mgCA3SMwxpcz"
   },
   "outputs": [],
   "source": [
    "# Random Agent\n",
    "\n",
    "class RandomAgent():\n",
    "    def __init__(self, n_actions):\n",
    "        self.n_actions = n_actions\n",
    "    \n",
    "    def get_action(self,state, *args, **kwargs):\n",
    "        a = np.random.choice(self.n_actions)\n",
    "        log_prob = np.log(1./self.n_actions) # just because it's the standard output of the other agent\n",
    "        return a, log_prob\n",
    "    \n",
    "    def update(self, *args):\n",
    "        return\n",
    "\n",
    "rnd_agent = RandomAgent(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 593
    },
    "colab_type": "code",
    "id": "Kf30nk3RqdHb",
    "outputId": "badd17e9-341e-4354-f224-7fc1a15eb529",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicola/anaconda3/envs/torch/lib/python3.7/site-packages/pycolab/ascii_art.py:318: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  art = np.vstack(np.fromstring(line, dtype=np.uint8) for line in art)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distribution:  tensor([[0.0469, 0.3524, 0.4029, 0.1978]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.1030, 0.2861, 0.2850, 0.3260]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0565, 0.2489, 0.5387, 0.1558]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0718, 0.3318, 0.2542, 0.3421]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0868, 0.1520, 0.5506, 0.2107]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0906, 0.4149, 0.3017, 0.1927]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0637, 0.2144, 0.3684, 0.3534]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.1399, 0.3144, 0.2605, 0.2852]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0675, 0.3322, 0.4236, 0.1767]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0936, 0.3682, 0.4459, 0.0923]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0500, 0.3600, 0.2992, 0.2909]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0692, 0.4174, 0.3606, 0.1528]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0499, 0.5568, 0.2611, 0.1321]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0318, 0.3353, 0.4728, 0.1600]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.1398, 0.3100, 0.2529, 0.2974]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0903, 0.2814, 0.4526, 0.1757]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.1288, 0.4484, 0.1061, 0.3167]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0467, 0.4108, 0.5007, 0.0419]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.1621, 0.3613, 0.2997, 0.1770]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.1727, 0.3542, 0.2898, 0.1833]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0543, 0.3943, 0.2330, 0.3184]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0479, 0.5467, 0.2679, 0.1375]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.2376, 0.2634, 0.3145, 0.1845]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0720, 0.3709, 0.4275, 0.1296]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.1089, 0.4366, 0.3683, 0.0861]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.1044, 0.3247, 0.2391, 0.3319]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0686, 0.5116, 0.2375, 0.1823]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0644, 0.3725, 0.2625, 0.3006]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.1096, 0.2103, 0.4432, 0.2369]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0485, 0.5464, 0.3219, 0.0832]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0622, 0.3934, 0.3734, 0.1711]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0874, 0.6191, 0.2260, 0.0675]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0797, 0.1647, 0.5132, 0.2424]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.1532, 0.1867, 0.2575, 0.4026]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0820, 0.3710, 0.3795, 0.1674]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.1187, 0.2362, 0.3332, 0.3119]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0841, 0.2294, 0.5441, 0.1425]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.1112, 0.3180, 0.4384, 0.1323]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.1184, 0.2840, 0.3798, 0.2179]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0472, 0.4118, 0.3966, 0.1444]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0557, 0.3029, 0.2538, 0.3875]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0831, 0.4834, 0.3717, 0.0618]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.1498, 0.1879, 0.3174, 0.3449]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0771, 0.4648, 0.2877, 0.1703]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.1740, 0.2852, 0.2761, 0.2647]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.1274, 0.2662, 0.3099, 0.2964]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0618, 0.4061, 0.1552, 0.3769]], grad_fn=<ExpBackward>)\n",
      "n_step_rewards.shape:  (47,)\n",
      "rewards.shape:  (47,)\n",
      "n_step_rewards:  [ 0.          0.          0.          0.          0.          0.\n",
      "  0.          6.75729049  6.82554595  6.89449086  6.96413218  7.03447695\n",
      "  7.10553227  7.17730533  7.24980336  7.3230337   7.39700373  7.47172094\n",
      "  7.54719287  7.62342714  7.70043146  7.77821359  7.85678141  7.93614284\n",
      "  8.0163059   8.09727868  8.17906938  8.26168624  8.34513761  8.42943193\n",
      "  8.51457771  8.60058355  8.68745813  8.77521023  8.86384872  8.95338254\n",
      "  9.04382075  9.13517247  9.22744694  9.32065348  9.41480149  9.5099005\n",
      "  9.6059601   9.70299     9.801       9.9        10.        ]\n",
      "rewards:  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 10.]\n",
      "bootstrap:  [False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False]\n",
      "done.shape: (before n_steps) (47,)\n",
      "done: (before n_steps) [False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False  True]\n",
      "done.shape: (after n_steps) (47,)\n",
      "Gamma_V.shape:  (47,)\n",
      "done: (after n_steps) [False False False False False False False  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True]\n",
      "Gamma_V:  [0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.67572905 0.6825546  0.68944909 0.69641322\n",
      " 0.70344769 0.71055323 0.71773053 0.72498034 0.73230337 0.73970037\n",
      " 0.74717209 0.75471929 0.76234271 0.77004315 0.77782136 0.78567814\n",
      " 0.79361428 0.80163059 0.80972787 0.81790694 0.82616862 0.83451376\n",
      " 0.84294319 0.85145777 0.86005835 0.86874581 0.87752102 0.88638487\n",
      " 0.89533825 0.90438208 0.91351725 0.92274469 0.93206535 0.94148015\n",
      " 0.95099005 0.96059601 0.970299   0.9801     0.99      ]\n",
      "old_states.shape:  torch.Size([47, 1, 9, 9])\n",
      "new_states.shape:  torch.Size([47, 1, 9, 9])\n",
      "log_probs:  [tensor(-0.9091, grad_fn=<SelectBackward>), tensor(-1.2516, grad_fn=<SelectBackward>), tensor(-0.6186, grad_fn=<SelectBackward>), tensor(-1.3695, grad_fn=<SelectBackward>), tensor(-1.5575, grad_fn=<SelectBackward>), tensor(-1.1982, grad_fn=<SelectBackward>), tensor(-1.0401, grad_fn=<SelectBackward>), tensor(-1.1570, grad_fn=<SelectBackward>), tensor(-0.8589, grad_fn=<SelectBackward>), tensor(-0.9991, grad_fn=<SelectBackward>), tensor(-1.0218, grad_fn=<SelectBackward>), tensor(-1.8788, grad_fn=<SelectBackward>), tensor(-0.5855, grad_fn=<SelectBackward>), tensor(-0.7490, grad_fn=<SelectBackward>), tensor(-1.2127, grad_fn=<SelectBackward>), tensor(-1.2681, grad_fn=<SelectBackward>), tensor(-0.8020, grad_fn=<SelectBackward>), tensor(-0.8897, grad_fn=<SelectBackward>), tensor(-1.0182, grad_fn=<SelectBackward>), tensor(-1.2384, grad_fn=<SelectBackward>), tensor(-1.1444, grad_fn=<SelectBackward>), tensor(-1.9841, grad_fn=<SelectBackward>), tensor(-1.1567, grad_fn=<SelectBackward>), tensor(-0.8499, grad_fn=<SelectBackward>), tensor(-0.8287, grad_fn=<SelectBackward>), tensor(-1.1030, grad_fn=<SelectBackward>), tensor(-0.6702, grad_fn=<SelectBackward>), tensor(-0.9874, grad_fn=<SelectBackward>), tensor(-1.5593, grad_fn=<SelectBackward>), tensor(-0.6044, grad_fn=<SelectBackward>), tensor(-1.7655, grad_fn=<SelectBackward>), tensor(-0.4795, grad_fn=<SelectBackward>), tensor(-0.6671, grad_fn=<SelectBackward>), tensor(-1.6782, grad_fn=<SelectBackward>), tensor(-0.9688, grad_fn=<SelectBackward>), tensor(-1.0989, grad_fn=<SelectBackward>), tensor(-0.6087, grad_fn=<SelectBackward>), tensor(-0.8246, grad_fn=<SelectBackward>), tensor(-1.2588, grad_fn=<SelectBackward>), tensor(-0.8873, grad_fn=<SelectBackward>), tensor(-1.3710, grad_fn=<SelectBackward>), tensor(-0.7268, grad_fn=<SelectBackward>), tensor(-1.0646, grad_fn=<SelectBackward>), tensor(-1.7701, grad_fn=<SelectBackward>), tensor(-1.3292, grad_fn=<SelectBackward>), tensor(-1.3234, grad_fn=<SelectBackward>), tensor(-2.7832, grad_fn=<SelectBackward>)]\n",
      "log_probs:  tensor([-0.9091, -1.2516, -0.6186, -1.3695, -1.5575, -1.1982, -1.0401, -1.1570,\n",
      "        -0.8589, -0.9991, -1.0218, -1.8788, -0.5855, -0.7490, -1.2127, -1.2681,\n",
      "        -0.8020, -0.8897, -1.0182, -1.2384, -1.1444, -1.9841, -1.1567, -0.8499,\n",
      "        -0.8287, -1.1030, -0.6702, -0.9874, -1.5593, -0.6044, -1.7655, -0.4795,\n",
      "        -0.6671, -1.6782, -0.9688, -1.0989, -0.6087, -0.8246, -1.2588, -0.8873,\n",
      "        -1.3710, -0.7268, -1.0646, -1.7701, -1.3292, -1.3234, -2.7832],\n",
      "       grad_fn=<StackBackward>)\n",
      "distributions.shape:  torch.Size([1, 47, 4])\n",
      "distributions:  tensor([[[0.0469, 0.3524, 0.4029, 0.1978],\n",
      "         [0.1030, 0.2861, 0.2850, 0.3260],\n",
      "         [0.0565, 0.2489, 0.5387, 0.1558],\n",
      "         [0.0718, 0.3318, 0.2542, 0.3421],\n",
      "         [0.0868, 0.1520, 0.5506, 0.2107],\n",
      "         [0.0906, 0.4149, 0.3017, 0.1927],\n",
      "         [0.0637, 0.2144, 0.3684, 0.3534],\n",
      "         [0.1399, 0.3144, 0.2605, 0.2852],\n",
      "         [0.0675, 0.3322, 0.4236, 0.1767],\n",
      "         [0.0936, 0.3682, 0.4459, 0.0923],\n",
      "         [0.0500, 0.3600, 0.2992, 0.2909],\n",
      "         [0.0692, 0.4174, 0.3606, 0.1528],\n",
      "         [0.0499, 0.5568, 0.2611, 0.1321],\n",
      "         [0.0318, 0.3353, 0.4728, 0.1600],\n",
      "         [0.1398, 0.3100, 0.2529, 0.2974],\n",
      "         [0.0903, 0.2814, 0.4526, 0.1757],\n",
      "         [0.1288, 0.4484, 0.1061, 0.3167],\n",
      "         [0.0467, 0.4108, 0.5007, 0.0419],\n",
      "         [0.1621, 0.3613, 0.2997, 0.1770],\n",
      "         [0.1727, 0.3542, 0.2898, 0.1833],\n",
      "         [0.0543, 0.3943, 0.2330, 0.3184],\n",
      "         [0.0479, 0.5467, 0.2679, 0.1375],\n",
      "         [0.2376, 0.2634, 0.3145, 0.1845],\n",
      "         [0.0720, 0.3709, 0.4275, 0.1296],\n",
      "         [0.1089, 0.4366, 0.3683, 0.0861],\n",
      "         [0.1044, 0.3247, 0.2391, 0.3319],\n",
      "         [0.0686, 0.5116, 0.2375, 0.1823],\n",
      "         [0.0644, 0.3725, 0.2625, 0.3006],\n",
      "         [0.1096, 0.2103, 0.4432, 0.2369],\n",
      "         [0.0485, 0.5464, 0.3219, 0.0832],\n",
      "         [0.0622, 0.3934, 0.3734, 0.1711],\n",
      "         [0.0874, 0.6191, 0.2260, 0.0675],\n",
      "         [0.0797, 0.1647, 0.5132, 0.2424],\n",
      "         [0.1532, 0.1867, 0.2575, 0.4026],\n",
      "         [0.0820, 0.3710, 0.3795, 0.1674],\n",
      "         [0.1187, 0.2362, 0.3332, 0.3119],\n",
      "         [0.0841, 0.2294, 0.5441, 0.1425],\n",
      "         [0.1112, 0.3180, 0.4384, 0.1323],\n",
      "         [0.1184, 0.2840, 0.3798, 0.2179],\n",
      "         [0.0472, 0.4118, 0.3966, 0.1444],\n",
      "         [0.0557, 0.3029, 0.2538, 0.3875],\n",
      "         [0.0831, 0.4834, 0.3717, 0.0618],\n",
      "         [0.1498, 0.1879, 0.3174, 0.3449],\n",
      "         [0.0771, 0.4648, 0.2877, 0.1703],\n",
      "         [0.1740, 0.2852, 0.2761, 0.2647],\n",
      "         [0.1274, 0.2662, 0.3099, 0.2964],\n",
      "         [0.0618, 0.4061, 0.1552, 0.3769]]], grad_fn=<StackBackward>)\n",
      "Updating critic...\n",
      "V_trg.shape (after critic):  torch.Size([47])\n",
      "V_trg.shape (after sum):  torch.Size([47])\n",
      "V_trg.shape (after squeeze):  torch.Size([47])\n",
      "V_trg.shape (after squeeze):  tensor([ 0.1062, -0.1508, -0.3810,  0.1029, -0.3638,  0.0108, -0.1003,  6.7573,\n",
      "         6.8255,  6.8945,  6.9641,  7.0345,  7.1055,  7.1773,  7.2498,  7.3230,\n",
      "         7.3970,  7.4717,  7.5472,  7.6234,  7.7004,  7.7782,  7.8568,  7.9361,\n",
      "         8.0163,  8.0973,  8.1791,  8.2617,  8.3451,  8.4294,  8.5146,  8.6006,\n",
      "         8.6875,  8.7752,  8.8638,  8.9534,  9.0438,  9.1352,  9.2274,  9.3207,\n",
      "         9.4148,  9.5099,  9.6060,  9.7030,  9.8010,  9.9000, 10.0000])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V1.shape:  torch.Size([47])\n",
      "V1:  tensor([[ 0.5859],\n",
      "        [ 0.2206],\n",
      "        [ 0.3176],\n",
      "        [ 0.3007],\n",
      "        [ 0.2747],\n",
      "        [ 0.1583],\n",
      "        [ 0.6236],\n",
      "        [ 0.4317],\n",
      "        [-0.7255],\n",
      "        [-0.3691],\n",
      "        [ 0.4405],\n",
      "        [ 0.1400],\n",
      "        [-0.3781],\n",
      "        [ 0.4212],\n",
      "        [ 0.0704],\n",
      "        [ 0.0202],\n",
      "        [ 0.1691],\n",
      "        [ 0.9707],\n",
      "        [-0.2781],\n",
      "        [ 0.1818],\n",
      "        [ 0.3323],\n",
      "        [-0.4326],\n",
      "        [-1.0807],\n",
      "        [-0.0749],\n",
      "        [-0.7235],\n",
      "        [-0.0991],\n",
      "        [ 0.5995],\n",
      "        [-0.1823],\n",
      "        [-0.2194],\n",
      "        [ 0.3198],\n",
      "        [-0.1203],\n",
      "        [-0.6519],\n",
      "        [ 0.2617],\n",
      "        [ 0.3949],\n",
      "        [-0.0048],\n",
      "        [-0.3949],\n",
      "        [ 0.1856],\n",
      "        [-0.7357],\n",
      "        [-0.0497],\n",
      "        [ 0.1285],\n",
      "        [-0.7248],\n",
      "        [ 0.3275],\n",
      "        [-0.4716],\n",
      "        [ 0.9068],\n",
      "        [-0.1003],\n",
      "        [-0.2856],\n",
      "        [-0.0735]], grad_fn=<AddmmBackward>)\n",
      "Updating actor...\n",
      "V_trg.shape:  torch.Size([47])\n",
      "V_trg:  tensor([ 0.8675,  1.2904,  1.1642,  1.0365,  1.2365, -0.0753,  0.6008,  6.7573,\n",
      "         6.8255,  6.8945,  6.9641,  7.0345,  7.1055,  7.1773,  7.2498,  7.3230,\n",
      "         7.3970,  7.4717,  7.5472,  7.6234,  7.7004,  7.7782,  7.8568,  7.9361,\n",
      "         8.0163,  8.0973,  8.1791,  8.2617,  8.3451,  8.4294,  8.5146,  8.6006,\n",
      "         8.6875,  8.7752,  8.8638,  8.9534,  9.0438,  9.1352,  9.2274,  9.3207,\n",
      "         9.4148,  9.5099,  9.6060,  9.7030,  9.8010,  9.9000, 10.0000])\n",
      "V_pred.shape:  torch.Size([47])\n",
      "V_pred:  tensor([1.7250, 0.8706, 0.9674, 2.0871, 1.9727, 1.0651, 1.2766, 1.3032, 0.9636,\n",
      "        1.0892, 1.3131, 1.3533, 2.2874, 2.1380, 1.3944, 1.2020, 0.7747, 1.0943,\n",
      "        1.5207, 1.4746, 1.5550, 1.7555, 1.4365, 0.9919, 0.4661, 1.6118, 1.6327,\n",
      "        2.0357, 1.2383, 1.5189, 1.6373, 1.5563, 0.5463, 1.2673, 0.3033, 1.7260,\n",
      "        1.5670, 0.9137, 1.3431, 0.7141, 1.3686, 1.5436, 1.9835, 1.7351, 1.5317,\n",
      "        0.4704, 1.2860])\n",
      "A.shape:  torch.Size([47])\n",
      "A:  tensor([-0.8575,  0.4197,  0.1968, -1.0506, -0.7362, -1.1404, -0.6757,  5.4541,\n",
      "         5.8619,  5.8053,  5.6510,  5.6812,  4.8182,  5.0393,  5.8554,  6.1210,\n",
      "         6.6223,  6.3774,  6.0264,  6.1488,  6.1455,  6.0227,  6.4202,  6.9442,\n",
      "         7.5502,  6.4855,  6.5464,  6.2260,  7.1069,  6.9105,  6.8772,  7.0443,\n",
      "         8.1412,  7.5079,  8.5606,  7.2274,  7.4769,  8.2215,  7.8844,  8.6065,\n",
      "         8.0462,  7.9663,  7.6225,  7.9679,  8.2693,  9.4296,  8.7140])\n",
      "policy_gradient.shape:  torch.Size([47])\n",
      "policy_gradient:  tensor([-0.7796,  0.5253,  0.1217, -1.4388, -1.1465, -1.3664, -0.7028,  6.3106,\n",
      "         5.0350,  5.8003,  5.7740, 10.6736,  2.8210,  3.7746,  7.1009,  7.7622,\n",
      "         5.3113,  5.6738,  6.1360,  7.6149,  7.0330, 11.9497,  7.4264,  5.9018,\n",
      "         6.2570,  7.1537,  4.3874,  6.1475, 11.0820,  4.1768, 12.1419,  3.3780,\n",
      "         5.4306, 12.6000,  8.2935,  7.9422,  4.5508,  6.7795,  9.9247,  7.6363,\n",
      "        11.0316,  5.7900,  8.1148, 14.1043, 10.9918, 12.4796, 24.2526],\n",
      "       grad_fn=<MulBackward0>)\n",
      "policy_grad:  tensor(311.9586, grad_fn=<SumBackward0>)\n",
      "Negative entropy:  tensor(-57.5105, grad_fn=<SumBackward0>)\n",
      "Actor loss:  tensor(311.3835, grad_fn=<AddBackward0>)\n",
      "distribution:  tensor([[0.0426, 0.5446, 0.2585, 0.1542]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0466, 0.8114, 0.1014, 0.0405]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0326, 0.6148, 0.1379, 0.2147]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0070, 0.8835, 0.0915, 0.0180]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0378, 0.2672, 0.2715, 0.4235]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0427, 0.6501, 0.1413, 0.1659]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0196, 0.7718, 0.1558, 0.0528]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.1932, 0.3407, 0.1921, 0.2740]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0183, 0.7017, 0.1962, 0.0839]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0879, 0.6394, 0.1397, 0.1330]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0266, 0.6434, 0.1471, 0.1829]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0489, 0.5372, 0.2785, 0.1354]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0956, 0.4752, 0.2799, 0.1493]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0305, 0.6190, 0.2085, 0.1420]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0895, 0.3522, 0.4085, 0.1499]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0397, 0.4385, 0.2790, 0.2428]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0301, 0.7279, 0.1502, 0.0919]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0645, 0.3508, 0.3137, 0.2709]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0498, 0.5490, 0.2618, 0.1394]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0700, 0.3514, 0.3267, 0.2519]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0441, 0.6715, 0.1800, 0.1044]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0348, 0.3277, 0.2388, 0.3987]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.1605, 0.2424, 0.1928, 0.4043]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0300, 0.7256, 0.1817, 0.0627]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0611, 0.3374, 0.2281, 0.3734]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0383, 0.4385, 0.1673, 0.3559]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0290, 0.5179, 0.3102, 0.1429]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0109, 0.6544, 0.1918, 0.1429]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0983, 0.4847, 0.2709, 0.1461]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0134, 0.7413, 0.1462, 0.0992]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.1694, 0.3765, 0.1758, 0.2783]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0593, 0.2778, 0.2222, 0.4407]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0453, 0.5969, 0.2530, 0.1048]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0217, 0.7455, 0.1221, 0.1107]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0645, 0.4431, 0.2117, 0.2807]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0405, 0.7672, 0.0864, 0.1060]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0536, 0.7828, 0.1249, 0.0386]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0264, 0.8205, 0.1204, 0.0327]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0341, 0.8271, 0.0836, 0.0552]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0228, 0.6949, 0.2118, 0.0704]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.1340, 0.2209, 0.1765, 0.4685]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.1137, 0.4871, 0.2254, 0.1738]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0182, 0.4681, 0.2748, 0.2389]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0562, 0.5167, 0.3277, 0.0994]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0452, 0.5357, 0.1840, 0.2351]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0928, 0.4488, 0.1176, 0.3408]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0583, 0.4855, 0.0764, 0.3798]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0604, 0.5560, 0.1720, 0.2115]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0432, 0.5242, 0.2166, 0.2160]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0279, 0.5918, 0.1557, 0.2246]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0301, 0.5018, 0.3524, 0.1156]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.1086, 0.2885, 0.3497, 0.2532]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0113, 0.6875, 0.1328, 0.1684]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0392, 0.7377, 0.1343, 0.0888]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0341, 0.2778, 0.1776, 0.5104]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0469, 0.5605, 0.1711, 0.2215]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0186, 0.6797, 0.2195, 0.0822]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0299, 0.8490, 0.0796, 0.0415]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0896, 0.5113, 0.1714, 0.2277]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0128, 0.9213, 0.0500, 0.0158]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0607, 0.4066, 0.3445, 0.1882]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0277, 0.3094, 0.1535, 0.5093]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0315, 0.3318, 0.3104, 0.3264]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.1079, 0.2803, 0.2767, 0.3350]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0359, 0.4336, 0.2244, 0.3061]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0193, 0.6805, 0.1634, 0.1368]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0323, 0.5692, 0.2334, 0.1650]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0682, 0.4736, 0.1994, 0.2588]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.1107, 0.7116, 0.1176, 0.0602]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0176, 0.8162, 0.1123, 0.0539]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0483, 0.4060, 0.3413, 0.2044]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0703, 0.4075, 0.3603, 0.1619]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0809, 0.3389, 0.1370, 0.4431]], grad_fn=<ExpBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distribution:  tensor([[0.0577, 0.5383, 0.2654, 0.1387]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0153, 0.7150, 0.0681, 0.2017]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0191, 0.7043, 0.1336, 0.1430]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0360, 0.6718, 0.1857, 0.1065]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0168, 0.6997, 0.1283, 0.1552]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0595, 0.4326, 0.3744, 0.1334]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0260, 0.5345, 0.1099, 0.3295]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0416, 0.4727, 0.1134, 0.3723]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0399, 0.7494, 0.1220, 0.0886]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0950, 0.3664, 0.1992, 0.3394]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0641, 0.4299, 0.2522, 0.2538]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0490, 0.5777, 0.2180, 0.1553]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0272, 0.7290, 0.1932, 0.0505]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0246, 0.7245, 0.1277, 0.1232]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0160, 0.7069, 0.1119, 0.1651]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0212, 0.7989, 0.0826, 0.0973]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0430, 0.4987, 0.0874, 0.3709]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0883, 0.3050, 0.3075, 0.2992]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0395, 0.4585, 0.3389, 0.1632]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0542, 0.6977, 0.1338, 0.1143]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0208, 0.5292, 0.3036, 0.1463]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0308, 0.6018, 0.1973, 0.1701]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0356, 0.4138, 0.2550, 0.2956]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0399, 0.7438, 0.1608, 0.0554]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0123, 0.7948, 0.0740, 0.1188]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0271, 0.6412, 0.1489, 0.1828]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0139, 0.6096, 0.1977, 0.1787]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.1011, 0.4913, 0.2268, 0.1809]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0181, 0.5079, 0.2396, 0.2343]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0774, 0.6257, 0.2210, 0.0759]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0322, 0.7423, 0.1845, 0.0410]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0283, 0.6851, 0.1715, 0.1152]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0321, 0.6555, 0.2300, 0.0824]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0453, 0.7556, 0.1260, 0.0731]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0223, 0.7595, 0.1791, 0.0391]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0106, 0.6907, 0.1747, 0.1240]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0259, 0.4806, 0.2790, 0.2145]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0206, 0.4535, 0.3069, 0.2190]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0132, 0.8223, 0.0989, 0.0656]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0338, 0.6440, 0.1737, 0.1485]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0259, 0.7727, 0.1289, 0.0725]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0471, 0.4787, 0.2751, 0.1991]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0262, 0.5858, 0.2195, 0.1685]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0212, 0.6832, 0.1836, 0.1120]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0537, 0.3322, 0.3006, 0.3134]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0375, 0.8021, 0.1092, 0.0513]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0159, 0.8092, 0.0905, 0.0844]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0462, 0.5966, 0.2638, 0.0934]], grad_fn=<ExpBackward>)\n",
      "n_step_rewards.shape:  (121,)\n",
      "rewards.shape:  (121,)\n",
      "n_step_rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0.]\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0.]\n",
      "bootstrap:  [False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      "  True]\n",
      "done.shape: (before n_steps) (121,)\n",
      "done: (before n_steps) [False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False]\n",
      "done.shape: (after n_steps) (121,)\n",
      "Gamma_V.shape:  (121,)\n",
      "done: (after n_steps) [False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False]\n",
      "Gamma_V:  [0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.67572905 0.6825546\n",
      " 0.68944909 0.69641322 0.70344769 0.71055323 0.71773053 0.72498034\n",
      " 0.73230337 0.73970037 0.74717209 0.75471929 0.76234271 0.77004315\n",
      " 0.77782136 0.78567814 0.79361428 0.80163059 0.80972787 0.81790694\n",
      " 0.82616862 0.83451376 0.84294319 0.85145777 0.86005835 0.86874581\n",
      " 0.87752102 0.88638487 0.89533825 0.90438208 0.91351725 0.92274469\n",
      " 0.93206535 0.94148015 0.95099005 0.96059601 0.970299   0.9801\n",
      " 0.99      ]\n",
      "old_states.shape:  torch.Size([121, 1, 9, 9])\n",
      "new_states.shape:  torch.Size([121, 1, 9, 9])\n",
      "log_probs:  [tensor(-0.6076, grad_fn=<SelectBackward>), tensor(-2.2884, grad_fn=<SelectBackward>), tensor(-0.4864, grad_fn=<SelectBackward>), tensor(-0.1238, grad_fn=<SelectBackward>), tensor(-0.8593, grad_fn=<SelectBackward>), tensor(-0.4306, grad_fn=<SelectBackward>), tensor(-0.2591, grad_fn=<SelectBackward>), tensor(-1.0766, grad_fn=<SelectBackward>), tensor(-0.3543, grad_fn=<SelectBackward>), tensor(-0.4473, grad_fn=<SelectBackward>), tensor(-1.9164, grad_fn=<SelectBackward>), tensor(-0.6214, grad_fn=<SelectBackward>), tensor(-0.7440, grad_fn=<SelectBackward>), tensor(-0.4796, grad_fn=<SelectBackward>), tensor(-0.8954, grad_fn=<SelectBackward>), tensor(-0.8243, grad_fn=<SelectBackward>), tensor(-0.3176, grad_fn=<SelectBackward>), tensor(-1.3059, grad_fn=<SelectBackward>), tensor(-0.5997, grad_fn=<SelectBackward>), tensor(-1.0458, grad_fn=<SelectBackward>), tensor(-0.3982, grad_fn=<SelectBackward>), tensor(-1.4320, grad_fn=<SelectBackward>), tensor(-0.9056, grad_fn=<SelectBackward>), tensor(-2.7698, grad_fn=<SelectBackward>), tensor(-1.0864, grad_fn=<SelectBackward>), tensor(-0.8243, grad_fn=<SelectBackward>), tensor(-0.6580, grad_fn=<SelectBackward>), tensor(-1.6511, grad_fn=<SelectBackward>), tensor(-0.7243, grad_fn=<SelectBackward>), tensor(-0.2994, grad_fn=<SelectBackward>), tensor(-0.9768, grad_fn=<SelectBackward>), tensor(-0.8193, grad_fn=<SelectBackward>), tensor(-1.3743, grad_fn=<SelectBackward>), tensor(-0.2937, grad_fn=<SelectBackward>), tensor(-1.2704, grad_fn=<SelectBackward>), tensor(-2.2446, grad_fn=<SelectBackward>), tensor(-0.2449, grad_fn=<SelectBackward>), tensor(-0.1978, grad_fn=<SelectBackward>), tensor(-0.1899, grad_fn=<SelectBackward>), tensor(-1.5521, grad_fn=<SelectBackward>), tensor(-0.7581, grad_fn=<SelectBackward>), tensor(-1.4898, grad_fn=<SelectBackward>), tensor(-1.4318, grad_fn=<SelectBackward>), tensor(-0.6602, grad_fn=<SelectBackward>), tensor(-1.4476, grad_fn=<SelectBackward>), tensor(-1.0764, grad_fn=<SelectBackward>), tensor(-0.9681, grad_fn=<SelectBackward>), tensor(-1.5535, grad_fn=<SelectBackward>), tensor(-0.6459, grad_fn=<SelectBackward>), tensor(-0.5247, grad_fn=<SelectBackward>), tensor(-1.0429, grad_fn=<SelectBackward>), tensor(-1.2432, grad_fn=<SelectBackward>), tensor(-1.7815, grad_fn=<SelectBackward>), tensor(-0.3043, grad_fn=<SelectBackward>), tensor(-0.6725, grad_fn=<SelectBackward>), tensor(-0.5790, grad_fn=<SelectBackward>), tensor(-0.3861, grad_fn=<SelectBackward>), tensor(-0.1637, grad_fn=<SelectBackward>), tensor(-1.7637, grad_fn=<SelectBackward>), tensor(-0.0819, grad_fn=<SelectBackward>), tensor(-1.0658, grad_fn=<SelectBackward>), tensor(-0.6747, grad_fn=<SelectBackward>), tensor(-1.1700, grad_fn=<SelectBackward>), tensor(-1.0936, grad_fn=<SelectBackward>), tensor(-0.8356, grad_fn=<SelectBackward>), tensor(-0.3850, grad_fn=<SelectBackward>), tensor(-0.5634, grad_fn=<SelectBackward>), tensor(-0.7474, grad_fn=<SelectBackward>), tensor(-2.2012, grad_fn=<SelectBackward>), tensor(-0.2032, grad_fn=<SelectBackward>), tensor(-0.9013, grad_fn=<SelectBackward>), tensor(-1.0208, grad_fn=<SelectBackward>), tensor(-0.8139, grad_fn=<SelectBackward>), tensor(-1.9756, grad_fn=<SelectBackward>), tensor(-0.3355, grad_fn=<SelectBackward>), tensor(-0.3505, grad_fn=<SelectBackward>), tensor(-0.3978, grad_fn=<SelectBackward>), tensor(-1.8633, grad_fn=<SelectBackward>), tensor(-0.9824, grad_fn=<SelectBackward>), tensor(-2.2081, grad_fn=<SelectBackward>), tensor(-0.9879, grad_fn=<SelectBackward>), tensor(-0.2884, grad_fn=<SelectBackward>), tensor(-1.6137, grad_fn=<SelectBackward>), tensor(-0.8443, grad_fn=<SelectBackward>), tensor(-1.5235, grad_fn=<SelectBackward>), tensor(-0.3160, grad_fn=<SelectBackward>), tensor(-2.0943, grad_fn=<SelectBackward>), tensor(-2.1898, grad_fn=<SelectBackward>), tensor(-0.2245, grad_fn=<SelectBackward>), tensor(-0.6958, grad_fn=<SelectBackward>), tensor(-1.2065, grad_fn=<SelectBackward>), tensor(-1.0821, grad_fn=<SelectBackward>), tensor(-0.3599, grad_fn=<SelectBackward>), tensor(-0.6363, grad_fn=<SelectBackward>), tensor(-0.5079, grad_fn=<SelectBackward>), tensor(-0.8823, grad_fn=<SelectBackward>), tensor(-0.2960, grad_fn=<SelectBackward>), tensor(-0.2296, grad_fn=<SelectBackward>), tensor(-0.4444, grad_fn=<SelectBackward>), tensor(-1.7222, grad_fn=<SelectBackward>), tensor(-0.7107, grad_fn=<SelectBackward>), tensor(-0.6775, grad_fn=<SelectBackward>), tensor(-1.5096, grad_fn=<SelectBackward>), tensor(-0.2980, grad_fn=<SelectBackward>), tensor(-1.7635, grad_fn=<SelectBackward>), tensor(-0.4224, grad_fn=<SelectBackward>), tensor(-0.2803, grad_fn=<SelectBackward>), tensor(-0.2751, grad_fn=<SelectBackward>), tensor(-1.7444, grad_fn=<SelectBackward>), tensor(-1.2766, grad_fn=<SelectBackward>), tensor(-1.5189, grad_fn=<SelectBackward>), tensor(-2.3132, grad_fn=<SelectBackward>), tensor(-0.4401, grad_fn=<SelectBackward>), tensor(-0.2578, grad_fn=<SelectBackward>), tensor(-1.6140, grad_fn=<SelectBackward>), tensor(-0.5348, grad_fn=<SelectBackward>), tensor(-0.3809, grad_fn=<SelectBackward>), tensor(-1.2018, grad_fn=<SelectBackward>), tensor(-0.2205, grad_fn=<SelectBackward>), tensor(-0.2117, grad_fn=<SelectBackward>), tensor(-0.5165, grad_fn=<SelectBackward>)]\n",
      "log_probs:  tensor([-0.6076, -2.2884, -0.4864, -0.1238, -0.8593, -0.4306, -0.2591, -1.0766,\n",
      "        -0.3543, -0.4473, -1.9164, -0.6214, -0.7440, -0.4796, -0.8954, -0.8243,\n",
      "        -0.3176, -1.3059, -0.5997, -1.0458, -0.3982, -1.4320, -0.9056, -2.7698,\n",
      "        -1.0864, -0.8243, -0.6580, -1.6511, -0.7243, -0.2994, -0.9768, -0.8193,\n",
      "        -1.3743, -0.2937, -1.2704, -2.2446, -0.2449, -0.1978, -0.1899, -1.5521,\n",
      "        -0.7581, -1.4898, -1.4318, -0.6602, -1.4476, -1.0764, -0.9681, -1.5535,\n",
      "        -0.6459, -0.5247, -1.0429, -1.2432, -1.7815, -0.3043, -0.6725, -0.5790,\n",
      "        -0.3861, -0.1637, -1.7637, -0.0819, -1.0658, -0.6747, -1.1700, -1.0936,\n",
      "        -0.8356, -0.3850, -0.5634, -0.7474, -2.2012, -0.2032, -0.9013, -1.0208,\n",
      "        -0.8139, -1.9756, -0.3355, -0.3505, -0.3978, -1.8633, -0.9824, -2.2081,\n",
      "        -0.9879, -0.2884, -1.6137, -0.8443, -1.5235, -0.3160, -2.0943, -2.1898,\n",
      "        -0.2245, -0.6958, -1.2065, -1.0821, -0.3599, -0.6363, -0.5079, -0.8823,\n",
      "        -0.2960, -0.2296, -0.4444, -1.7222, -0.7107, -0.6775, -1.5096, -0.2980,\n",
      "        -1.7635, -0.4224, -0.2803, -0.2751, -1.7444, -1.2766, -1.5189, -2.3132,\n",
      "        -0.4401, -0.2578, -1.6140, -0.5348, -0.3809, -1.2018, -0.2205, -0.2117,\n",
      "        -0.5165], grad_fn=<StackBackward>)\n",
      "distributions.shape:  torch.Size([1, 121, 4])\n",
      "distributions:  tensor([[[0.0426, 0.5446, 0.2585, 0.1542],\n",
      "         [0.0466, 0.8114, 0.1014, 0.0405],\n",
      "         [0.0326, 0.6148, 0.1379, 0.2147],\n",
      "         [0.0070, 0.8835, 0.0915, 0.0180],\n",
      "         [0.0378, 0.2672, 0.2715, 0.4235],\n",
      "         [0.0427, 0.6501, 0.1413, 0.1659],\n",
      "         [0.0196, 0.7718, 0.1558, 0.0528],\n",
      "         [0.1932, 0.3407, 0.1921, 0.2740],\n",
      "         [0.0183, 0.7017, 0.1962, 0.0839],\n",
      "         [0.0879, 0.6394, 0.1397, 0.1330],\n",
      "         [0.0266, 0.6434, 0.1471, 0.1829],\n",
      "         [0.0489, 0.5372, 0.2785, 0.1354],\n",
      "         [0.0956, 0.4752, 0.2799, 0.1493],\n",
      "         [0.0305, 0.6190, 0.2085, 0.1420],\n",
      "         [0.0895, 0.3522, 0.4085, 0.1499],\n",
      "         [0.0397, 0.4385, 0.2790, 0.2428],\n",
      "         [0.0301, 0.7279, 0.1502, 0.0919],\n",
      "         [0.0645, 0.3508, 0.3137, 0.2709],\n",
      "         [0.0498, 0.5490, 0.2618, 0.1394],\n",
      "         [0.0700, 0.3514, 0.3267, 0.2519],\n",
      "         [0.0441, 0.6715, 0.1800, 0.1044],\n",
      "         [0.0348, 0.3277, 0.2388, 0.3987],\n",
      "         [0.1605, 0.2424, 0.1928, 0.4043],\n",
      "         [0.0300, 0.7256, 0.1817, 0.0627],\n",
      "         [0.0611, 0.3374, 0.2281, 0.3734],\n",
      "         [0.0383, 0.4385, 0.1673, 0.3559],\n",
      "         [0.0290, 0.5179, 0.3102, 0.1429],\n",
      "         [0.0109, 0.6544, 0.1918, 0.1429],\n",
      "         [0.0983, 0.4847, 0.2709, 0.1461],\n",
      "         [0.0134, 0.7413, 0.1462, 0.0992],\n",
      "         [0.1694, 0.3765, 0.1758, 0.2783],\n",
      "         [0.0593, 0.2778, 0.2222, 0.4407],\n",
      "         [0.0453, 0.5969, 0.2530, 0.1048],\n",
      "         [0.0217, 0.7455, 0.1221, 0.1107],\n",
      "         [0.0645, 0.4431, 0.2117, 0.2807],\n",
      "         [0.0405, 0.7672, 0.0864, 0.1060],\n",
      "         [0.0536, 0.7828, 0.1249, 0.0386],\n",
      "         [0.0264, 0.8205, 0.1204, 0.0327],\n",
      "         [0.0341, 0.8271, 0.0836, 0.0552],\n",
      "         [0.0228, 0.6949, 0.2118, 0.0704],\n",
      "         [0.1340, 0.2209, 0.1765, 0.4685],\n",
      "         [0.1137, 0.4871, 0.2254, 0.1738],\n",
      "         [0.0182, 0.4681, 0.2748, 0.2389],\n",
      "         [0.0562, 0.5167, 0.3277, 0.0994],\n",
      "         [0.0452, 0.5357, 0.1840, 0.2351],\n",
      "         [0.0928, 0.4488, 0.1176, 0.3408],\n",
      "         [0.0583, 0.4855, 0.0764, 0.3798],\n",
      "         [0.0604, 0.5560, 0.1720, 0.2115],\n",
      "         [0.0432, 0.5242, 0.2166, 0.2160],\n",
      "         [0.0279, 0.5918, 0.1557, 0.2246],\n",
      "         [0.0301, 0.5018, 0.3524, 0.1156],\n",
      "         [0.1086, 0.2885, 0.3497, 0.2532],\n",
      "         [0.0113, 0.6875, 0.1328, 0.1684],\n",
      "         [0.0392, 0.7377, 0.1343, 0.0888],\n",
      "         [0.0341, 0.2778, 0.1776, 0.5104],\n",
      "         [0.0469, 0.5605, 0.1711, 0.2215],\n",
      "         [0.0186, 0.6797, 0.2195, 0.0822],\n",
      "         [0.0299, 0.8490, 0.0796, 0.0415],\n",
      "         [0.0896, 0.5113, 0.1714, 0.2277],\n",
      "         [0.0128, 0.9213, 0.0500, 0.0158],\n",
      "         [0.0607, 0.4066, 0.3445, 0.1882],\n",
      "         [0.0277, 0.3094, 0.1535, 0.5093],\n",
      "         [0.0315, 0.3318, 0.3104, 0.3264],\n",
      "         [0.1079, 0.2803, 0.2767, 0.3350],\n",
      "         [0.0359, 0.4336, 0.2244, 0.3061],\n",
      "         [0.0193, 0.6805, 0.1634, 0.1368],\n",
      "         [0.0323, 0.5692, 0.2334, 0.1650],\n",
      "         [0.0682, 0.4736, 0.1994, 0.2588],\n",
      "         [0.1107, 0.7116, 0.1176, 0.0602],\n",
      "         [0.0176, 0.8162, 0.1123, 0.0539],\n",
      "         [0.0483, 0.4060, 0.3413, 0.2044],\n",
      "         [0.0703, 0.4075, 0.3603, 0.1619],\n",
      "         [0.0809, 0.3389, 0.1370, 0.4431],\n",
      "         [0.0577, 0.5383, 0.2654, 0.1387],\n",
      "         [0.0153, 0.7150, 0.0681, 0.2017],\n",
      "         [0.0191, 0.7043, 0.1336, 0.1430],\n",
      "         [0.0360, 0.6718, 0.1857, 0.1065],\n",
      "         [0.0168, 0.6997, 0.1283, 0.1552],\n",
      "         [0.0595, 0.4326, 0.3744, 0.1334],\n",
      "         [0.0260, 0.5345, 0.1099, 0.3295],\n",
      "         [0.0416, 0.4727, 0.1134, 0.3723],\n",
      "         [0.0399, 0.7494, 0.1220, 0.0886],\n",
      "         [0.0950, 0.3664, 0.1992, 0.3394],\n",
      "         [0.0641, 0.4299, 0.2522, 0.2538],\n",
      "         [0.0490, 0.5777, 0.2180, 0.1553],\n",
      "         [0.0272, 0.7290, 0.1932, 0.0505],\n",
      "         [0.0246, 0.7245, 0.1277, 0.1232],\n",
      "         [0.0160, 0.7069, 0.1119, 0.1651],\n",
      "         [0.0212, 0.7989, 0.0826, 0.0973],\n",
      "         [0.0430, 0.4987, 0.0874, 0.3709],\n",
      "         [0.0883, 0.3050, 0.3075, 0.2992],\n",
      "         [0.0395, 0.4585, 0.3389, 0.1632],\n",
      "         [0.0542, 0.6977, 0.1338, 0.1143],\n",
      "         [0.0208, 0.5292, 0.3036, 0.1463],\n",
      "         [0.0308, 0.6018, 0.1973, 0.1701],\n",
      "         [0.0356, 0.4138, 0.2550, 0.2956],\n",
      "         [0.0399, 0.7438, 0.1608, 0.0554],\n",
      "         [0.0123, 0.7948, 0.0740, 0.1188],\n",
      "         [0.0271, 0.6412, 0.1489, 0.1828],\n",
      "         [0.0139, 0.6096, 0.1977, 0.1787],\n",
      "         [0.1011, 0.4913, 0.2268, 0.1809],\n",
      "         [0.0181, 0.5079, 0.2396, 0.2343],\n",
      "         [0.0774, 0.6257, 0.2210, 0.0759],\n",
      "         [0.0322, 0.7423, 0.1845, 0.0410],\n",
      "         [0.0283, 0.6851, 0.1715, 0.1152],\n",
      "         [0.0321, 0.6555, 0.2300, 0.0824],\n",
      "         [0.0453, 0.7556, 0.1260, 0.0731],\n",
      "         [0.0223, 0.7595, 0.1791, 0.0391],\n",
      "         [0.0106, 0.6907, 0.1747, 0.1240],\n",
      "         [0.0259, 0.4806, 0.2790, 0.2145],\n",
      "         [0.0206, 0.4535, 0.3069, 0.2190],\n",
      "         [0.0132, 0.8223, 0.0989, 0.0656],\n",
      "         [0.0338, 0.6440, 0.1737, 0.1485],\n",
      "         [0.0259, 0.7727, 0.1289, 0.0725],\n",
      "         [0.0471, 0.4787, 0.2751, 0.1991],\n",
      "         [0.0262, 0.5858, 0.2195, 0.1685],\n",
      "         [0.0212, 0.6832, 0.1836, 0.1120],\n",
      "         [0.0537, 0.3322, 0.3006, 0.3134],\n",
      "         [0.0375, 0.8021, 0.1092, 0.0513],\n",
      "         [0.0159, 0.8092, 0.0905, 0.0844],\n",
      "         [0.0462, 0.5966, 0.2638, 0.0934]]], grad_fn=<StackBackward>)\n",
      "Updating critic...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V_trg.shape (after critic):  torch.Size([121])\n",
      "V_trg.shape (after sum):  torch.Size([121])\n",
      "V_trg.shape (after squeeze):  torch.Size([121])\n",
      "V_trg.shape (after squeeze):  tensor([ 0.2570,  0.1100, -0.0914,  0.2078, -0.2322,  0.0607,  0.0584, -0.1715,\n",
      "         0.4650,  0.1238, -0.2338, -0.1211,  0.7462,  0.1060,  0.1926,  0.0347,\n",
      "         0.2076,  0.3013, -0.0686,  0.3416, -0.2913, -0.1524,  0.2432,  0.4209,\n",
      "         0.2129, -0.1509,  0.3038, -0.0232,  0.0351, -0.3811,  0.4732,  0.0496,\n",
      "         0.2777, -0.0486,  0.3886,  0.2561,  0.5540,  0.0395,  0.0224,  0.3976,\n",
      "         0.1485,  0.1888, -0.4534,  0.4603,  0.5567,  0.0235, -0.0297,  0.5398,\n",
      "         0.3063,  0.1360, -0.1075,  0.2464, -0.2688,  0.4652,  0.3323,  0.6300,\n",
      "         0.2728,  0.1424, -0.2156,  0.2677,  0.1033,  0.4736, -0.1904, -0.0161,\n",
      "         0.0779, -0.0126,  0.2039, -0.0159,  0.1423,  0.3091,  0.2807, -0.0091,\n",
      "        -0.1826,  0.1870,  0.4205, -0.0306,  0.6309,  0.3488,  0.1520, -0.0734,\n",
      "        -0.1420,  0.0310,  0.5066,  0.1286,  0.0219, -0.0731,  0.2384, -0.4523,\n",
      "        -0.1724,  0.3047, -0.0888,  0.4611, -0.0988, -0.3058,  0.4674,  0.3337,\n",
      "        -0.2574, -0.0666,  0.1610,  0.5257,  0.3655,  0.1790,  0.3910,  0.3625,\n",
      "         0.8306, -0.5323,  0.4047,  0.3872,  0.4988, -0.2126,  0.6444,  0.4045,\n",
      "         0.3590, -0.0033,  0.2157, -0.0911,  0.0645,  0.6525, -0.0361,  0.5127,\n",
      "         0.0444])\n",
      "V1.shape:  torch.Size([121])\n",
      "V1:  tensor([[1.7396],\n",
      "        [0.1469],\n",
      "        [1.1436],\n",
      "        [0.7735],\n",
      "        [2.1460],\n",
      "        [1.3790],\n",
      "        [2.3315],\n",
      "        [0.4862],\n",
      "        [1.3306],\n",
      "        [2.1255],\n",
      "        [1.5059],\n",
      "        [1.9381],\n",
      "        [1.6899],\n",
      "        [1.6511],\n",
      "        [0.7076],\n",
      "        [2.2581],\n",
      "        [0.9551],\n",
      "        [2.5731],\n",
      "        [0.2309],\n",
      "        [1.9368],\n",
      "        [1.2277],\n",
      "        [0.9195],\n",
      "        [2.4235],\n",
      "        [1.7749],\n",
      "        [1.1160],\n",
      "        [1.5315],\n",
      "        [0.7920],\n",
      "        [3.0645],\n",
      "        [0.7804],\n",
      "        [1.0758],\n",
      "        [1.4797],\n",
      "        [1.7403],\n",
      "        [1.7128],\n",
      "        [1.0755],\n",
      "        [1.5299],\n",
      "        [2.6329],\n",
      "        [2.1528],\n",
      "        [1.3935],\n",
      "        [1.1423],\n",
      "        [1.8403],\n",
      "        [0.4933],\n",
      "        [2.4603],\n",
      "        [2.2827],\n",
      "        [1.2312],\n",
      "        [0.8278],\n",
      "        [2.6252],\n",
      "        [1.3139],\n",
      "        [1.5538],\n",
      "        [1.2355],\n",
      "        [1.0713],\n",
      "        [1.2172],\n",
      "        [1.6004],\n",
      "        [2.0719],\n",
      "        [1.5238],\n",
      "        [1.2101],\n",
      "        [2.2595],\n",
      "        [1.4143],\n",
      "        [1.8657],\n",
      "        [2.7243],\n",
      "        [0.2798],\n",
      "        [2.2294],\n",
      "        [0.0646],\n",
      "        [1.7043],\n",
      "        [1.2246],\n",
      "        [2.1814],\n",
      "        [1.2662],\n",
      "        [1.1205],\n",
      "        [2.1933],\n",
      "        [1.6676],\n",
      "        [0.7378],\n",
      "        [0.5241],\n",
      "        [1.4092],\n",
      "        [1.4051],\n",
      "        [0.7545],\n",
      "        [1.9216],\n",
      "        [1.0895],\n",
      "        [0.8943],\n",
      "        [1.4903],\n",
      "        [0.8021],\n",
      "        [1.3892],\n",
      "        [1.6874],\n",
      "        [1.6021],\n",
      "        [0.4995],\n",
      "        [1.5847],\n",
      "        [1.7251],\n",
      "        [1.7940],\n",
      "        [1.0654],\n",
      "        [0.9822],\n",
      "        [1.9366],\n",
      "        [0.3658],\n",
      "        [1.7533],\n",
      "        [1.1173],\n",
      "        [1.0541],\n",
      "        [1.5383],\n",
      "        [1.3814],\n",
      "        [0.9335],\n",
      "        [1.4047],\n",
      "        [1.2368],\n",
      "        [1.4823],\n",
      "        [0.0592],\n",
      "        [1.7455],\n",
      "        [1.9287],\n",
      "        [1.6625],\n",
      "        [1.3800],\n",
      "        [2.4419],\n",
      "        [0.9361],\n",
      "        [0.3900],\n",
      "        [2.0741],\n",
      "        [0.6527],\n",
      "        [1.5880],\n",
      "        [2.3284],\n",
      "        [1.9257],\n",
      "        [1.8779],\n",
      "        [1.0234],\n",
      "        [0.6704],\n",
      "        [1.8870],\n",
      "        [2.1213],\n",
      "        [1.3641],\n",
      "        [1.4955],\n",
      "        [0.7397],\n",
      "        [1.7621]], grad_fn=<AddmmBackward>)\n",
      "Updating actor...\n",
      "V_trg.shape:  torch.Size([121])\n",
      "V_trg:  tensor([1.2020, 0.2786, 0.9802, 1.0648, 1.0842, 1.1609, 0.6171, 0.9213, 0.8650,\n",
      "        0.5194, 0.9329, 0.9801, 1.3038, 0.8419, 1.3949, 0.7326, 1.3124, 1.0911,\n",
      "        0.6327, 0.7201, 1.0822, 0.5627, 0.7094, 1.3193, 0.9534, 1.5945, 0.7272,\n",
      "        0.6810, 0.7589, 0.7996, 0.6660, 1.0125, 1.1983, 0.1471, 1.0511, 0.5648,\n",
      "        0.2588, 0.4613, 0.2384, 0.9060, 0.6017, 1.0268, 1.3140, 1.6247, 0.2195,\n",
      "        1.0957, 0.8999, 1.1137, 0.9852, 0.5495, 0.4585, 1.1394, 1.0548, 0.8236,\n",
      "        1.1614, 0.5325, 1.1944, 0.7295, 0.8481, 1.0516, 1.2082, 1.1195, 0.7718,\n",
      "        0.5455, 1.3003, 0.4297, 1.6047, 0.7604, 1.2179, 0.5488, 1.3220, 1.0200,\n",
      "        0.9991, 0.9049, 0.9598, 1.2951, 1.2135, 0.7298, 0.9060, 0.7181, 1.5369,\n",
      "        0.9581, 1.1215, 0.6003, 1.3779, 0.7183, 1.3481, 1.1412, 1.4832, 0.4354,\n",
      "        1.1349, 0.7200, 0.9917, 1.3085, 1.2149, 1.6818, 0.4671, 1.0126, 0.9625,\n",
      "        1.4599, 0.7151, 0.9630, 0.6565, 1.2315, 0.1292, 0.1759, 1.2783, 0.1455,\n",
      "        1.4819, 1.2413, 1.3935, 1.8396, 0.7507, 0.8936, 1.6597, 1.2984, 1.8244,\n",
      "        1.4486, 1.2498, 2.2126, 1.8981])\n",
      "V_pred.shape:  torch.Size([121])\n",
      "V_pred:  tensor([1.9576, 0.3797, 1.2448, 1.4886, 1.1421, 1.6529, 1.1595, 0.8538, 1.8097,\n",
      "        1.1117, 1.5575, 2.2391, 1.3916, 2.6867, 0.5550, 1.1388, 1.2506, 0.6655,\n",
      "        1.4051, 1.7045, 2.2675, 0.8219, 1.1509, 1.2500, 1.1349, 0.7751, 1.9567,\n",
      "        1.0443, 1.5569, 0.7206, 1.8743, 1.3159, 1.8489, 1.5611, 2.1349, 1.0037,\n",
      "        2.7011, 1.9726, 2.2530, 2.0857, 0.7582, 1.7685, 1.0863, 1.2886, 1.6053,\n",
      "        0.8154, 1.3440, 2.0950, 1.1031, 1.5661, 1.3812, 0.7123, 0.8625, 0.9716,\n",
      "        1.7819, 2.1339, 1.3075, 0.8526, 0.8712, 2.5036, 1.4065, 1.5068, 0.6846,\n",
      "        1.4202, 1.0623, 1.4242, 0.7366, 1.2531, 2.1955, 2.0640, 1.8124, 1.5562,\n",
      "        2.0734, 1.5917, 1.4961, 1.7409, 0.6069, 1.3948, 0.4280, 2.3792, 2.4346,\n",
      "        2.3487, 1.4839, 1.9162, 1.7859, 1.7695, 1.1733, 1.4052, 0.8891, 0.9825,\n",
      "        1.6825, 1.1886, 1.9573, 0.7736, 0.8267, 1.6594, 1.3689, 1.2789, 1.9417,\n",
      "        1.3689, 1.9292, 1.7134, 0.4607, 0.2441, 1.6786, 1.3241, 0.5941, 1.2546,\n",
      "        0.1949, 0.8255, 2.0077, 0.1508, 1.4388, 0.9242, 0.9859, 1.5790, 0.9314,\n",
      "        1.7289, 0.8560, 1.6637, 2.0833])\n",
      "A.shape:  torch.Size([121])\n",
      "A:  tensor([-0.7556, -0.1010, -0.2646, -0.4238, -0.0579, -0.4920, -0.5424,  0.0674,\n",
      "        -0.9447, -0.5923, -0.6246, -1.2590, -0.0878, -1.8448,  0.8399, -0.4062,\n",
      "         0.0619,  0.4256, -0.7724, -0.9844, -1.1853, -0.2592, -0.4415,  0.0692,\n",
      "        -0.1814,  0.8194, -1.2295, -0.3633, -0.7980,  0.0790, -1.2083, -0.3035,\n",
      "        -0.6506, -1.4140, -1.0838, -0.4389, -2.4423, -1.5113, -2.0146, -1.1797,\n",
      "        -0.1565, -0.7417,  0.2277,  0.3361, -1.3858,  0.2804, -0.4441, -0.9813,\n",
      "        -0.1178, -1.0166, -0.9228,  0.4271,  0.1924, -0.1480, -0.6205, -1.6014,\n",
      "        -0.1131, -0.1231, -0.0230, -1.4520, -0.1983, -0.3873,  0.0872, -0.8747,\n",
      "         0.2380, -0.9945,  0.8681, -0.4928, -0.9776, -1.5153, -0.4904, -0.5362,\n",
      "        -1.0743, -0.6868, -0.5363, -0.4458,  0.6066, -0.6650,  0.4780, -1.6611,\n",
      "        -0.8976, -1.3906, -0.3625, -1.3159, -0.4080, -1.0512,  0.1748, -0.2639,\n",
      "         0.5941, -0.5471, -0.5475, -0.4686, -0.9655,  0.5349,  0.3882,  0.0224,\n",
      "        -0.9018, -0.2663, -0.9792,  0.0910, -1.2141, -0.7504,  0.1959,  0.9874,\n",
      "        -1.5494, -1.1482,  0.6843, -1.1091,  1.2870,  0.4158, -0.6142,  1.6888,\n",
      "        -0.6881, -0.0306,  0.6737, -0.2805,  0.8929, -0.2803,  0.3939,  0.5489,\n",
      "        -0.1852])\n",
      "policy_gradient.shape:  torch.Size([121])\n",
      "policy_gradient:  tensor([-0.4591, -0.2312, -0.1287, -0.0525, -0.0497, -0.2118, -0.1405,  0.0726,\n",
      "        -0.3347, -0.2649, -1.1970, -0.7824, -0.0653, -0.8848,  0.7520, -0.3348,\n",
      "         0.0197,  0.5558, -0.4632, -1.0295, -0.4720, -0.3713, -0.3998,  0.1917,\n",
      "        -0.1971,  0.6754, -0.8090, -0.5999, -0.5780,  0.0237, -1.1803, -0.2486,\n",
      "        -0.8941, -0.4153, -1.3769, -0.9852, -0.5980, -0.2990, -0.3825, -1.8309,\n",
      "        -0.1186, -1.1049,  0.3260,  0.2219, -2.0061,  0.3018, -0.4300, -1.5244,\n",
      "        -0.0761, -0.5334, -0.9624,  0.5310,  0.3427, -0.0450, -0.4172, -0.9272,\n",
      "        -0.0437, -0.0201, -0.0406, -0.1190, -0.2113, -0.2613,  0.1020, -0.9566,\n",
      "         0.1989, -0.3828,  0.4891, -0.3683, -2.1519, -0.3078, -0.4420, -0.5474,\n",
      "        -0.8744, -1.3568, -0.1799, -0.1562,  0.2413, -1.2391,  0.4696, -3.6679,\n",
      "        -0.8868, -0.4011, -0.5849, -1.1110, -0.6216, -0.3322,  0.3661, -0.5779,\n",
      "         0.1334, -0.3807, -0.6606, -0.5070, -0.3475,  0.3404,  0.1972,  0.0197,\n",
      "        -0.2669, -0.0612, -0.4352,  0.1568, -0.8629, -0.5084,  0.2957,  0.2943,\n",
      "        -2.7323, -0.4850,  0.1918, -0.3051,  2.2451,  0.5308, -0.9328,  3.9065,\n",
      "        -0.3028, -0.0079,  1.0874, -0.1500,  0.3401, -0.3369,  0.0869,  0.1162,\n",
      "        -0.0957], grad_fn=<MulBackward0>)\n",
      "policy_grad:  tensor(-37.8118, grad_fn=<SumBackward0>)\n",
      "Negative entropy:  tensor(-122.0603, grad_fn=<SumBackward0>)\n",
      "Actor loss:  tensor(-39.0324, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distribution:  tensor([[0.0275, 0.5323, 0.3050, 0.1352]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0177, 0.8844, 0.0726, 0.0253]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0475, 0.6115, 0.1560, 0.1850]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0610, 0.4755, 0.3523, 0.1112]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0521, 0.7322, 0.0642, 0.1515]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0161, 0.8146, 0.1206, 0.0487]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0478, 0.4943, 0.0942, 0.3637]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0224, 0.5352, 0.1757, 0.2668]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0291, 0.7507, 0.0946, 0.1255]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0113, 0.6378, 0.1448, 0.2061]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0249, 0.8705, 0.0717, 0.0329]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0138, 0.8580, 0.0657, 0.0625]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0156, 0.8575, 0.0793, 0.0476]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0156, 0.8005, 0.0541, 0.1299]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0188, 0.3307, 0.3764, 0.2741]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0554, 0.4706, 0.2436, 0.2304]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0173, 0.8657, 0.0511, 0.0659]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0114, 0.5194, 0.2022, 0.2671]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0253, 0.6826, 0.1410, 0.1510]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0261, 0.7831, 0.1092, 0.0816]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0168, 0.7508, 0.1342, 0.0982]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0442, 0.4250, 0.3062, 0.2246]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0234, 0.4927, 0.3724, 0.1115]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0232, 0.6755, 0.1636, 0.1377]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0109, 0.7593, 0.0744, 0.1553]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0066, 0.8592, 0.0888, 0.0454]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0373, 0.6818, 0.1703, 0.1107]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.1356, 0.5229, 0.1413, 0.2003]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0603, 0.6217, 0.1288, 0.1892]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0435, 0.6467, 0.2008, 0.1090]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0096, 0.8269, 0.0761, 0.0873]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0204, 0.4512, 0.1306, 0.3978]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0179, 0.7701, 0.1338, 0.0782]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0262, 0.5356, 0.0824, 0.3557]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0581, 0.7544, 0.0859, 0.1016]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0505, 0.7287, 0.1402, 0.0806]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0201, 0.6198, 0.1929, 0.1672]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0150, 0.7185, 0.1023, 0.1641]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0248, 0.6079, 0.1180, 0.2493]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0183, 0.8090, 0.0938, 0.0789]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0471, 0.7562, 0.0961, 0.1007]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0183, 0.8673, 0.0615, 0.0530]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0420, 0.7152, 0.1172, 0.1255]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0423, 0.5139, 0.2447, 0.1991]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0221, 0.8269, 0.1037, 0.0473]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0120, 0.7451, 0.1273, 0.1156]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0220, 0.8668, 0.0576, 0.0536]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0142, 0.6976, 0.1489, 0.1393]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0218, 0.5570, 0.0792, 0.3419]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0298, 0.6827, 0.1560, 0.1315]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0236, 0.7082, 0.2311, 0.0370]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0229, 0.8279, 0.0906, 0.0585]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0500, 0.4201, 0.1745, 0.3554]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0187, 0.6669, 0.1527, 0.1617]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0161, 0.6528, 0.1855, 0.1455]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0655, 0.6392, 0.1297, 0.1657]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0117, 0.8847, 0.0650, 0.0386]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0330, 0.4352, 0.1338, 0.3980]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0338, 0.4849, 0.2477, 0.2336]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0717, 0.3015, 0.3542, 0.2726]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0284, 0.7304, 0.1046, 0.1366]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0438, 0.2371, 0.1194, 0.5998]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0353, 0.3976, 0.3054, 0.2617]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0489, 0.5958, 0.1348, 0.2204]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0406, 0.2755, 0.1879, 0.4960]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0270, 0.4559, 0.1452, 0.3719]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0236, 0.7520, 0.0661, 0.1582]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0239, 0.7701, 0.0962, 0.1098]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0395, 0.5288, 0.1719, 0.2599]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0198, 0.6694, 0.1902, 0.1207]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0190, 0.6373, 0.1888, 0.1549]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0293, 0.4587, 0.1499, 0.3621]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0205, 0.2808, 0.4377, 0.2610]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0268, 0.5297, 0.2319, 0.2116]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0235, 0.5940, 0.1840, 0.1985]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0289, 0.5086, 0.2882, 0.1743]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0244, 0.5292, 0.2274, 0.2190]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0151, 0.7501, 0.1735, 0.0614]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0470, 0.4908, 0.2513, 0.2109]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0139, 0.7972, 0.0974, 0.0915]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0430, 0.3281, 0.3583, 0.2706]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0526, 0.4233, 0.2299, 0.2942]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0892, 0.4992, 0.2675, 0.1441]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0212, 0.7051, 0.1104, 0.1632]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0107, 0.8516, 0.1045, 0.0331]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0547, 0.3725, 0.1397, 0.4330]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0440, 0.3792, 0.3269, 0.2499]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0191, 0.6938, 0.1850, 0.1021]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0302, 0.5859, 0.1500, 0.2338]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0262, 0.7593, 0.0796, 0.1348]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0085, 0.8356, 0.0505, 0.1054]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0669, 0.3623, 0.2459, 0.3249]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0110, 0.7092, 0.1543, 0.1255]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0193, 0.6668, 0.1777, 0.1362]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0855, 0.6205, 0.1606, 0.1335]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0193, 0.4312, 0.3386, 0.2109]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0234, 0.7332, 0.1609, 0.0825]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0189, 0.8343, 0.0306, 0.1162]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0122, 0.7386, 0.0680, 0.1812]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0363, 0.3802, 0.2462, 0.3374]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0300, 0.4190, 0.1985, 0.3526]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0052, 0.7171, 0.1825, 0.0953]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0723, 0.6726, 0.1077, 0.1474]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0597, 0.5404, 0.1442, 0.2557]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0124, 0.6767, 0.1774, 0.1335]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0385, 0.6033, 0.2257, 0.1325]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0039, 0.9658, 0.0070, 0.0232]], grad_fn=<ExpBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distribution:  tensor([[0.0366, 0.4295, 0.2484, 0.2855]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0099, 0.7427, 0.1618, 0.0856]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0380, 0.6354, 0.1894, 0.1372]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0355, 0.4774, 0.0676, 0.4194]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0930, 0.3316, 0.2812, 0.2942]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0124, 0.9362, 0.0373, 0.0142]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0530, 0.6083, 0.1251, 0.2135]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0281, 0.5910, 0.1763, 0.2046]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0385, 0.2447, 0.2426, 0.4742]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0278, 0.3296, 0.3508, 0.2917]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0382, 0.4456, 0.1271, 0.3892]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0153, 0.8477, 0.0703, 0.0667]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0194, 0.8613, 0.0607, 0.0586]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0409, 0.6311, 0.0593, 0.2687]], grad_fn=<ExpBackward>)\n",
      "n_step_rewards.shape:  (121,)\n",
      "rewards.shape:  (121,)\n",
      "n_step_rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0.]\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0.]\n",
      "bootstrap:  [False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      "  True]\n",
      "done.shape: (before n_steps) (121,)\n",
      "done: (before n_steps) [False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False]\n",
      "done.shape: (after n_steps) (121,)\n",
      "Gamma_V.shape:  (121,)\n",
      "done: (after n_steps) [False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False]\n",
      "Gamma_V:  [0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.67572905 0.6825546\n",
      " 0.68944909 0.69641322 0.70344769 0.71055323 0.71773053 0.72498034\n",
      " 0.73230337 0.73970037 0.74717209 0.75471929 0.76234271 0.77004315\n",
      " 0.77782136 0.78567814 0.79361428 0.80163059 0.80972787 0.81790694\n",
      " 0.82616862 0.83451376 0.84294319 0.85145777 0.86005835 0.86874581\n",
      " 0.87752102 0.88638487 0.89533825 0.90438208 0.91351725 0.92274469\n",
      " 0.93206535 0.94148015 0.95099005 0.96059601 0.970299   0.9801\n",
      " 0.99      ]\n",
      "old_states.shape:  torch.Size([121, 1, 9, 9])\n",
      "new_states.shape:  torch.Size([121, 1, 9, 9])\n",
      "log_probs:  [tensor(-1.1875, grad_fn=<SelectBackward>), tensor(-0.1228, grad_fn=<SelectBackward>), tensor(-0.4918, grad_fn=<SelectBackward>), tensor(-0.7434, grad_fn=<SelectBackward>), tensor(-2.9538, grad_fn=<SelectBackward>), tensor(-2.1149, grad_fn=<SelectBackward>), tensor(-1.0114, grad_fn=<SelectBackward>), tensor(-0.6251, grad_fn=<SelectBackward>), tensor(-0.2867, grad_fn=<SelectBackward>), tensor(-0.4497, grad_fn=<SelectBackward>), tensor(-0.1387, grad_fn=<SelectBackward>), tensor(-0.1532, grad_fn=<SelectBackward>), tensor(-0.1537, grad_fn=<SelectBackward>), tensor(-2.0413, grad_fn=<SelectBackward>), tensor(-0.9770, grad_fn=<SelectBackward>), tensor(-1.4681, grad_fn=<SelectBackward>), tensor(-0.1442, grad_fn=<SelectBackward>), tensor(-0.6551, grad_fn=<SelectBackward>), tensor(-0.3818, grad_fn=<SelectBackward>), tensor(-0.2445, grad_fn=<SelectBackward>), tensor(-0.2867, grad_fn=<SelectBackward>), tensor(-1.4935, grad_fn=<SelectBackward>), tensor(-0.7079, grad_fn=<SelectBackward>), tensor(-0.3923, grad_fn=<SelectBackward>), tensor(-0.2753, grad_fn=<SelectBackward>), tensor(-0.1517, grad_fn=<SelectBackward>), tensor(-1.7700, grad_fn=<SelectBackward>), tensor(-1.9569, grad_fn=<SelectBackward>), tensor(-0.4753, grad_fn=<SelectBackward>), tensor(-3.1354, grad_fn=<SelectBackward>), tensor(-0.1900, grad_fn=<SelectBackward>), tensor(-0.7958, grad_fn=<SelectBackward>), tensor(-0.2612, grad_fn=<SelectBackward>), tensor(-2.4956, grad_fn=<SelectBackward>), tensor(-0.2818, grad_fn=<SelectBackward>), tensor(-2.5179, grad_fn=<SelectBackward>), tensor(-0.4784, grad_fn=<SelectBackward>), tensor(-0.3305, grad_fn=<SelectBackward>), tensor(-0.4978, grad_fn=<SelectBackward>), tensor(-0.2119, grad_fn=<SelectBackward>), tensor(-0.2794, grad_fn=<SelectBackward>), tensor(-0.1424, grad_fn=<SelectBackward>), tensor(-0.3352, grad_fn=<SelectBackward>), tensor(-0.6657, grad_fn=<SelectBackward>), tensor(-0.1900, grad_fn=<SelectBackward>), tensor(-0.2943, grad_fn=<SelectBackward>), tensor(-0.1430, grad_fn=<SelectBackward>), tensor(-0.3600, grad_fn=<SelectBackward>), tensor(-0.5851, grad_fn=<SelectBackward>), tensor(-0.3817, grad_fn=<SelectBackward>), tensor(-1.4647, grad_fn=<SelectBackward>), tensor(-0.1888, grad_fn=<SelectBackward>), tensor(-0.8672, grad_fn=<SelectBackward>), tensor(-3.9785, grad_fn=<SelectBackward>), tensor(-1.6847, grad_fn=<SelectBackward>), tensor(-0.4476, grad_fn=<SelectBackward>), tensor(-0.1225, grad_fn=<SelectBackward>), tensor(-0.8320, grad_fn=<SelectBackward>), tensor(-1.3957, grad_fn=<SelectBackward>), tensor(-1.2996, grad_fn=<SelectBackward>), tensor(-0.3141, grad_fn=<SelectBackward>), tensor(-0.5112, grad_fn=<SelectBackward>), tensor(-0.9223, grad_fn=<SelectBackward>), tensor(-1.5123, grad_fn=<SelectBackward>), tensor(-0.7012, grad_fn=<SelectBackward>), tensor(-0.7855, grad_fn=<SelectBackward>), tensor(-0.2850, grad_fn=<SelectBackward>), tensor(-0.2612, grad_fn=<SelectBackward>), tensor(-3.2318, grad_fn=<SelectBackward>), tensor(-0.4014, grad_fn=<SelectBackward>), tensor(-0.4505, grad_fn=<SelectBackward>), tensor(-3.5309, grad_fn=<SelectBackward>), tensor(-1.2700, grad_fn=<SelectBackward>), tensor(-1.5530, grad_fn=<SelectBackward>), tensor(-0.5209, grad_fn=<SelectBackward>), tensor(-0.6762, grad_fn=<SelectBackward>), tensor(-0.6364, grad_fn=<SelectBackward>), tensor(-0.2876, grad_fn=<SelectBackward>), tensor(-1.5564, grad_fn=<SelectBackward>), tensor(-0.2267, grad_fn=<SelectBackward>), tensor(-3.1473, grad_fn=<SelectBackward>), tensor(-2.9457, grad_fn=<SelectBackward>), tensor(-1.3185, grad_fn=<SelectBackward>), tensor(-1.8128, grad_fn=<SelectBackward>), tensor(-0.1606, grad_fn=<SelectBackward>), tensor(-0.8369, grad_fn=<SelectBackward>), tensor(-0.9697, grad_fn=<SelectBackward>), tensor(-0.3655, grad_fn=<SelectBackward>), tensor(-1.4531, grad_fn=<SelectBackward>), tensor(-0.2754, grad_fn=<SelectBackward>), tensor(-0.1796, grad_fn=<SelectBackward>), tensor(-1.1244, grad_fn=<SelectBackward>), tensor(-0.3436, grad_fn=<SelectBackward>), tensor(-1.9938, grad_fn=<SelectBackward>), tensor(-0.4772, grad_fn=<SelectBackward>), tensor(-0.8411, grad_fn=<SelectBackward>), tensor(-0.3104, grad_fn=<SelectBackward>), tensor(-0.1811, grad_fn=<SelectBackward>), tensor(-0.3029, grad_fn=<SelectBackward>), tensor(-3.3168, grad_fn=<SelectBackward>), tensor(-0.8700, grad_fn=<SelectBackward>), tensor(-1.7010, grad_fn=<SelectBackward>), tensor(-0.3967, grad_fn=<SelectBackward>), tensor(-0.6155, grad_fn=<SelectBackward>), tensor(-1.7291, grad_fn=<SelectBackward>), tensor(-1.4886, grad_fn=<SelectBackward>), tensor(-0.0348, grad_fn=<SelectBackward>), tensor(-1.3929, grad_fn=<SelectBackward>), tensor(-0.2975, grad_fn=<SelectBackward>), tensor(-1.6640, grad_fn=<SelectBackward>), tensor(-0.7394, grad_fn=<SelectBackward>), tensor(-1.2235, grad_fn=<SelectBackward>), tensor(-0.0660, grad_fn=<SelectBackward>), tensor(-2.9372, grad_fn=<SelectBackward>), tensor(-1.5867, grad_fn=<SelectBackward>), tensor(-0.7460, grad_fn=<SelectBackward>), tensor(-1.2319, grad_fn=<SelectBackward>), tensor(-0.8084, grad_fn=<SelectBackward>), tensor(-0.1652, grad_fn=<SelectBackward>), tensor(-0.1493, grad_fn=<SelectBackward>), tensor(-0.4603, grad_fn=<SelectBackward>)]\n",
      "log_probs:  tensor([-1.1875, -0.1228, -0.4918, -0.7434, -2.9538, -2.1149, -1.0114, -0.6251,\n",
      "        -0.2867, -0.4497, -0.1387, -0.1532, -0.1537, -2.0413, -0.9770, -1.4681,\n",
      "        -0.1442, -0.6551, -0.3818, -0.2445, -0.2867, -1.4935, -0.7079, -0.3923,\n",
      "        -0.2753, -0.1517, -1.7700, -1.9569, -0.4753, -3.1354, -0.1900, -0.7958,\n",
      "        -0.2612, -2.4956, -0.2818, -2.5179, -0.4784, -0.3305, -0.4978, -0.2119,\n",
      "        -0.2794, -0.1424, -0.3352, -0.6657, -0.1900, -0.2943, -0.1430, -0.3600,\n",
      "        -0.5851, -0.3817, -1.4647, -0.1888, -0.8672, -3.9785, -1.6847, -0.4476,\n",
      "        -0.1225, -0.8320, -1.3957, -1.2996, -0.3141, -0.5112, -0.9223, -1.5123,\n",
      "        -0.7012, -0.7855, -0.2850, -0.2612, -3.2318, -0.4014, -0.4505, -3.5309,\n",
      "        -1.2700, -1.5530, -0.5209, -0.6762, -0.6364, -0.2876, -1.5564, -0.2267,\n",
      "        -3.1473, -2.9457, -1.3185, -1.8128, -0.1606, -0.8369, -0.9697, -0.3655,\n",
      "        -1.4531, -0.2754, -0.1796, -1.1244, -0.3436, -1.9938, -0.4772, -0.8411,\n",
      "        -0.3104, -0.1811, -0.3029, -3.3168, -0.8700, -1.7010, -0.3967, -0.6155,\n",
      "        -1.7291, -1.4886, -0.0348, -1.3929, -0.2975, -1.6640, -0.7394, -1.2235,\n",
      "        -0.0660, -2.9372, -1.5867, -0.7460, -1.2319, -0.8084, -0.1652, -0.1493,\n",
      "        -0.4603], grad_fn=<StackBackward>)\n",
      "distributions.shape:  torch.Size([1, 121, 4])\n",
      "distributions:  tensor([[[0.0275, 0.5323, 0.3050, 0.1352],\n",
      "         [0.0177, 0.8844, 0.0726, 0.0253],\n",
      "         [0.0475, 0.6115, 0.1560, 0.1850],\n",
      "         [0.0610, 0.4755, 0.3523, 0.1112],\n",
      "         [0.0521, 0.7322, 0.0642, 0.1515],\n",
      "         [0.0161, 0.8146, 0.1206, 0.0487],\n",
      "         [0.0478, 0.4943, 0.0942, 0.3637],\n",
      "         [0.0224, 0.5352, 0.1757, 0.2668],\n",
      "         [0.0291, 0.7507, 0.0946, 0.1255],\n",
      "         [0.0113, 0.6378, 0.1448, 0.2061],\n",
      "         [0.0249, 0.8705, 0.0717, 0.0329],\n",
      "         [0.0138, 0.8580, 0.0657, 0.0625],\n",
      "         [0.0156, 0.8575, 0.0793, 0.0476],\n",
      "         [0.0156, 0.8005, 0.0541, 0.1299],\n",
      "         [0.0188, 0.3307, 0.3764, 0.2741],\n",
      "         [0.0554, 0.4706, 0.2436, 0.2304],\n",
      "         [0.0173, 0.8657, 0.0511, 0.0659],\n",
      "         [0.0114, 0.5194, 0.2022, 0.2671],\n",
      "         [0.0253, 0.6826, 0.1410, 0.1510],\n",
      "         [0.0261, 0.7831, 0.1092, 0.0816],\n",
      "         [0.0168, 0.7508, 0.1342, 0.0982],\n",
      "         [0.0442, 0.4250, 0.3062, 0.2246],\n",
      "         [0.0234, 0.4927, 0.3724, 0.1115],\n",
      "         [0.0232, 0.6755, 0.1636, 0.1377],\n",
      "         [0.0109, 0.7593, 0.0744, 0.1553],\n",
      "         [0.0066, 0.8592, 0.0888, 0.0454],\n",
      "         [0.0373, 0.6818, 0.1703, 0.1107],\n",
      "         [0.1356, 0.5229, 0.1413, 0.2003],\n",
      "         [0.0603, 0.6217, 0.1288, 0.1892],\n",
      "         [0.0435, 0.6467, 0.2008, 0.1090],\n",
      "         [0.0096, 0.8269, 0.0761, 0.0873],\n",
      "         [0.0204, 0.4512, 0.1306, 0.3978],\n",
      "         [0.0179, 0.7701, 0.1338, 0.0782],\n",
      "         [0.0262, 0.5356, 0.0824, 0.3557],\n",
      "         [0.0581, 0.7544, 0.0859, 0.1016],\n",
      "         [0.0505, 0.7287, 0.1402, 0.0806],\n",
      "         [0.0201, 0.6198, 0.1929, 0.1672],\n",
      "         [0.0150, 0.7185, 0.1023, 0.1641],\n",
      "         [0.0248, 0.6079, 0.1180, 0.2493],\n",
      "         [0.0183, 0.8090, 0.0938, 0.0789],\n",
      "         [0.0471, 0.7562, 0.0961, 0.1007],\n",
      "         [0.0183, 0.8673, 0.0615, 0.0530],\n",
      "         [0.0420, 0.7152, 0.1172, 0.1255],\n",
      "         [0.0423, 0.5139, 0.2447, 0.1991],\n",
      "         [0.0221, 0.8269, 0.1037, 0.0473],\n",
      "         [0.0120, 0.7451, 0.1273, 0.1156],\n",
      "         [0.0220, 0.8668, 0.0576, 0.0536],\n",
      "         [0.0142, 0.6976, 0.1489, 0.1393],\n",
      "         [0.0218, 0.5570, 0.0792, 0.3419],\n",
      "         [0.0298, 0.6827, 0.1560, 0.1315],\n",
      "         [0.0236, 0.7082, 0.2311, 0.0370],\n",
      "         [0.0229, 0.8279, 0.0906, 0.0585],\n",
      "         [0.0500, 0.4201, 0.1745, 0.3554],\n",
      "         [0.0187, 0.6669, 0.1527, 0.1617],\n",
      "         [0.0161, 0.6528, 0.1855, 0.1455],\n",
      "         [0.0655, 0.6392, 0.1297, 0.1657],\n",
      "         [0.0117, 0.8847, 0.0650, 0.0386],\n",
      "         [0.0330, 0.4352, 0.1338, 0.3980],\n",
      "         [0.0338, 0.4849, 0.2477, 0.2336],\n",
      "         [0.0717, 0.3015, 0.3542, 0.2726],\n",
      "         [0.0284, 0.7304, 0.1046, 0.1366],\n",
      "         [0.0438, 0.2371, 0.1194, 0.5998],\n",
      "         [0.0353, 0.3976, 0.3054, 0.2617],\n",
      "         [0.0489, 0.5958, 0.1348, 0.2204],\n",
      "         [0.0406, 0.2755, 0.1879, 0.4960],\n",
      "         [0.0270, 0.4559, 0.1452, 0.3719],\n",
      "         [0.0236, 0.7520, 0.0661, 0.1582],\n",
      "         [0.0239, 0.7701, 0.0962, 0.1098],\n",
      "         [0.0395, 0.5288, 0.1719, 0.2599],\n",
      "         [0.0198, 0.6694, 0.1902, 0.1207],\n",
      "         [0.0190, 0.6373, 0.1888, 0.1549],\n",
      "         [0.0293, 0.4587, 0.1499, 0.3621],\n",
      "         [0.0205, 0.2808, 0.4377, 0.2610],\n",
      "         [0.0268, 0.5297, 0.2319, 0.2116],\n",
      "         [0.0235, 0.5940, 0.1840, 0.1985],\n",
      "         [0.0289, 0.5086, 0.2882, 0.1743],\n",
      "         [0.0244, 0.5292, 0.2274, 0.2190],\n",
      "         [0.0151, 0.7501, 0.1735, 0.0614],\n",
      "         [0.0470, 0.4908, 0.2513, 0.2109],\n",
      "         [0.0139, 0.7972, 0.0974, 0.0915],\n",
      "         [0.0430, 0.3281, 0.3583, 0.2706],\n",
      "         [0.0526, 0.4233, 0.2299, 0.2942],\n",
      "         [0.0892, 0.4992, 0.2675, 0.1441],\n",
      "         [0.0212, 0.7051, 0.1104, 0.1632],\n",
      "         [0.0107, 0.8516, 0.1045, 0.0331],\n",
      "         [0.0547, 0.3725, 0.1397, 0.4330],\n",
      "         [0.0440, 0.3792, 0.3269, 0.2499],\n",
      "         [0.0191, 0.6938, 0.1850, 0.1021],\n",
      "         [0.0302, 0.5859, 0.1500, 0.2338],\n",
      "         [0.0262, 0.7593, 0.0796, 0.1348],\n",
      "         [0.0085, 0.8356, 0.0505, 0.1054],\n",
      "         [0.0669, 0.3623, 0.2459, 0.3249],\n",
      "         [0.0110, 0.7092, 0.1543, 0.1255],\n",
      "         [0.0193, 0.6668, 0.1777, 0.1362],\n",
      "         [0.0855, 0.6205, 0.1606, 0.1335],\n",
      "         [0.0193, 0.4312, 0.3386, 0.2109],\n",
      "         [0.0234, 0.7332, 0.1609, 0.0825],\n",
      "         [0.0189, 0.8343, 0.0306, 0.1162],\n",
      "         [0.0122, 0.7386, 0.0680, 0.1812],\n",
      "         [0.0363, 0.3802, 0.2462, 0.3374],\n",
      "         [0.0300, 0.4190, 0.1985, 0.3526],\n",
      "         [0.0052, 0.7171, 0.1825, 0.0953],\n",
      "         [0.0723, 0.6726, 0.1077, 0.1474],\n",
      "         [0.0597, 0.5404, 0.1442, 0.2557],\n",
      "         [0.0124, 0.6767, 0.1774, 0.1335],\n",
      "         [0.0385, 0.6033, 0.2257, 0.1325],\n",
      "         [0.0039, 0.9658, 0.0070, 0.0232],\n",
      "         [0.0366, 0.4295, 0.2484, 0.2855],\n",
      "         [0.0099, 0.7427, 0.1618, 0.0856],\n",
      "         [0.0380, 0.6354, 0.1894, 0.1372],\n",
      "         [0.0355, 0.4774, 0.0676, 0.4194],\n",
      "         [0.0930, 0.3316, 0.2812, 0.2942],\n",
      "         [0.0124, 0.9362, 0.0373, 0.0142],\n",
      "         [0.0530, 0.6083, 0.1251, 0.2135],\n",
      "         [0.0281, 0.5910, 0.1763, 0.2046],\n",
      "         [0.0385, 0.2447, 0.2426, 0.4742],\n",
      "         [0.0278, 0.3296, 0.3508, 0.2917],\n",
      "         [0.0382, 0.4456, 0.1271, 0.3892],\n",
      "         [0.0153, 0.8477, 0.0703, 0.0667],\n",
      "         [0.0194, 0.8613, 0.0607, 0.0586],\n",
      "         [0.0409, 0.6311, 0.0593, 0.2687]]], grad_fn=<StackBackward>)\n",
      "Updating critic...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V_trg.shape (after critic):  torch.Size([121])\n",
      "V_trg.shape (after sum):  torch.Size([121])\n",
      "V_trg.shape (after squeeze):  torch.Size([121])\n",
      "V_trg.shape (after squeeze):  tensor([ 5.9489e-01,  4.6012e-01,  6.6426e-01,  6.8205e-01,  2.1336e-01,\n",
      "         6.0229e-01,  6.3206e-01,  7.6581e-01,  8.4131e-01,  4.5807e-01,\n",
      "         4.0974e-01,  6.6420e-02,  4.9295e-01,  7.6558e-01,  2.6622e-01,\n",
      "         4.5730e-01,  4.2124e-01,  7.5740e-01,  2.6394e-01,  3.5273e-01,\n",
      "         3.5184e-01,  2.1945e-01,  5.0457e-02,  6.3897e-01,  3.4294e-01,\n",
      "         5.6414e-01,  3.4865e-02,  1.7185e-01,  4.3358e-01,  4.9552e-01,\n",
      "         3.9015e-01, -8.6056e-03,  4.9933e-01,  2.3513e-01,  5.3634e-01,\n",
      "         2.1133e-01,  3.0070e-01,  3.3733e-01,  4.2510e-01,  1.8285e-01,\n",
      "         1.0551e-01,  2.6509e-01,  7.0395e-01,  3.9195e-01,  5.9955e-01,\n",
      "         3.3030e-01,  5.4225e-01,  3.2606e-01, -2.9198e-01,  2.6824e-01,\n",
      "        -1.1794e-01,  6.9178e-01,  5.2804e-01,  4.6889e-01,  3.7590e-01,\n",
      "         5.7392e-02,  1.2026e-01,  5.1428e-01,  8.6747e-01,  1.4089e-01,\n",
      "         4.5431e-01,  5.7546e-01,  8.0685e-01,  8.1397e-01,  3.2988e-01,\n",
      "         9.2025e-01,  3.3876e-01,  6.6911e-01,  8.9508e-01,  3.1829e-01,\n",
      "         7.8916e-02,  4.0409e-01,  6.2540e-02,  4.0430e-01, -3.5505e-02,\n",
      "         1.4833e-01,  5.9202e-01,  2.4802e-01,  7.9984e-01, -1.2693e-01,\n",
      "         3.0611e-01,  3.5409e-01,  8.4713e-04,  2.6476e-01,  2.8907e-01,\n",
      "         3.1651e-01,  6.9795e-01,  8.2054e-01,  4.6351e-01,  5.6669e-01,\n",
      "        -7.1512e-02,  3.4205e-01,  3.6489e-01, -3.6737e-02,  7.3271e-01,\n",
      "         2.0219e-01,  1.9219e-01,  1.0354e+00,  6.8138e-01,  3.0132e-01,\n",
      "         4.1087e-01,  8.0979e-01,  6.8153e-01,  6.3956e-02,  2.2241e-01,\n",
      "         3.5353e-01, -6.3669e-02,  9.8744e-01,  1.2189e+00,  5.1711e-01,\n",
      "         1.7417e-01,  7.5920e-01,  5.6494e-01,  3.4610e-01,  3.0569e-01,\n",
      "         9.8063e-01,  6.6428e-01, -2.7267e-01,  2.1627e-01,  3.0923e-01,\n",
      "         9.1615e-01])\n",
      "V1.shape:  torch.Size([121])\n",
      "V1:  tensor([[0.0964],\n",
      "        [1.2096],\n",
      "        [1.2400],\n",
      "        [2.1741],\n",
      "        [0.9232],\n",
      "        [2.0524],\n",
      "        [1.7231],\n",
      "        [1.5256],\n",
      "        [1.1288],\n",
      "        [1.5425],\n",
      "        [2.0723],\n",
      "        [1.5035],\n",
      "        [1.9491],\n",
      "        [1.1029],\n",
      "        [1.8072],\n",
      "        [2.2119],\n",
      "        [1.0126],\n",
      "        [1.7109],\n",
      "        [2.3052],\n",
      "        [0.1804],\n",
      "        [1.3242],\n",
      "        [1.6694],\n",
      "        [2.5309],\n",
      "        [1.3226],\n",
      "        [1.8229],\n",
      "        [2.0569],\n",
      "        [2.3255],\n",
      "        [1.7368],\n",
      "        [1.9932],\n",
      "        [1.8278],\n",
      "        [1.8176],\n",
      "        [2.3980],\n",
      "        [1.8972],\n",
      "        [2.3511],\n",
      "        [1.6902],\n",
      "        [1.2492],\n",
      "        [2.4361],\n",
      "        [0.1007],\n",
      "        [0.4873],\n",
      "        [2.2396],\n",
      "        [1.2315],\n",
      "        [0.9341],\n",
      "        [2.0418],\n",
      "        [0.8096],\n",
      "        [2.1509],\n",
      "        [2.6736],\n",
      "        [1.5668],\n",
      "        [1.8828],\n",
      "        [1.1020],\n",
      "        [1.5476],\n",
      "        [0.4444],\n",
      "        [3.0524],\n",
      "        [1.4187],\n",
      "        [2.8340],\n",
      "        [1.5666],\n",
      "        [1.8225],\n",
      "        [1.7128],\n",
      "        [2.3747],\n",
      "        [1.4134],\n",
      "        [1.8454],\n",
      "        [2.4393],\n",
      "        [1.8838],\n",
      "        [1.6005],\n",
      "        [1.8855],\n",
      "        [1.4389],\n",
      "        [0.9271],\n",
      "        [1.8112],\n",
      "        [1.3252],\n",
      "        [1.2716],\n",
      "        [1.8051],\n",
      "        [0.7293],\n",
      "        [1.8534],\n",
      "        [1.1271],\n",
      "        [2.3250],\n",
      "        [1.8425],\n",
      "        [2.6052],\n",
      "        [2.5577],\n",
      "        [1.2166],\n",
      "        [1.9365],\n",
      "        [1.5102],\n",
      "        [1.5988],\n",
      "        [1.2099],\n",
      "        [1.5739],\n",
      "        [1.5077],\n",
      "        [1.0822],\n",
      "        [0.9912],\n",
      "        [0.7077],\n",
      "        [2.0991],\n",
      "        [1.9167],\n",
      "        [1.8264],\n",
      "        [3.3368],\n",
      "        [1.3304],\n",
      "        [1.8448],\n",
      "        [1.0527],\n",
      "        [1.8577],\n",
      "        [2.1689],\n",
      "        [1.7543],\n",
      "        [1.9731],\n",
      "        [1.6281],\n",
      "        [0.7384],\n",
      "        [0.5054],\n",
      "        [2.4119],\n",
      "        [1.0030],\n",
      "        [1.6994],\n",
      "        [1.8266],\n",
      "        [0.9970],\n",
      "        [2.6020],\n",
      "        [2.2205],\n",
      "        [2.0096],\n",
      "        [2.3792],\n",
      "        [1.4601],\n",
      "        [2.4201],\n",
      "        [2.4367],\n",
      "        [0.8499],\n",
      "        [1.1831],\n",
      "        [1.0983],\n",
      "        [1.3369],\n",
      "        [1.6316],\n",
      "        [0.5666],\n",
      "        [1.1930],\n",
      "        [2.3158]], grad_fn=<AddmmBackward>)\n",
      "Updating actor...\n",
      "V_trg.shape:  torch.Size([121])\n",
      "V_trg:  tensor([ 0.9183,  0.6780,  0.4153,  0.8548,  0.5048,  0.7464,  0.4564,  0.5660,\n",
      "         0.9617,  1.3231,  0.2239,  0.2795,  0.6394,  0.9278,  0.5737,  1.2355,\n",
      "         0.5973,  0.6538,  1.0765,  0.4573,  1.1010,  0.8171,  0.7900,  0.3134,\n",
      "         0.5014,  0.9916,  1.0404,  0.6878,  1.5126,  0.5051,  0.5208,  0.9968,\n",
      "         0.3429,  0.5303,  0.6223,  0.6650,  0.2864,  0.9319,  0.5994,  1.1132,\n",
      "         0.5026,  0.8216,  0.2631,  0.7378,  0.5426,  1.0237,  1.2315,  0.6040,\n",
      "         0.8055,  0.8375,  0.9899,  1.0104,  0.6343,  0.9125,  0.5980,  0.8227,\n",
      "         1.1166,  0.1569,  0.6928,  1.4015,  0.5902,  1.1818,  0.6473,  0.9908,\n",
      "         0.8972,  0.6519,  0.7552,  0.2457,  0.4702,  0.4581,  1.2145,  0.9178,\n",
      "         1.1401,  0.7557,  0.3724,  1.1430,  1.1335,  0.4327,  1.1289,  1.0310,\n",
      "         0.3727,  0.3475,  0.7182,  1.0713,  0.0744,  0.7716,  0.9954,  0.9168,\n",
      "         0.7180,  0.7741,  1.0056,  0.7438,  1.0428,  1.4938,  0.7747,  0.7098,\n",
      "         0.9220, -0.0167,  1.0196,  1.6616, -0.0434,  0.8893,  1.3532,  1.3545,\n",
      "         1.7866,  1.1987,  1.2058,  1.0891,  0.6432,  0.7885,  1.0533,  0.9037,\n",
      "         1.6865,  1.1030,  0.7402,  0.5125,  0.9013,  0.7110,  0.3910,  2.0234,\n",
      "         0.3966])\n",
      "V_pred.shape:  torch.Size([121])\n",
      "V_pred:  tensor([ 1.4349,  1.3571,  1.0471,  0.4848,  0.9415,  0.6450,  1.8549,  1.4784,\n",
      "         1.2179,  1.2575,  0.8616,  1.4625,  0.4645, -0.1515,  1.3426,  1.5635,\n",
      "         0.9725,  1.1318,  1.0308,  1.4045,  1.4588,  0.9089,  0.8663,  0.7736,\n",
      "         0.8333,  0.9816,  2.0391,  0.5571,  1.1115,  0.7048,  0.2713,  0.8821,\n",
      "         0.4104,  1.5673,  1.1390,  1.7543,  1.0500,  2.0438,  1.0335,  1.0544,\n",
      "         2.0393,  0.6909,  0.0449,  0.8063,  0.2364,  1.2837,  1.6176,  1.2633,\n",
      "         2.4439,  0.7078,  0.4271,  0.5177,  1.8308,  1.3973,  1.4717,  1.0224,\n",
      "         0.7315,  1.3532,  0.4285,  0.5178,  0.9584,  1.0671,  1.2159,  1.0654,\n",
      "         1.6470,  0.5983,  0.4575,  1.5811,  1.0944,  0.9683,  0.5997, -0.0886,\n",
      "         1.9770,  1.1429,  1.2703,  0.3624,  1.5424,  0.4133,  1.4525,  0.2844,\n",
      "         0.4666,  1.1472,  1.4508,  0.8807,  1.5341,  1.0081,  1.3178,  0.4508,\n",
      "         0.5393,  1.6864,  1.6345,  1.1910,  0.9560,  1.3062,  1.2535,  1.6949,\n",
      "         0.1188,  1.3919,  1.7577,  0.3935,  0.8285,  0.6027,  1.1703,  1.3243,\n",
      "         2.2656,  0.9899,  0.9278,  1.9272,  1.4912,  0.7877,  2.0858,  1.4208,\n",
      "         1.3478,  1.1974,  1.4758,  1.1305,  1.5806,  1.1081,  1.4619,  1.3698,\n",
      "         1.4643])\n",
      "A.shape:  torch.Size([121])\n",
      "A:  tensor([-5.1659e-01, -6.7912e-01, -6.3182e-01,  3.7000e-01, -4.3669e-01,\n",
      "         1.0140e-01, -1.3985e+00, -9.1233e-01, -2.5617e-01,  6.5573e-02,\n",
      "        -6.3776e-01, -1.1830e+00,  1.7485e-01,  1.0793e+00, -7.6899e-01,\n",
      "        -3.2801e-01, -3.7525e-01, -4.7797e-01,  4.5682e-02, -9.4717e-01,\n",
      "        -3.5777e-01, -9.1845e-02, -7.6332e-02, -4.6023e-01, -3.3192e-01,\n",
      "         9.9859e-03, -9.9873e-01,  1.3072e-01,  4.0105e-01, -1.9964e-01,\n",
      "         2.4954e-01,  1.1471e-01, -6.7523e-02, -1.0370e+00, -5.1672e-01,\n",
      "        -1.0893e+00, -7.6358e-01, -1.1120e+00, -4.3417e-01,  5.8853e-02,\n",
      "        -1.5366e+00,  1.3065e-01,  2.1818e-01, -6.8561e-02,  3.0621e-01,\n",
      "        -2.6000e-01, -3.8606e-01, -6.5937e-01, -1.6385e+00,  1.2973e-01,\n",
      "         5.6281e-01,  4.9266e-01, -1.1966e+00, -4.8486e-01, -8.7365e-01,\n",
      "        -1.9968e-01,  3.8511e-01, -1.1963e+00,  2.6427e-01,  8.8367e-01,\n",
      "        -3.6822e-01,  1.1475e-01, -5.6860e-01, -7.4576e-02, -7.4984e-01,\n",
      "         5.3587e-02,  2.9777e-01, -1.3354e+00, -6.2423e-01, -5.1020e-01,\n",
      "         6.1475e-01,  1.0064e+00, -8.3693e-01, -3.8716e-01, -8.9786e-01,\n",
      "         7.8060e-01, -4.0889e-01,  1.9362e-02, -3.2353e-01,  7.4652e-01,\n",
      "        -9.3839e-02, -7.9976e-01, -7.3263e-01,  1.9060e-01, -1.4597e+00,\n",
      "        -2.3644e-01, -3.2233e-01,  4.6602e-01,  1.7861e-01, -9.1232e-01,\n",
      "        -6.2885e-01, -4.4721e-01,  8.6798e-02,  1.8757e-01, -4.7879e-01,\n",
      "        -9.8511e-01,  8.0316e-01, -1.4086e+00, -7.3816e-01,  1.2681e+00,\n",
      "        -8.7194e-01,  2.8659e-01,  1.8291e-01,  3.0207e-02, -4.7904e-01,\n",
      "         2.0882e-01,  2.7799e-01, -8.3817e-01, -8.4797e-01,  7.5305e-04,\n",
      "        -1.0325e+00, -5.1709e-01,  3.3868e-01, -9.4437e-02, -7.3555e-01,\n",
      "        -6.1803e-01, -6.7923e-01, -3.9708e-01, -1.0709e+00,  6.5363e-01,\n",
      "        -1.0677e+00])\n",
      "policy_gradient.shape:  torch.Size([121])\n",
      "policy_gradient:  tensor([-6.1346e-01, -8.3429e-02, -3.1073e-01,  2.7507e-01, -1.2899e+00,\n",
      "         2.1445e-01, -1.4144e+00, -5.7034e-01, -7.3447e-02,  2.9491e-02,\n",
      "        -8.8475e-02, -1.8121e-01,  2.6881e-02,  2.2032e+00, -7.5134e-01,\n",
      "        -4.8157e-01, -5.4119e-02, -3.1314e-01,  1.7441e-02, -2.3156e-01,\n",
      "        -1.0256e-01, -1.3717e-01, -5.4033e-02, -1.8054e-01, -9.1380e-02,\n",
      "         1.5153e-03, -1.7678e+00,  2.5581e-01,  1.9064e-01, -6.2594e-01,\n",
      "         4.7421e-02,  9.1278e-02, -1.7640e-02, -2.5879e+00, -1.4562e-01,\n",
      "        -2.7426e+00, -3.6530e-01, -3.6754e-01, -2.1612e-01,  1.2471e-02,\n",
      "        -4.2941e-01,  1.8604e-02,  7.3124e-02, -4.5641e-02,  5.8194e-02,\n",
      "        -7.6511e-02, -5.5191e-02, -2.3740e-01, -9.5873e-01,  4.9518e-02,\n",
      "         8.2435e-01,  9.3025e-02, -1.0376e+00, -1.9290e+00, -1.4718e+00,\n",
      "        -8.9372e-02,  4.7190e-02, -9.9523e-01,  3.6883e-01,  1.1485e+00,\n",
      "        -1.1567e-01,  5.8665e-02, -5.2444e-01, -1.1278e-01, -5.2577e-01,\n",
      "         4.2092e-02,  8.4851e-02, -3.4888e-01, -2.0174e+00, -2.0477e-01,\n",
      "         2.7693e-01,  3.5535e+00, -1.0629e+00, -6.0127e-01, -4.6772e-01,\n",
      "         5.2782e-01, -2.6023e-01,  5.5677e-03, -5.0354e-01,  1.6924e-01,\n",
      "        -2.9534e-01, -2.3559e+00, -9.6600e-01,  3.4552e-01, -2.3442e-01,\n",
      "        -1.9788e-01, -3.1256e-01,  1.7034e-01,  2.5954e-01, -2.5121e-01,\n",
      "        -1.1297e-01, -5.0284e-01,  2.9827e-02,  3.7399e-01, -2.2849e-01,\n",
      "        -8.2856e-01,  2.4930e-01, -2.5510e-01, -2.2362e-01,  4.2061e+00,\n",
      "        -7.5859e-01,  4.8750e-01,  7.2555e-02,  1.8593e-02, -8.2832e-01,\n",
      "         3.1086e-01,  9.6621e-03, -1.1675e+00, -2.5225e-01,  1.2530e-03,\n",
      "        -7.6337e-01, -6.3267e-01,  2.2343e-02, -2.7738e-01, -1.1671e+00,\n",
      "        -4.6107e-01, -8.3673e-01, -3.2101e-01, -1.7694e-01,  9.7581e-02,\n",
      "        -4.9144e-01], grad_fn=<MulBackward0>)\n",
      "policy_grad:  tensor(-27.4094, grad_fn=<SumBackward0>)\n",
      "Negative entropy:  tensor(-111.2717, grad_fn=<SumBackward0>)\n",
      "Actor loss:  tensor(-28.5221, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distribution:  tensor([[0.0305, 0.5661, 0.1596, 0.2438]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0170, 0.7700, 0.1567, 0.0563]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0371, 0.3597, 0.4431, 0.1600]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0075, 0.4584, 0.1412, 0.3930]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0735, 0.2104, 0.4624, 0.2537]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0269, 0.4117, 0.3144, 0.2470]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0207, 0.6285, 0.1105, 0.2403]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0224, 0.4653, 0.2930, 0.2193]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0299, 0.8357, 0.0744, 0.0600]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0133, 0.8945, 0.0545, 0.0377]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0357, 0.3083, 0.1174, 0.5386]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0205, 0.5764, 0.2178, 0.1853]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0511, 0.7670, 0.0512, 0.1306]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0650, 0.1927, 0.2540, 0.4884]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0070, 0.9278, 0.0317, 0.0336]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0059, 0.8368, 0.1127, 0.0446]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0164, 0.8205, 0.0874, 0.0757]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0137, 0.7664, 0.0587, 0.1613]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0057, 0.9194, 0.0331, 0.0418]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0149, 0.3122, 0.2664, 0.4065]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0177, 0.7223, 0.1726, 0.0875]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0133, 0.9137, 0.0478, 0.0252]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0085, 0.8904, 0.0648, 0.0363]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0169, 0.6699, 0.1675, 0.1457]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0294, 0.4711, 0.3548, 0.1448]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0211, 0.6273, 0.1451, 0.2065]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0148, 0.7815, 0.1223, 0.0814]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0270, 0.5198, 0.3227, 0.1306]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0220, 0.8802, 0.0406, 0.0572]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0095, 0.8752, 0.0710, 0.0443]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0338, 0.5197, 0.2324, 0.2141]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0180, 0.6504, 0.2396, 0.0920]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0480, 0.4173, 0.0968, 0.4379]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0207, 0.5926, 0.1448, 0.2419]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0219, 0.7919, 0.0786, 0.1075]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0252, 0.2473, 0.3192, 0.4082]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0372, 0.6614, 0.1232, 0.1782]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0319, 0.5573, 0.1221, 0.2886]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0654, 0.3623, 0.0649, 0.5074]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0510, 0.4232, 0.0703, 0.4555]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0229, 0.6049, 0.1642, 0.2080]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0340, 0.5903, 0.0742, 0.3015]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0603, 0.5507, 0.1457, 0.2433]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0129, 0.8652, 0.0624, 0.0595]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0345, 0.5314, 0.1911, 0.2430]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0102, 0.7159, 0.2045, 0.0695]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0245, 0.7612, 0.1140, 0.1003]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0199, 0.6584, 0.2328, 0.0889]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0408, 0.4202, 0.3097, 0.2292]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0297, 0.8507, 0.0808, 0.0389]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0043, 0.5698, 0.2627, 0.1632]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0110, 0.9024, 0.0274, 0.0591]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0402, 0.5939, 0.1403, 0.2257]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0154, 0.5832, 0.0687, 0.3327]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0194, 0.8322, 0.0669, 0.0815]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0481, 0.4704, 0.0966, 0.3849]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0249, 0.7855, 0.1225, 0.0671]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0167, 0.3559, 0.1455, 0.4818]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0300, 0.7205, 0.1488, 0.1007]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0097, 0.8815, 0.0642, 0.0446]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0168, 0.6539, 0.0873, 0.2419]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0109, 0.8754, 0.0756, 0.0381]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0487, 0.6181, 0.1152, 0.2180]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0150, 0.7808, 0.0625, 0.1416]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0193, 0.3730, 0.0885, 0.5192]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0196, 0.4775, 0.1941, 0.3088]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0383, 0.5962, 0.2282, 0.1373]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0221, 0.5361, 0.0813, 0.3605]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0609, 0.1516, 0.2870, 0.5005]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0800, 0.3691, 0.1727, 0.3782]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0478, 0.4552, 0.2019, 0.2951]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0087, 0.9225, 0.0466, 0.0222]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0318, 0.6247, 0.0602, 0.2833]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0197, 0.4890, 0.1681, 0.3231]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0078, 0.7523, 0.1610, 0.0790]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0041, 0.9346, 0.0504, 0.0109]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0190, 0.6732, 0.1168, 0.1911]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0345, 0.6660, 0.1590, 0.1406]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0353, 0.6474, 0.1106, 0.2067]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0198, 0.9435, 0.0196, 0.0172]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0374, 0.7006, 0.1411, 0.1209]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0571, 0.3941, 0.2528, 0.2960]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0452, 0.5040, 0.1375, 0.3133]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0157, 0.8162, 0.0844, 0.0836]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0299, 0.6344, 0.1580, 0.1777]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0090, 0.4138, 0.0801, 0.4971]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0211, 0.6753, 0.1183, 0.1853]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0528, 0.5546, 0.1547, 0.2379]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0857, 0.3636, 0.2615, 0.2892]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0062, 0.8001, 0.1154, 0.0783]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0242, 0.7682, 0.0967, 0.1109]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0483, 0.3488, 0.2288, 0.3741]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0245, 0.7697, 0.0761, 0.1297]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0239, 0.6262, 0.2036, 0.1463]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0132, 0.8669, 0.0738, 0.0460]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0278, 0.6931, 0.1999, 0.0792]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0293, 0.8234, 0.0912, 0.0561]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0083, 0.8155, 0.0663, 0.1099]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0207, 0.5504, 0.2268, 0.2021]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0162, 0.3709, 0.0288, 0.5840]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0186, 0.6728, 0.2525, 0.0561]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0144, 0.7112, 0.2090, 0.0655]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0361, 0.5936, 0.1898, 0.1806]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0127, 0.8993, 0.0571, 0.0309]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0688, 0.4607, 0.2312, 0.2392]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0261, 0.5595, 0.2554, 0.1590]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0133, 0.6741, 0.2364, 0.0762]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0136, 0.8246, 0.0893, 0.0726]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0303, 0.5581, 0.1756, 0.2359]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0141, 0.8039, 0.0306, 0.1514]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0301, 0.8477, 0.0463, 0.0759]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0137, 0.8102, 0.1103, 0.0658]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0181, 0.7975, 0.1107, 0.0738]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0289, 0.4533, 0.2166, 0.3012]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0726, 0.5326, 0.0916, 0.3032]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0214, 0.6460, 0.0962, 0.2363]], grad_fn=<ExpBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distribution:  tensor([[0.0181, 0.6857, 0.1898, 0.1064]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0214, 0.7844, 0.1062, 0.0879]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0112, 0.4650, 0.1278, 0.3960]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0350, 0.5127, 0.2122, 0.2400]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0409, 0.3659, 0.2190, 0.3742]], grad_fn=<ExpBackward>)\n",
      "n_step_rewards.shape:  (121,)\n",
      "rewards.shape:  (121,)\n",
      "n_step_rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0.]\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0.]\n",
      "bootstrap:  [False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      "  True]\n",
      "done.shape: (before n_steps) (121,)\n",
      "done: (before n_steps) [False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False]\n",
      "done.shape: (after n_steps) (121,)\n",
      "Gamma_V.shape:  (121,)\n",
      "done: (after n_steps) [False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False]\n",
      "Gamma_V:  [0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.67572905 0.6825546\n",
      " 0.68944909 0.69641322 0.70344769 0.71055323 0.71773053 0.72498034\n",
      " 0.73230337 0.73970037 0.74717209 0.75471929 0.76234271 0.77004315\n",
      " 0.77782136 0.78567814 0.79361428 0.80163059 0.80972787 0.81790694\n",
      " 0.82616862 0.83451376 0.84294319 0.85145777 0.86005835 0.86874581\n",
      " 0.87752102 0.88638487 0.89533825 0.90438208 0.91351725 0.92274469\n",
      " 0.93206535 0.94148015 0.95099005 0.96059601 0.970299   0.9801\n",
      " 0.99      ]\n",
      "old_states.shape:  torch.Size([121, 1, 9, 9])\n",
      "new_states.shape:  torch.Size([121, 1, 9, 9])\n",
      "log_probs:  [tensor(-0.5690, grad_fn=<SelectBackward>), tensor(-4.0723, grad_fn=<SelectBackward>), tensor(-0.8139, grad_fn=<SelectBackward>), tensor(-0.9341, grad_fn=<SelectBackward>), tensor(-0.7714, grad_fn=<SelectBackward>), tensor(-1.1569, grad_fn=<SelectBackward>), tensor(-0.4644, grad_fn=<SelectBackward>), tensor(-1.2276, grad_fn=<SelectBackward>), tensor(-0.1795, grad_fn=<SelectBackward>), tensor(-0.1115, grad_fn=<SelectBackward>), tensor(-1.1766, grad_fn=<SelectBackward>), tensor(-0.5509, grad_fn=<SelectBackward>), tensor(-0.2652, grad_fn=<SelectBackward>), tensor(-2.7337, grad_fn=<SelectBackward>), tensor(-0.0750, grad_fn=<SelectBackward>), tensor(-0.1782, grad_fn=<SelectBackward>), tensor(-0.1979, grad_fn=<SelectBackward>), tensor(-0.2661, grad_fn=<SelectBackward>), tensor(-0.0841, grad_fn=<SelectBackward>), tensor(-0.9001, grad_fn=<SelectBackward>), tensor(-0.3253, grad_fn=<SelectBackward>), tensor(-0.0903, grad_fn=<SelectBackward>), tensor(-0.1161, grad_fn=<SelectBackward>), tensor(-0.4006, grad_fn=<SelectBackward>), tensor(-1.0363, grad_fn=<SelectBackward>), tensor(-0.4663, grad_fn=<SelectBackward>), tensor(-0.2465, grad_fn=<SelectBackward>), tensor(-0.6544, grad_fn=<SelectBackward>), tensor(-0.1276, grad_fn=<SelectBackward>), tensor(-0.1333, grad_fn=<SelectBackward>), tensor(-0.6545, grad_fn=<SelectBackward>), tensor(-0.4301, grad_fn=<SelectBackward>), tensor(-0.8740, grad_fn=<SelectBackward>), tensor(-1.4192, grad_fn=<SelectBackward>), tensor(-2.2301, grad_fn=<SelectBackward>), tensor(-0.8959, grad_fn=<SelectBackward>), tensor(-0.4134, grad_fn=<SelectBackward>), tensor(-0.5846, grad_fn=<SelectBackward>), tensor(-1.0153, grad_fn=<SelectBackward>), tensor(-0.7864, grad_fn=<SelectBackward>), tensor(-1.5703, grad_fn=<SelectBackward>), tensor(-1.1991, grad_fn=<SelectBackward>), tensor(-0.5966, grad_fn=<SelectBackward>), tensor(-0.1447, grad_fn=<SelectBackward>), tensor(-0.6323, grad_fn=<SelectBackward>), tensor(-0.3343, grad_fn=<SelectBackward>), tensor(-0.2729, grad_fn=<SelectBackward>), tensor(-0.4179, grad_fn=<SelectBackward>), tensor(-1.1721, grad_fn=<SelectBackward>), tensor(-0.1617, grad_fn=<SelectBackward>), tensor(-0.5624, grad_fn=<SelectBackward>), tensor(-0.1027, grad_fn=<SelectBackward>), tensor(-1.9642, grad_fn=<SelectBackward>), tensor(-1.1006, grad_fn=<SelectBackward>), tensor(-0.1837, grad_fn=<SelectBackward>), tensor(-0.9549, grad_fn=<SelectBackward>), tensor(-0.2415, grad_fn=<SelectBackward>), tensor(-1.0330, grad_fn=<SelectBackward>), tensor(-2.2954, grad_fn=<SelectBackward>), tensor(-0.1262, grad_fn=<SelectBackward>), tensor(-0.4248, grad_fn=<SelectBackward>), tensor(-0.1330, grad_fn=<SelectBackward>), tensor(-1.5231, grad_fn=<SelectBackward>), tensor(-4.1984, grad_fn=<SelectBackward>), tensor(-0.9861, grad_fn=<SelectBackward>), tensor(-0.7392, grad_fn=<SelectBackward>), tensor(-0.5172, grad_fn=<SelectBackward>), tensor(-0.6234, grad_fn=<SelectBackward>), tensor(-2.7985, grad_fn=<SelectBackward>), tensor(-0.9722, grad_fn=<SelectBackward>), tensor(-0.7870, grad_fn=<SelectBackward>), tensor(-0.0807, grad_fn=<SelectBackward>), tensor(-1.2611, grad_fn=<SelectBackward>), tensor(-1.1298, grad_fn=<SelectBackward>), tensor(-0.2846, grad_fn=<SelectBackward>), tensor(-0.0676, grad_fn=<SelectBackward>), tensor(-0.3957, grad_fn=<SelectBackward>), tensor(-1.9620, grad_fn=<SelectBackward>), tensor(-2.2019, grad_fn=<SelectBackward>), tensor(-0.0582, grad_fn=<SelectBackward>), tensor(-0.3558, grad_fn=<SelectBackward>), tensor(-1.3750, grad_fn=<SelectBackward>), tensor(-0.6852, grad_fn=<SelectBackward>), tensor(-0.2031, grad_fn=<SelectBackward>), tensor(-1.8449, grad_fn=<SelectBackward>), tensor(-0.8825, grad_fn=<SelectBackward>), tensor(-2.1349, grad_fn=<SelectBackward>), tensor(-0.5895, grad_fn=<SelectBackward>), tensor(-1.2406, grad_fn=<SelectBackward>), tensor(-0.2230, grad_fn=<SelectBackward>), tensor(-0.2636, grad_fn=<SelectBackward>), tensor(-0.9833, grad_fn=<SelectBackward>), tensor(-2.0428, grad_fn=<SelectBackward>), tensor(-1.5915, grad_fn=<SelectBackward>), tensor(-0.1428, grad_fn=<SelectBackward>), tensor(-0.3666, grad_fn=<SelectBackward>), tensor(-0.1943, grad_fn=<SelectBackward>), tensor(-0.2040, grad_fn=<SelectBackward>), tensor(-0.5972, grad_fn=<SelectBackward>), tensor(-0.5378, grad_fn=<SelectBackward>), tensor(-1.3762, grad_fn=<SelectBackward>), tensor(-0.3408, grad_fn=<SelectBackward>), tensor(-0.5215, grad_fn=<SelectBackward>), tensor(-0.1061, grad_fn=<SelectBackward>), tensor(-1.4304, grad_fn=<SelectBackward>), tensor(-1.3650, grad_fn=<SelectBackward>), tensor(-0.3943, grad_fn=<SelectBackward>), tensor(-0.1928, grad_fn=<SelectBackward>), tensor(-1.7394, grad_fn=<SelectBackward>), tensor(-1.8876, grad_fn=<SelectBackward>), tensor(-0.1653, grad_fn=<SelectBackward>), tensor(-0.2105, grad_fn=<SelectBackward>), tensor(-0.2263, grad_fn=<SelectBackward>), tensor(-0.7912, grad_fn=<SelectBackward>), tensor(-0.6299, grad_fn=<SelectBackward>), tensor(-0.4370, grad_fn=<SelectBackward>), tensor(-0.3773, grad_fn=<SelectBackward>), tensor(-2.4312, grad_fn=<SelectBackward>), tensor(-0.7658, grad_fn=<SelectBackward>), tensor(-0.6681, grad_fn=<SelectBackward>), tensor(-1.5188, grad_fn=<SelectBackward>)]\n",
      "log_probs:  tensor([-0.5690, -4.0723, -0.8139, -0.9341, -0.7714, -1.1569, -0.4644, -1.2276,\n",
      "        -0.1795, -0.1115, -1.1766, -0.5509, -0.2652, -2.7337, -0.0750, -0.1782,\n",
      "        -0.1979, -0.2661, -0.0841, -0.9001, -0.3253, -0.0903, -0.1161, -0.4006,\n",
      "        -1.0363, -0.4663, -0.2465, -0.6544, -0.1276, -0.1333, -0.6545, -0.4301,\n",
      "        -0.8740, -1.4192, -2.2301, -0.8959, -0.4134, -0.5846, -1.0153, -0.7864,\n",
      "        -1.5703, -1.1991, -0.5966, -0.1447, -0.6323, -0.3343, -0.2729, -0.4179,\n",
      "        -1.1721, -0.1617, -0.5624, -0.1027, -1.9642, -1.1006, -0.1837, -0.9549,\n",
      "        -0.2415, -1.0330, -2.2954, -0.1262, -0.4248, -0.1330, -1.5231, -4.1984,\n",
      "        -0.9861, -0.7392, -0.5172, -0.6234, -2.7985, -0.9722, -0.7870, -0.0807,\n",
      "        -1.2611, -1.1298, -0.2846, -0.0676, -0.3957, -1.9620, -2.2019, -0.0582,\n",
      "        -0.3558, -1.3750, -0.6852, -0.2031, -1.8449, -0.8825, -2.1349, -0.5895,\n",
      "        -1.2406, -0.2230, -0.2636, -0.9833, -2.0428, -1.5915, -0.1428, -0.3666,\n",
      "        -0.1943, -0.2040, -0.5972, -0.5378, -1.3762, -0.3408, -0.5215, -0.1061,\n",
      "        -1.4304, -1.3650, -0.3943, -0.1928, -1.7394, -1.8876, -0.1653, -0.2105,\n",
      "        -0.2263, -0.7912, -0.6299, -0.4370, -0.3773, -2.4312, -0.7658, -0.6681,\n",
      "        -1.5188], grad_fn=<StackBackward>)\n",
      "distributions.shape:  torch.Size([1, 121, 4])\n",
      "distributions:  tensor([[[0.0305, 0.5661, 0.1596, 0.2438],\n",
      "         [0.0170, 0.7700, 0.1567, 0.0563],\n",
      "         [0.0371, 0.3597, 0.4431, 0.1600],\n",
      "         [0.0075, 0.4584, 0.1412, 0.3930],\n",
      "         [0.0735, 0.2104, 0.4624, 0.2537],\n",
      "         [0.0269, 0.4117, 0.3144, 0.2470],\n",
      "         [0.0207, 0.6285, 0.1105, 0.2403],\n",
      "         [0.0224, 0.4653, 0.2930, 0.2193],\n",
      "         [0.0299, 0.8357, 0.0744, 0.0600],\n",
      "         [0.0133, 0.8945, 0.0545, 0.0377],\n",
      "         [0.0357, 0.3083, 0.1174, 0.5386],\n",
      "         [0.0205, 0.5764, 0.2178, 0.1853],\n",
      "         [0.0511, 0.7670, 0.0512, 0.1306],\n",
      "         [0.0650, 0.1927, 0.2540, 0.4884],\n",
      "         [0.0070, 0.9278, 0.0317, 0.0336],\n",
      "         [0.0059, 0.8368, 0.1127, 0.0446],\n",
      "         [0.0164, 0.8205, 0.0874, 0.0757],\n",
      "         [0.0137, 0.7664, 0.0587, 0.1613],\n",
      "         [0.0057, 0.9194, 0.0331, 0.0418],\n",
      "         [0.0149, 0.3122, 0.2664, 0.4065],\n",
      "         [0.0177, 0.7223, 0.1726, 0.0875],\n",
      "         [0.0133, 0.9137, 0.0478, 0.0252],\n",
      "         [0.0085, 0.8904, 0.0648, 0.0363],\n",
      "         [0.0169, 0.6699, 0.1675, 0.1457],\n",
      "         [0.0294, 0.4711, 0.3548, 0.1448],\n",
      "         [0.0211, 0.6273, 0.1451, 0.2065],\n",
      "         [0.0148, 0.7815, 0.1223, 0.0814],\n",
      "         [0.0270, 0.5198, 0.3227, 0.1306],\n",
      "         [0.0220, 0.8802, 0.0406, 0.0572],\n",
      "         [0.0095, 0.8752, 0.0710, 0.0443],\n",
      "         [0.0338, 0.5197, 0.2324, 0.2141],\n",
      "         [0.0180, 0.6504, 0.2396, 0.0920],\n",
      "         [0.0480, 0.4173, 0.0968, 0.4379],\n",
      "         [0.0207, 0.5926, 0.1448, 0.2419],\n",
      "         [0.0219, 0.7919, 0.0786, 0.1075],\n",
      "         [0.0252, 0.2473, 0.3192, 0.4082],\n",
      "         [0.0372, 0.6614, 0.1232, 0.1782],\n",
      "         [0.0319, 0.5573, 0.1221, 0.2886],\n",
      "         [0.0654, 0.3623, 0.0649, 0.5074],\n",
      "         [0.0510, 0.4232, 0.0703, 0.4555],\n",
      "         [0.0229, 0.6049, 0.1642, 0.2080],\n",
      "         [0.0340, 0.5903, 0.0742, 0.3015],\n",
      "         [0.0603, 0.5507, 0.1457, 0.2433],\n",
      "         [0.0129, 0.8652, 0.0624, 0.0595],\n",
      "         [0.0345, 0.5314, 0.1911, 0.2430],\n",
      "         [0.0102, 0.7159, 0.2045, 0.0695],\n",
      "         [0.0245, 0.7612, 0.1140, 0.1003],\n",
      "         [0.0199, 0.6584, 0.2328, 0.0889],\n",
      "         [0.0408, 0.4202, 0.3097, 0.2292],\n",
      "         [0.0297, 0.8507, 0.0808, 0.0389],\n",
      "         [0.0043, 0.5698, 0.2627, 0.1632],\n",
      "         [0.0110, 0.9024, 0.0274, 0.0591],\n",
      "         [0.0402, 0.5939, 0.1403, 0.2257],\n",
      "         [0.0154, 0.5832, 0.0687, 0.3327],\n",
      "         [0.0194, 0.8322, 0.0669, 0.0815],\n",
      "         [0.0481, 0.4704, 0.0966, 0.3849],\n",
      "         [0.0249, 0.7855, 0.1225, 0.0671],\n",
      "         [0.0167, 0.3559, 0.1455, 0.4818],\n",
      "         [0.0300, 0.7205, 0.1488, 0.1007],\n",
      "         [0.0097, 0.8815, 0.0642, 0.0446],\n",
      "         [0.0168, 0.6539, 0.0873, 0.2419],\n",
      "         [0.0109, 0.8754, 0.0756, 0.0381],\n",
      "         [0.0487, 0.6181, 0.1152, 0.2180],\n",
      "         [0.0150, 0.7808, 0.0625, 0.1416],\n",
      "         [0.0193, 0.3730, 0.0885, 0.5192],\n",
      "         [0.0196, 0.4775, 0.1941, 0.3088],\n",
      "         [0.0383, 0.5962, 0.2282, 0.1373],\n",
      "         [0.0221, 0.5361, 0.0813, 0.3605],\n",
      "         [0.0609, 0.1516, 0.2870, 0.5005],\n",
      "         [0.0800, 0.3691, 0.1727, 0.3782],\n",
      "         [0.0478, 0.4552, 0.2019, 0.2951],\n",
      "         [0.0087, 0.9225, 0.0466, 0.0222],\n",
      "         [0.0318, 0.6247, 0.0602, 0.2833],\n",
      "         [0.0197, 0.4890, 0.1681, 0.3231],\n",
      "         [0.0078, 0.7523, 0.1610, 0.0790],\n",
      "         [0.0041, 0.9346, 0.0504, 0.0109],\n",
      "         [0.0190, 0.6732, 0.1168, 0.1911],\n",
      "         [0.0345, 0.6660, 0.1590, 0.1406],\n",
      "         [0.0353, 0.6474, 0.1106, 0.2067],\n",
      "         [0.0198, 0.9435, 0.0196, 0.0172],\n",
      "         [0.0374, 0.7006, 0.1411, 0.1209],\n",
      "         [0.0571, 0.3941, 0.2528, 0.2960],\n",
      "         [0.0452, 0.5040, 0.1375, 0.3133],\n",
      "         [0.0157, 0.8162, 0.0844, 0.0836],\n",
      "         [0.0299, 0.6344, 0.1580, 0.1777],\n",
      "         [0.0090, 0.4138, 0.0801, 0.4971],\n",
      "         [0.0211, 0.6753, 0.1183, 0.1853],\n",
      "         [0.0528, 0.5546, 0.1547, 0.2379],\n",
      "         [0.0857, 0.3636, 0.2615, 0.2892],\n",
      "         [0.0062, 0.8001, 0.1154, 0.0783],\n",
      "         [0.0242, 0.7682, 0.0967, 0.1109],\n",
      "         [0.0483, 0.3488, 0.2288, 0.3741],\n",
      "         [0.0245, 0.7697, 0.0761, 0.1297],\n",
      "         [0.0239, 0.6262, 0.2036, 0.1463],\n",
      "         [0.0132, 0.8669, 0.0738, 0.0460],\n",
      "         [0.0278, 0.6931, 0.1999, 0.0792],\n",
      "         [0.0293, 0.8234, 0.0912, 0.0561],\n",
      "         [0.0083, 0.8155, 0.0663, 0.1099],\n",
      "         [0.0207, 0.5504, 0.2268, 0.2021],\n",
      "         [0.0162, 0.3709, 0.0288, 0.5840],\n",
      "         [0.0186, 0.6728, 0.2525, 0.0561],\n",
      "         [0.0144, 0.7112, 0.2090, 0.0655],\n",
      "         [0.0361, 0.5936, 0.1898, 0.1806],\n",
      "         [0.0127, 0.8993, 0.0571, 0.0309],\n",
      "         [0.0688, 0.4607, 0.2312, 0.2392],\n",
      "         [0.0261, 0.5595, 0.2554, 0.1590],\n",
      "         [0.0133, 0.6741, 0.2364, 0.0762],\n",
      "         [0.0136, 0.8246, 0.0893, 0.0726],\n",
      "         [0.0303, 0.5581, 0.1756, 0.2359],\n",
      "         [0.0141, 0.8039, 0.0306, 0.1514],\n",
      "         [0.0301, 0.8477, 0.0463, 0.0759],\n",
      "         [0.0137, 0.8102, 0.1103, 0.0658],\n",
      "         [0.0181, 0.7975, 0.1107, 0.0738],\n",
      "         [0.0289, 0.4533, 0.2166, 0.3012],\n",
      "         [0.0726, 0.5326, 0.0916, 0.3032],\n",
      "         [0.0214, 0.6460, 0.0962, 0.2363],\n",
      "         [0.0181, 0.6857, 0.1898, 0.1064],\n",
      "         [0.0214, 0.7844, 0.1062, 0.0879],\n",
      "         [0.0112, 0.4650, 0.1278, 0.3960],\n",
      "         [0.0350, 0.5127, 0.2122, 0.2400],\n",
      "         [0.0409, 0.3659, 0.2190, 0.3742]]], grad_fn=<StackBackward>)\n",
      "Updating critic...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V_trg.shape (after critic):  torch.Size([121])\n",
      "V_trg.shape (after sum):  torch.Size([121])\n",
      "V_trg.shape (after squeeze):  torch.Size([121])\n",
      "V_trg.shape (after squeeze):  tensor([ 0.2241,  0.2141, -0.2116,  0.3722,  0.3254,  1.0426,  0.3312,  0.2949,\n",
      "         0.1325,  0.4083,  0.5798,  0.8059,  0.9436,  0.6693,  0.9693,  0.9345,\n",
      "         0.6457,  0.0142,  0.4662,  0.4120,  0.5218,  0.3693,  0.4911,  0.7913,\n",
      "         0.5940,  0.4541,  0.9294,  0.3552,  0.4957,  0.6121,  0.2034,  0.4555,\n",
      "         1.0980,  0.4745,  0.9856,  0.4800,  0.6455,  0.4161,  0.6312, -0.1739,\n",
      "        -0.0211,  0.4001,  0.7173,  0.8608,  0.9922,  0.4992, -0.0473,  0.0215,\n",
      "         0.7557,  0.4737,  0.3115,  0.6323,  0.5449,  0.3995,  0.6018,  0.3066,\n",
      "         0.2421,  0.8900,  0.4333,  0.3125,  0.2034,  0.6323,  0.8370,  0.6373,\n",
      "         0.5736,  0.1636,  1.0352,  0.3426,  0.4931,  0.1873,  0.1731,  0.1890,\n",
      "         0.2991,  1.0782,  0.3442,  0.2824,  0.1196,  0.0330,  0.1444,  0.7162,\n",
      "         0.4506,  0.4321,  0.4937,  0.9421,  0.6271,  0.3931,  0.2937,  0.6352,\n",
      "         0.4500,  0.5130,  0.1199,  0.9149,  0.2532,  0.5282,  0.2986,  0.3558,\n",
      "         0.5978,  0.3245,  0.6992,  0.9748,  0.4419,  1.2364,  1.1749,  1.3449,\n",
      "         1.1564,  1.0297,  0.7617,  0.7628,  1.2507,  0.7357,  0.2729,  0.6738,\n",
      "         0.8577,  0.5979,  0.7946, -0.0127,  0.0934,  0.9749,  0.8626,  0.9898,\n",
      "         0.8478])\n",
      "V1.shape:  torch.Size([121])\n",
      "V1:  tensor([[0.3912],\n",
      "        [1.9067],\n",
      "        [2.0271],\n",
      "        [1.7836],\n",
      "        [1.4718],\n",
      "        [1.5693],\n",
      "        [0.5142],\n",
      "        [2.3842],\n",
      "        [1.6973],\n",
      "        [0.9458],\n",
      "        [2.0832],\n",
      "        [1.1325],\n",
      "        [2.7277],\n",
      "        [1.7010],\n",
      "        [2.1050],\n",
      "        [1.9547],\n",
      "        [0.4401],\n",
      "        [2.7429],\n",
      "        [2.3421],\n",
      "        [0.8474],\n",
      "        [1.7671],\n",
      "        [0.9763],\n",
      "        [2.5311],\n",
      "        [1.5858],\n",
      "        [1.7456],\n",
      "        [2.7060],\n",
      "        [1.3874],\n",
      "        [2.1208],\n",
      "        [2.5824],\n",
      "        [1.0575],\n",
      "        [1.4623],\n",
      "        [0.7910],\n",
      "        [1.5495],\n",
      "        [0.9571],\n",
      "        [1.6967],\n",
      "        [2.0617],\n",
      "        [2.1928],\n",
      "        [2.6540],\n",
      "        [1.9745],\n",
      "        [2.8251],\n",
      "        [1.2044],\n",
      "        [2.5999],\n",
      "        [1.6484],\n",
      "        [2.4280],\n",
      "        [2.0192],\n",
      "        [1.9249],\n",
      "        [1.5456],\n",
      "        [0.9908],\n",
      "        [0.7819],\n",
      "        [1.2948],\n",
      "        [2.0885],\n",
      "        [3.0599],\n",
      "        [1.8316],\n",
      "        [2.3824],\n",
      "        [2.9396],\n",
      "        [0.9869],\n",
      "        [2.6653],\n",
      "        [2.4095],\n",
      "        [1.2390],\n",
      "        [1.8881],\n",
      "        [2.0158],\n",
      "        [1.4786],\n",
      "        [1.6639],\n",
      "        [1.8734],\n",
      "        [1.2109],\n",
      "        [0.6714],\n",
      "        [1.6940],\n",
      "        [2.3962],\n",
      "        [1.9291],\n",
      "        [1.4848],\n",
      "        [1.7425],\n",
      "        [1.0085],\n",
      "        [1.5252],\n",
      "        [2.9412],\n",
      "        [1.5120],\n",
      "        [1.5244],\n",
      "        [1.4569],\n",
      "        [1.8113],\n",
      "        [2.3302],\n",
      "        [1.0500],\n",
      "        [1.9388],\n",
      "        [2.1130],\n",
      "        [0.6595],\n",
      "        [2.4174],\n",
      "        [1.7698],\n",
      "        [2.3254],\n",
      "        [1.1002],\n",
      "        [1.9787],\n",
      "        [2.9925],\n",
      "        [2.6277],\n",
      "        [1.8354],\n",
      "        [2.2846],\n",
      "        [1.2082],\n",
      "        [3.1721],\n",
      "        [1.3108],\n",
      "        [1.7737],\n",
      "        [2.5826],\n",
      "        [2.2160],\n",
      "        [2.1150],\n",
      "        [1.7368],\n",
      "        [1.7787],\n",
      "        [1.2101],\n",
      "        [1.2283],\n",
      "        [2.2212],\n",
      "        [1.7508],\n",
      "        [2.1569],\n",
      "        [1.8284],\n",
      "        [2.3087],\n",
      "        [0.7989],\n",
      "        [3.2389],\n",
      "        [2.4463],\n",
      "        [1.9536],\n",
      "        [2.7167],\n",
      "        [1.4321],\n",
      "        [1.9575],\n",
      "        [1.1147],\n",
      "        [2.3059],\n",
      "        [1.3828],\n",
      "        [1.4447],\n",
      "        [2.8769],\n",
      "        [0.8165]], grad_fn=<AddmmBackward>)\n",
      "Updating actor...\n",
      "V_trg.shape:  torch.Size([121])\n",
      "V_trg:  tensor([ 0.8222,  1.3571,  0.3833,  0.4231,  0.5797,  1.1279,  0.6471,  0.6101,\n",
      "         0.3070,  0.6818,  0.8251,  0.0874,  0.5536,  0.5684,  0.4247,  0.7913,\n",
      "         0.6259,  0.5833,  0.6165,  0.8454,  0.9355,  0.7903,  1.1429,  0.8550,\n",
      "         0.7631,  0.9026,  0.9332,  0.7252,  0.6160,  0.3215,  0.6005,  0.7114,\n",
      "         0.3844,  0.5820,  0.6976,  0.5377,  1.1383,  0.6260,  0.8169,  0.5078,\n",
      "         0.9091,  1.0234,  0.9061,  0.6788,  0.7717,  0.7871,  0.9915,  0.7830,\n",
      "         0.8457,  0.7755,  1.2696,  0.6074,  0.8870,  0.5186,  0.9871,  0.7097,\n",
      "         0.7146,  0.6423,  0.9222,  0.9617,  0.8646,  0.7034,  0.4407,  0.5113,\n",
      "        -0.0838,  0.5387,  0.6499,  1.0269,  0.4629,  0.8072, -0.1044,  0.5050,\n",
      "         1.0277,  0.9191,  0.4139,  0.5955,  1.0049,  0.0936,  0.7437,  0.8405,\n",
      "         0.9379,  0.8517,  0.3831,  0.2959,  0.5421,  0.7263,  0.7235,  0.5485,\n",
      "         0.1786,  0.3370,  0.4375,  0.6070,  0.7454,  0.5994,  1.1367,  1.2034,\n",
      "         0.3737,  0.8948,  0.1139,  1.4296,  0.4559,  0.9966,  0.3858,  1.8749,\n",
      "         0.3896,  1.0467,  1.2011,  0.9667,  0.5523,  1.3470,  0.8531,  1.0483,\n",
      "         0.7096,  0.7335,  1.0914,  0.8985,  0.4775,  0.9368,  1.0804,  1.4662,\n",
      "         0.5441])\n",
      "V_pred.shape:  torch.Size([121])\n",
      "V_pred:  tensor([ 0.8948,  1.0076,  1.2326,  0.4540,  1.1816,  1.1241,  1.0905,  1.6184,\n",
      "         1.4826,  1.2596,  0.9369,  1.3949,  1.5140,  1.3807,  1.2169,  1.3148,\n",
      "         1.2276,  1.3301,  0.9167,  0.6728,  1.2999,  1.4409,  1.4583,  1.6154,\n",
      "         1.0701,  1.1744,  1.2710,  1.2132,  1.0837,  1.0819,  1.4737,  1.5887,\n",
      "         0.7095,  1.2269,  0.5731,  1.1587,  1.2441,  0.9613,  0.9324,  0.3388,\n",
      "         1.2334,  0.1511,  1.6787,  0.6234,  1.0503,  0.8827,  1.2423,  1.3969,\n",
      "         1.1393,  1.8781,  0.7598,  0.6382,  1.1544,  1.0119,  0.9555,  0.7506,\n",
      "         1.3796,  1.3039,  0.9884,  0.5531,  1.3787,  0.8183,  1.0649,  1.3813,\n",
      "         0.8140,  0.9716,  1.2551,  1.3196,  0.7924,  1.1430,  1.4326,  1.1979,\n",
      "         0.8039,  1.0995,  0.9655,  0.8935,  2.1671,  0.7108,  1.0671,  0.5517,\n",
      "         0.6034,  1.4567,  1.0273,  1.5859,  1.2191,  0.4500,  1.4460,  0.7905,\n",
      "         1.6042,  1.5906,  1.1347,  1.2587,  1.3421,  1.1245,  1.0943,  0.4441,\n",
      "         1.1023,  1.2612,  1.2258,  1.1458,  0.9572,  0.1401,  1.2751,  1.2463,\n",
      "         1.3489,  1.1219,  1.5972,  0.8595,  1.4471,  0.9911,  1.0420,  0.6907,\n",
      "         1.1777,  0.8860,  1.2268,  1.0376, -0.1427,  0.8897,  1.0864,  1.5047,\n",
      "         1.7441])\n",
      "A.shape:  torch.Size([121])\n",
      "A:  tensor([-0.0726,  0.3494, -0.8493, -0.0309, -0.6019,  0.0039, -0.4434, -1.0083,\n",
      "        -1.1757, -0.5779, -0.1118, -1.3075, -0.9604, -0.8123, -0.7922, -0.5235,\n",
      "        -0.6017, -0.7468, -0.3003,  0.1725, -0.3645, -0.6506, -0.3154, -0.7604,\n",
      "        -0.3071, -0.2718, -0.3379, -0.4880, -0.4677, -0.7605, -0.8732, -0.8773,\n",
      "        -0.3250, -0.6450,  0.1245, -0.6210, -0.1058, -0.3352, -0.1155,  0.1690,\n",
      "        -0.3243,  0.8723, -0.7726,  0.0554, -0.2786, -0.0955, -0.2508, -0.6139,\n",
      "        -0.2936, -1.1026,  0.5098, -0.0307, -0.2673, -0.4934,  0.0316, -0.0410,\n",
      "        -0.6651, -0.6615, -0.0661,  0.4086, -0.5140, -0.1148, -0.6242, -0.8700,\n",
      "        -0.8978, -0.4329, -0.6052, -0.2927, -0.3295, -0.3358, -1.5370, -0.6929,\n",
      "         0.2238, -0.1804, -0.5516, -0.2980, -1.1622, -0.6172, -0.3234,  0.2888,\n",
      "         0.3345, -0.6050, -0.6442, -1.2900, -0.6770,  0.2763, -0.7225, -0.2421,\n",
      "        -1.4256, -1.2536, -0.6971, -0.6518, -0.5968, -0.5251,  0.0423,  0.7593,\n",
      "        -0.7286, -0.3664, -1.1119,  0.2838, -0.5012,  0.8565, -0.8893,  0.6286,\n",
      "        -0.9593, -0.0752, -0.3960,  0.1072, -0.8948,  0.3559, -0.1889,  0.3576,\n",
      "        -0.4681, -0.1525, -0.1354, -0.1391,  0.6202,  0.0472, -0.0061, -0.0385,\n",
      "        -1.2001])\n",
      "policy_gradient.shape:  torch.Size([121])\n",
      "policy_gradient:  tensor([-4.1311e-02,  1.4229e+00, -6.9120e-01, -2.8883e-02, -4.6429e-01,\n",
      "         4.4604e-03, -2.0592e-01, -1.2377e+00, -2.1103e-01, -6.4409e-02,\n",
      "        -1.3158e-01, -7.2026e-01, -2.5474e-01, -2.2207e+00, -5.9400e-02,\n",
      "        -9.3291e-02, -1.1905e-01, -1.9872e-01, -2.5248e-02,  1.5528e-01,\n",
      "        -1.1857e-01, -5.8736e-02, -3.6619e-02, -3.0465e-01, -3.1822e-01,\n",
      "        -1.2676e-01, -8.3303e-02, -3.1937e-01, -5.9687e-02, -1.0139e-01,\n",
      "        -5.7147e-01, -3.7734e-01, -2.8406e-01, -9.1536e-01,  2.7765e-01,\n",
      "        -5.5641e-01, -4.3739e-02, -1.9597e-01, -1.1731e-01,  1.3290e-01,\n",
      "        -5.0924e-01,  1.0460e+00, -4.6094e-01,  8.0135e-03, -1.7619e-01,\n",
      "        -3.1932e-02, -6.8461e-02, -2.5657e-01, -3.4411e-01, -1.7835e-01,\n",
      "         2.8669e-01, -3.1573e-03, -5.2514e-01, -5.4300e-01,  5.8106e-03,\n",
      "        -3.9115e-02, -1.6059e-01, -6.8336e-01, -1.5180e-01,  5.1546e-02,\n",
      "        -2.1837e-01, -1.5277e-02, -9.5079e-01, -3.6525e+00, -8.8527e-01,\n",
      "        -3.2001e-01, -3.1301e-01, -1.8246e-01, -9.2204e-01, -3.2646e-01,\n",
      "        -1.2095e+00, -5.5893e-02,  2.8220e-01, -2.0380e-01, -1.5700e-01,\n",
      "        -2.0148e-02, -4.5994e-01, -1.2109e+00, -7.1203e-01,  1.6804e-02,\n",
      "         1.1902e-01, -8.3186e-01, -4.4143e-01, -2.6196e-01, -1.2490e+00,\n",
      "         2.4382e-01, -1.5425e+00, -1.4269e-01, -1.7686e+00, -2.7952e-01,\n",
      "        -1.8380e-01, -6.4088e-01, -1.2191e+00, -8.3573e-01,  6.0428e-03,\n",
      "         2.7838e-01, -1.4159e-01, -7.4743e-02, -6.6400e-01,  1.5262e-01,\n",
      "        -6.8982e-01,  2.9191e-01, -4.6383e-01,  6.6717e-02, -1.3721e+00,\n",
      "        -1.0264e-01, -1.5616e-01,  2.0679e-02, -1.5564e+00,  6.7181e-01,\n",
      "        -3.1218e-02,  7.5253e-02, -1.0593e-01, -1.2064e-01, -8.5303e-02,\n",
      "        -6.0785e-02,  2.3402e-01,  1.1463e-01, -4.6474e-03, -2.5744e-02,\n",
      "        -1.8227e+00], grad_fn=<MulBackward0>)\n",
      "policy_grad:  tensor(-38.2140, grad_fn=<SumBackward0>)\n",
      "Negative entropy:  tensor(-106.3103, grad_fn=<SumBackward0>)\n",
      "Actor loss:  tensor(-39.2771, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distribution:  tensor([[0.0039, 0.6936, 0.0836, 0.2189]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0277, 0.3862, 0.2203, 0.3658]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0097, 0.9280, 0.0336, 0.0288]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0159, 0.7887, 0.0618, 0.1336]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0030, 0.8495, 0.1101, 0.0374]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0104, 0.8503, 0.0696, 0.0697]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0078, 0.7386, 0.0776, 0.1761]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0217, 0.5641, 0.2090, 0.2053]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0413, 0.5305, 0.0718, 0.3563]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0280, 0.8005, 0.0767, 0.0949]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0075, 0.7973, 0.0922, 0.1031]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0157, 0.4676, 0.2330, 0.2837]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0037, 0.9365, 0.0434, 0.0164]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0807, 0.5523, 0.1287, 0.2383]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0458, 0.7009, 0.1139, 0.1394]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0105, 0.6530, 0.1158, 0.2207]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0069, 0.8917, 0.0505, 0.0509]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0089, 0.3412, 0.0512, 0.5987]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0119, 0.5541, 0.0863, 0.3478]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0141, 0.8147, 0.1055, 0.0657]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0068, 0.9000, 0.0366, 0.0566]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0304, 0.3784, 0.1814, 0.4098]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0141, 0.7362, 0.0317, 0.2180]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0118, 0.6799, 0.1589, 0.1494]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0558, 0.4255, 0.0875, 0.4313]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0447, 0.7573, 0.0944, 0.1037]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0070, 0.6363, 0.1785, 0.1781]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0086, 0.6548, 0.1385, 0.1981]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0056, 0.8354, 0.0826, 0.0763]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0089, 0.8045, 0.1206, 0.0659]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0167, 0.6717, 0.0874, 0.2241]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0183, 0.4884, 0.1880, 0.3053]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0542, 0.4294, 0.2301, 0.2862]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0133, 0.8006, 0.0774, 0.1087]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0175, 0.4458, 0.2263, 0.3104]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0213, 0.2814, 0.2215, 0.4758]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0042, 0.6747, 0.0533, 0.2678]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0122, 0.4822, 0.2323, 0.2733]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0060, 0.8903, 0.0405, 0.0632]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0181, 0.6149, 0.1164, 0.2505]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0220, 0.7575, 0.0767, 0.1438]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0211, 0.6447, 0.1131, 0.2211]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0373, 0.3930, 0.0668, 0.5029]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0142, 0.7568, 0.0666, 0.1624]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0231, 0.5388, 0.1043, 0.3337]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0224, 0.5836, 0.1450, 0.2490]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0030, 0.9156, 0.0397, 0.0416]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0597, 0.6598, 0.1203, 0.1602]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0216, 0.6046, 0.0940, 0.2798]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0193, 0.5055, 0.2915, 0.1837]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0250, 0.8523, 0.0463, 0.0764]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0241, 0.3218, 0.1230, 0.5311]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0758, 0.2700, 0.1362, 0.5180]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0096, 0.8123, 0.0386, 0.1395]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0102, 0.4794, 0.1360, 0.3743]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0142, 0.4123, 0.0973, 0.4762]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0196, 0.5876, 0.2467, 0.1462]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0075, 0.6429, 0.0134, 0.3362]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0098, 0.8645, 0.0776, 0.0482]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0252, 0.5458, 0.1084, 0.3206]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0098, 0.9117, 0.0293, 0.0491]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0311, 0.2599, 0.3484, 0.3606]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0181, 0.7154, 0.0960, 0.1705]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0119, 0.6475, 0.0917, 0.2489]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0172, 0.9002, 0.0466, 0.0361]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0135, 0.8612, 0.0761, 0.0491]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0193, 0.7449, 0.1245, 0.1113]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0228, 0.6937, 0.1674, 0.1161]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0053, 0.8401, 0.0777, 0.0769]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0239, 0.5898, 0.1480, 0.2384]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0198, 0.5905, 0.2178, 0.1719]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0854, 0.1603, 0.1632, 0.5911]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0350, 0.4376, 0.2352, 0.2923]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0176, 0.5857, 0.1189, 0.2777]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0092, 0.6696, 0.1084, 0.2127]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0104, 0.6657, 0.0598, 0.2640]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0188, 0.7263, 0.1228, 0.1321]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0478, 0.2299, 0.1598, 0.5625]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0436, 0.4988, 0.1186, 0.3390]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0415, 0.4484, 0.2122, 0.2979]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0280, 0.5173, 0.0698, 0.3849]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0135, 0.7060, 0.0994, 0.1810]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0197, 0.8034, 0.0478, 0.1290]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0396, 0.4741, 0.1558, 0.3304]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0425, 0.5437, 0.2992, 0.1146]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0108, 0.9067, 0.0369, 0.0456]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0130, 0.9254, 0.0305, 0.0310]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0482, 0.3900, 0.2367, 0.3251]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0156, 0.7409, 0.1232, 0.1202]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0135, 0.7713, 0.1646, 0.0506]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0119, 0.7320, 0.0834, 0.1726]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0199, 0.4625, 0.0756, 0.4420]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0145, 0.6486, 0.1282, 0.2086]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0389, 0.2244, 0.0736, 0.6630]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0062, 0.9416, 0.0333, 0.0189]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0195, 0.7114, 0.1053, 0.1638]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0249, 0.8546, 0.0779, 0.0426]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0171, 0.7552, 0.0895, 0.1383]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0390, 0.2595, 0.1606, 0.5409]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0262, 0.5609, 0.2020, 0.2109]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0170, 0.7817, 0.0660, 0.1353]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0053, 0.8048, 0.1119, 0.0781]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0659, 0.4286, 0.0868, 0.4187]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0537, 0.6741, 0.0849, 0.1874]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0324, 0.4549, 0.2682, 0.2445]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0336, 0.6260, 0.2198, 0.1206]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0146, 0.4832, 0.0798, 0.4225]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0428, 0.4880, 0.2549, 0.2143]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0185, 0.8526, 0.0643, 0.0646]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0080, 0.8966, 0.0607, 0.0347]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0395, 0.4147, 0.2388, 0.3069]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0057, 0.8035, 0.1062, 0.0845]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0399, 0.5405, 0.0648, 0.3548]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0166, 0.3920, 0.0454, 0.5460]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0196, 0.5711, 0.0800, 0.3293]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0245, 0.8246, 0.0931, 0.0578]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0149, 0.8029, 0.0665, 0.1157]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0084, 0.9048, 0.0517, 0.0352]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0315, 0.3983, 0.0767, 0.4935]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0233, 0.5852, 0.1904, 0.2011]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0422, 0.3358, 0.1801, 0.4419]], grad_fn=<ExpBackward>)\n",
      "n_step_rewards.shape:  (121,)\n",
      "rewards.shape:  (121,)\n",
      "n_step_rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0.]\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0.]\n",
      "bootstrap:  [False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      "  True]\n",
      "done.shape: (before n_steps) (121,)\n",
      "done: (before n_steps) [False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False]\n",
      "done.shape: (after n_steps) (121,)\n",
      "Gamma_V.shape:  (121,)\n",
      "done: (after n_steps) [False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False]\n",
      "Gamma_V:  [0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.67572905 0.6825546\n",
      " 0.68944909 0.69641322 0.70344769 0.71055323 0.71773053 0.72498034\n",
      " 0.73230337 0.73970037 0.74717209 0.75471929 0.76234271 0.77004315\n",
      " 0.77782136 0.78567814 0.79361428 0.80163059 0.80972787 0.81790694\n",
      " 0.82616862 0.83451376 0.84294319 0.85145777 0.86005835 0.86874581\n",
      " 0.87752102 0.88638487 0.89533825 0.90438208 0.91351725 0.92274469\n",
      " 0.93206535 0.94148015 0.95099005 0.96059601 0.970299   0.9801\n",
      " 0.99      ]\n",
      "old_states.shape:  torch.Size([121, 1, 9, 9])\n",
      "new_states.shape:  torch.Size([121, 1, 9, 9])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_probs:  [tensor(-1.5190, grad_fn=<SelectBackward>), tensor(-1.5127, grad_fn=<SelectBackward>), tensor(-0.0748, grad_fn=<SelectBackward>), tensor(-0.2374, grad_fn=<SelectBackward>), tensor(-0.1631, grad_fn=<SelectBackward>), tensor(-0.1622, grad_fn=<SelectBackward>), tensor(-1.7369, grad_fn=<SelectBackward>), tensor(-1.5835, grad_fn=<SelectBackward>), tensor(-1.0320, grad_fn=<SelectBackward>), tensor(-0.2226, grad_fn=<SelectBackward>), tensor(-0.2266, grad_fn=<SelectBackward>), tensor(-1.4569, grad_fn=<SelectBackward>), tensor(-0.0656, grad_fn=<SelectBackward>), tensor(-2.5171, grad_fn=<SelectBackward>), tensor(-0.3555, grad_fn=<SelectBackward>), tensor(-0.4262, grad_fn=<SelectBackward>), tensor(-2.9779, grad_fn=<SelectBackward>), tensor(-2.9714, grad_fn=<SelectBackward>), tensor(-1.0563, grad_fn=<SelectBackward>), tensor(-0.2049, grad_fn=<SelectBackward>), tensor(-0.1053, grad_fn=<SelectBackward>), tensor(-0.8921, grad_fn=<SelectBackward>), tensor(-0.3062, grad_fn=<SelectBackward>), tensor(-0.3858, grad_fn=<SelectBackward>), tensor(-0.8411, grad_fn=<SelectBackward>), tensor(-0.2780, grad_fn=<SelectBackward>), tensor(-0.4520, grad_fn=<SelectBackward>), tensor(-1.6188, grad_fn=<SelectBackward>), tensor(-0.1798, grad_fn=<SelectBackward>), tensor(-0.2175, grad_fn=<SelectBackward>), tensor(-0.3979, grad_fn=<SelectBackward>), tensor(-0.7167, grad_fn=<SelectBackward>), tensor(-2.9151, grad_fn=<SelectBackward>), tensor(-0.2224, grad_fn=<SelectBackward>), tensor(-0.8079, grad_fn=<SelectBackward>), tensor(-1.5074, grad_fn=<SelectBackward>), tensor(-1.3173, grad_fn=<SelectBackward>), tensor(-1.4597, grad_fn=<SelectBackward>), tensor(-0.1162, grad_fn=<SelectBackward>), tensor(-1.3843, grad_fn=<SelectBackward>), tensor(-0.2777, grad_fn=<SelectBackward>), tensor(-0.4390, grad_fn=<SelectBackward>), tensor(-0.6874, grad_fn=<SelectBackward>), tensor(-0.2786, grad_fn=<SelectBackward>), tensor(-0.6184, grad_fn=<SelectBackward>), tensor(-3.7979, grad_fn=<SelectBackward>), tensor(-0.0881, grad_fn=<SelectBackward>), tensor(-0.4158, grad_fn=<SelectBackward>), tensor(-1.2737, grad_fn=<SelectBackward>), tensor(-1.2329, grad_fn=<SelectBackward>), tensor(-0.1598, grad_fn=<SelectBackward>), tensor(-1.1338, grad_fn=<SelectBackward>), tensor(-1.9938, grad_fn=<SelectBackward>), tensor(-1.9699, grad_fn=<SelectBackward>), tensor(-0.7352, grad_fn=<SelectBackward>), tensor(-0.8861, grad_fn=<SelectBackward>), tensor(-0.5318, grad_fn=<SelectBackward>), tensor(-1.0900, grad_fn=<SelectBackward>), tensor(-0.1456, grad_fn=<SelectBackward>), tensor(-1.1376, grad_fn=<SelectBackward>), tensor(-0.0924, grad_fn=<SelectBackward>), tensor(-1.0544, grad_fn=<SelectBackward>), tensor(-0.3349, grad_fn=<SelectBackward>), tensor(-0.4347, grad_fn=<SelectBackward>), tensor(-0.1052, grad_fn=<SelectBackward>), tensor(-0.1494, grad_fn=<SelectBackward>), tensor(-0.2945, grad_fn=<SelectBackward>), tensor(-2.1534, grad_fn=<SelectBackward>), tensor(-0.1742, grad_fn=<SelectBackward>), tensor(-0.5280, grad_fn=<SelectBackward>), tensor(-0.5268, grad_fn=<SelectBackward>), tensor(-1.8307, grad_fn=<SelectBackward>), tensor(-1.4473, grad_fn=<SelectBackward>), tensor(-1.2811, grad_fn=<SelectBackward>), tensor(-0.4010, grad_fn=<SelectBackward>), tensor(-0.4069, grad_fn=<SelectBackward>), tensor(-0.3197, grad_fn=<SelectBackward>), tensor(-0.5753, grad_fn=<SelectBackward>), tensor(-0.6956, grad_fn=<SelectBackward>), tensor(-0.8021, grad_fn=<SelectBackward>), tensor(-0.6591, grad_fn=<SelectBackward>), tensor(-0.3481, grad_fn=<SelectBackward>), tensor(-0.2189, grad_fn=<SelectBackward>), tensor(-0.7462, grad_fn=<SelectBackward>), tensor(-0.6094, grad_fn=<SelectBackward>), tensor(-0.0979, grad_fn=<SelectBackward>), tensor(-0.0775, grad_fn=<SelectBackward>), tensor(-1.4412, grad_fn=<SelectBackward>), tensor(-0.2999, grad_fn=<SelectBackward>), tensor(-1.8040, grad_fn=<SelectBackward>), tensor(-0.3120, grad_fn=<SelectBackward>), tensor(-0.7712, grad_fn=<SelectBackward>), tensor(-0.4329, grad_fn=<SelectBackward>), tensor(-0.4110, grad_fn=<SelectBackward>), tensor(-0.0602, grad_fn=<SelectBackward>), tensor(-0.3405, grad_fn=<SelectBackward>), tensor(-0.1571, grad_fn=<SelectBackward>), tensor(-0.2808, grad_fn=<SelectBackward>), tensor(-1.8290, grad_fn=<SelectBackward>), tensor(-0.5782, grad_fn=<SelectBackward>), tensor(-4.0762, grad_fn=<SelectBackward>), tensor(-0.2172, grad_fn=<SelectBackward>), tensor(-0.8707, grad_fn=<SelectBackward>), tensor(-0.3944, grad_fn=<SelectBackward>), tensor(-0.7877, grad_fn=<SelectBackward>), tensor(-2.1155, grad_fn=<SelectBackward>), tensor(-0.8617, grad_fn=<SelectBackward>), tensor(-0.7174, grad_fn=<SelectBackward>), tensor(-0.1594, grad_fn=<SelectBackward>), tensor(-0.1092, grad_fn=<SelectBackward>), tensor(-1.1812, grad_fn=<SelectBackward>), tensor(-0.2188, grad_fn=<SelectBackward>), tensor(-0.6154, grad_fn=<SelectBackward>), tensor(-0.6052, grad_fn=<SelectBackward>), tensor(-0.5601, grad_fn=<SelectBackward>), tensor(-0.1929, grad_fn=<SelectBackward>), tensor(-0.2195, grad_fn=<SelectBackward>), tensor(-3.3481, grad_fn=<SelectBackward>), tensor(-0.9206, grad_fn=<SelectBackward>), tensor(-1.6589, grad_fn=<SelectBackward>), tensor(-1.7145, grad_fn=<SelectBackward>)]\n",
      "log_probs:  tensor([-1.5190, -1.5127, -0.0748, -0.2374, -0.1631, -0.1622, -1.7369, -1.5835,\n",
      "        -1.0320, -0.2226, -0.2266, -1.4569, -0.0656, -2.5171, -0.3555, -0.4262,\n",
      "        -2.9779, -2.9714, -1.0563, -0.2049, -0.1053, -0.8921, -0.3062, -0.3858,\n",
      "        -0.8411, -0.2780, -0.4520, -1.6188, -0.1798, -0.2175, -0.3979, -0.7167,\n",
      "        -2.9151, -0.2224, -0.8079, -1.5074, -1.3173, -1.4597, -0.1162, -1.3843,\n",
      "        -0.2777, -0.4390, -0.6874, -0.2786, -0.6184, -3.7979, -0.0881, -0.4158,\n",
      "        -1.2737, -1.2329, -0.1598, -1.1338, -1.9938, -1.9699, -0.7352, -0.8861,\n",
      "        -0.5318, -1.0900, -0.1456, -1.1376, -0.0924, -1.0544, -0.3349, -0.4347,\n",
      "        -0.1052, -0.1494, -0.2945, -2.1534, -0.1742, -0.5280, -0.5268, -1.8307,\n",
      "        -1.4473, -1.2811, -0.4010, -0.4069, -0.3197, -0.5753, -0.6956, -0.8021,\n",
      "        -0.6591, -0.3481, -0.2189, -0.7462, -0.6094, -0.0979, -0.0775, -1.4412,\n",
      "        -0.2999, -1.8040, -0.3120, -0.7712, -0.4329, -0.4110, -0.0602, -0.3405,\n",
      "        -0.1571, -0.2808, -1.8290, -0.5782, -4.0762, -0.2172, -0.8707, -0.3944,\n",
      "        -0.7877, -2.1155, -0.8617, -0.7174, -0.1594, -0.1092, -1.1812, -0.2188,\n",
      "        -0.6154, -0.6052, -0.5601, -0.1929, -0.2195, -3.3481, -0.9206, -1.6589,\n",
      "        -1.7145], grad_fn=<StackBackward>)\n",
      "distributions.shape:  torch.Size([1, 121, 4])\n",
      "distributions:  tensor([[[0.0039, 0.6936, 0.0836, 0.2189],\n",
      "         [0.0277, 0.3862, 0.2203, 0.3658],\n",
      "         [0.0097, 0.9280, 0.0336, 0.0288],\n",
      "         [0.0159, 0.7887, 0.0618, 0.1336],\n",
      "         [0.0030, 0.8495, 0.1101, 0.0374],\n",
      "         [0.0104, 0.8503, 0.0696, 0.0697],\n",
      "         [0.0078, 0.7386, 0.0776, 0.1761],\n",
      "         [0.0217, 0.5641, 0.2090, 0.2053],\n",
      "         [0.0413, 0.5305, 0.0718, 0.3563],\n",
      "         [0.0280, 0.8005, 0.0767, 0.0949],\n",
      "         [0.0075, 0.7973, 0.0922, 0.1031],\n",
      "         [0.0157, 0.4676, 0.2330, 0.2837],\n",
      "         [0.0037, 0.9365, 0.0434, 0.0164],\n",
      "         [0.0807, 0.5523, 0.1287, 0.2383],\n",
      "         [0.0458, 0.7009, 0.1139, 0.1394],\n",
      "         [0.0105, 0.6530, 0.1158, 0.2207],\n",
      "         [0.0069, 0.8917, 0.0505, 0.0509],\n",
      "         [0.0089, 0.3412, 0.0512, 0.5987],\n",
      "         [0.0119, 0.5541, 0.0863, 0.3478],\n",
      "         [0.0141, 0.8147, 0.1055, 0.0657],\n",
      "         [0.0068, 0.9000, 0.0366, 0.0566],\n",
      "         [0.0304, 0.3784, 0.1814, 0.4098],\n",
      "         [0.0141, 0.7362, 0.0317, 0.2180],\n",
      "         [0.0118, 0.6799, 0.1589, 0.1494],\n",
      "         [0.0558, 0.4255, 0.0875, 0.4313],\n",
      "         [0.0447, 0.7573, 0.0944, 0.1037],\n",
      "         [0.0070, 0.6363, 0.1785, 0.1781],\n",
      "         [0.0086, 0.6548, 0.1385, 0.1981],\n",
      "         [0.0056, 0.8354, 0.0826, 0.0763],\n",
      "         [0.0089, 0.8045, 0.1206, 0.0659],\n",
      "         [0.0167, 0.6717, 0.0874, 0.2241],\n",
      "         [0.0183, 0.4884, 0.1880, 0.3053],\n",
      "         [0.0542, 0.4294, 0.2301, 0.2862],\n",
      "         [0.0133, 0.8006, 0.0774, 0.1087],\n",
      "         [0.0175, 0.4458, 0.2263, 0.3104],\n",
      "         [0.0213, 0.2814, 0.2215, 0.4758],\n",
      "         [0.0042, 0.6747, 0.0533, 0.2678],\n",
      "         [0.0122, 0.4822, 0.2323, 0.2733],\n",
      "         [0.0060, 0.8903, 0.0405, 0.0632],\n",
      "         [0.0181, 0.6149, 0.1164, 0.2505],\n",
      "         [0.0220, 0.7575, 0.0767, 0.1438],\n",
      "         [0.0211, 0.6447, 0.1131, 0.2211],\n",
      "         [0.0373, 0.3930, 0.0668, 0.5029],\n",
      "         [0.0142, 0.7568, 0.0666, 0.1624],\n",
      "         [0.0231, 0.5388, 0.1043, 0.3337],\n",
      "         [0.0224, 0.5836, 0.1450, 0.2490],\n",
      "         [0.0030, 0.9156, 0.0397, 0.0416],\n",
      "         [0.0597, 0.6598, 0.1203, 0.1602],\n",
      "         [0.0216, 0.6046, 0.0940, 0.2798],\n",
      "         [0.0193, 0.5055, 0.2915, 0.1837],\n",
      "         [0.0250, 0.8523, 0.0463, 0.0764],\n",
      "         [0.0241, 0.3218, 0.1230, 0.5311],\n",
      "         [0.0758, 0.2700, 0.1362, 0.5180],\n",
      "         [0.0096, 0.8123, 0.0386, 0.1395],\n",
      "         [0.0102, 0.4794, 0.1360, 0.3743],\n",
      "         [0.0142, 0.4123, 0.0973, 0.4762],\n",
      "         [0.0196, 0.5876, 0.2467, 0.1462],\n",
      "         [0.0075, 0.6429, 0.0134, 0.3362],\n",
      "         [0.0098, 0.8645, 0.0776, 0.0482],\n",
      "         [0.0252, 0.5458, 0.1084, 0.3206],\n",
      "         [0.0098, 0.9117, 0.0293, 0.0491],\n",
      "         [0.0311, 0.2599, 0.3484, 0.3606],\n",
      "         [0.0181, 0.7154, 0.0960, 0.1705],\n",
      "         [0.0119, 0.6475, 0.0917, 0.2489],\n",
      "         [0.0172, 0.9002, 0.0466, 0.0361],\n",
      "         [0.0135, 0.8612, 0.0761, 0.0491],\n",
      "         [0.0193, 0.7449, 0.1245, 0.1113],\n",
      "         [0.0228, 0.6937, 0.1674, 0.1161],\n",
      "         [0.0053, 0.8401, 0.0777, 0.0769],\n",
      "         [0.0239, 0.5898, 0.1480, 0.2384],\n",
      "         [0.0198, 0.5905, 0.2178, 0.1719],\n",
      "         [0.0854, 0.1603, 0.1632, 0.5911],\n",
      "         [0.0350, 0.4376, 0.2352, 0.2923],\n",
      "         [0.0176, 0.5857, 0.1189, 0.2777],\n",
      "         [0.0092, 0.6696, 0.1084, 0.2127],\n",
      "         [0.0104, 0.6657, 0.0598, 0.2640],\n",
      "         [0.0188, 0.7263, 0.1228, 0.1321],\n",
      "         [0.0478, 0.2299, 0.1598, 0.5625],\n",
      "         [0.0436, 0.4988, 0.1186, 0.3390],\n",
      "         [0.0415, 0.4484, 0.2122, 0.2979],\n",
      "         [0.0280, 0.5173, 0.0698, 0.3849],\n",
      "         [0.0135, 0.7060, 0.0994, 0.1810],\n",
      "         [0.0197, 0.8034, 0.0478, 0.1290],\n",
      "         [0.0396, 0.4741, 0.1558, 0.3304],\n",
      "         [0.0425, 0.5437, 0.2992, 0.1146],\n",
      "         [0.0108, 0.9067, 0.0369, 0.0456],\n",
      "         [0.0130, 0.9254, 0.0305, 0.0310],\n",
      "         [0.0482, 0.3900, 0.2367, 0.3251],\n",
      "         [0.0156, 0.7409, 0.1232, 0.1202],\n",
      "         [0.0135, 0.7713, 0.1646, 0.0506],\n",
      "         [0.0119, 0.7320, 0.0834, 0.1726],\n",
      "         [0.0199, 0.4625, 0.0756, 0.4420],\n",
      "         [0.0145, 0.6486, 0.1282, 0.2086],\n",
      "         [0.0389, 0.2244, 0.0736, 0.6630],\n",
      "         [0.0062, 0.9416, 0.0333, 0.0189],\n",
      "         [0.0195, 0.7114, 0.1053, 0.1638],\n",
      "         [0.0249, 0.8546, 0.0779, 0.0426],\n",
      "         [0.0171, 0.7552, 0.0895, 0.1383],\n",
      "         [0.0390, 0.2595, 0.1606, 0.5409],\n",
      "         [0.0262, 0.5609, 0.2020, 0.2109],\n",
      "         [0.0170, 0.7817, 0.0660, 0.1353],\n",
      "         [0.0053, 0.8048, 0.1119, 0.0781],\n",
      "         [0.0659, 0.4286, 0.0868, 0.4187],\n",
      "         [0.0537, 0.6741, 0.0849, 0.1874],\n",
      "         [0.0324, 0.4549, 0.2682, 0.2445],\n",
      "         [0.0336, 0.6260, 0.2198, 0.1206],\n",
      "         [0.0146, 0.4832, 0.0798, 0.4225],\n",
      "         [0.0428, 0.4880, 0.2549, 0.2143],\n",
      "         [0.0185, 0.8526, 0.0643, 0.0646],\n",
      "         [0.0080, 0.8966, 0.0607, 0.0347],\n",
      "         [0.0395, 0.4147, 0.2388, 0.3069],\n",
      "         [0.0057, 0.8035, 0.1062, 0.0845],\n",
      "         [0.0399, 0.5405, 0.0648, 0.3548],\n",
      "         [0.0166, 0.3920, 0.0454, 0.5460],\n",
      "         [0.0196, 0.5711, 0.0800, 0.3293],\n",
      "         [0.0245, 0.8246, 0.0931, 0.0578],\n",
      "         [0.0149, 0.8029, 0.0665, 0.1157],\n",
      "         [0.0084, 0.9048, 0.0517, 0.0352],\n",
      "         [0.0315, 0.3983, 0.0767, 0.4935],\n",
      "         [0.0233, 0.5852, 0.1904, 0.2011],\n",
      "         [0.0422, 0.3358, 0.1801, 0.4419]]], grad_fn=<StackBackward>)\n",
      "Updating critic...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V_trg.shape (after critic):  torch.Size([121])\n",
      "V_trg.shape (after sum):  torch.Size([121])\n",
      "V_trg.shape (after squeeze):  torch.Size([121])\n",
      "V_trg.shape (after squeeze):  tensor([ 0.6294,  0.2954,  0.6881,  0.4879,  0.3624,  0.3493,  0.5807,  0.2752,\n",
      "         0.6929,  0.2941,  0.3132,  0.9823,  0.2140,  0.3253,  0.3342,  0.6804,\n",
      "        -0.3964,  0.5970,  0.7848,  0.6319,  1.0561,  0.7121,  0.5154,  0.4233,\n",
      "         0.4176,  0.2586,  0.9142,  0.7955,  0.3017,  0.8682,  1.1710,  0.6997,\n",
      "         0.4158,  0.5419,  0.7709,  0.9176,  1.0152,  0.4087,  0.8303,  0.2624,\n",
      "         0.4808,  0.5831,  0.4994,  0.2423,  0.9124,  0.8784,  0.5230,  0.7515,\n",
      "         0.4496,  0.5449,  0.4970,  0.6810,  0.6290,  0.4851,  0.8371,  0.8721,\n",
      "         0.6920,  0.4038,  0.3005,  0.4533,  0.2489,  0.7282,  0.2501,  0.5623,\n",
      "         0.1544,  0.5456,  0.3159,  0.0119,  0.3379,  0.0752,  1.1461,  0.5609,\n",
      "         1.0726,  0.4783,  1.1571,  0.0993,  0.5408,  0.6234,  0.1158, -0.1740,\n",
      "         0.5285,  0.5535,  0.5230,  0.0977,  0.0864,  0.4259,  0.3077,  0.8779,\n",
      "         0.6655,  0.3093,  0.2667,  0.3085,  0.7819,  0.6665,  0.6387,  0.7732,\n",
      "        -0.0849,  1.1781,  1.1290,  1.2586, -0.0960,  0.3856,  1.0214,  0.9331,\n",
      "         0.7381,  0.4591,  0.6048,  0.6077,  0.4622,  0.7445,  0.8591,  0.7511,\n",
      "         1.3253,  0.9707,  1.2767,  0.8225,  1.6503,  0.8886,  0.7854,  0.8320,\n",
      "         0.8084])\n",
      "V1.shape:  torch.Size([121])\n",
      "V1:  tensor([[1.8000],\n",
      "        [1.5003],\n",
      "        [1.5422],\n",
      "        [1.4954],\n",
      "        [1.9622],\n",
      "        [1.8451],\n",
      "        [1.6643],\n",
      "        [1.7694],\n",
      "        [1.2341],\n",
      "        [2.7948],\n",
      "        [0.9200],\n",
      "        [2.2298],\n",
      "        [1.8535],\n",
      "        [1.0946],\n",
      "        [1.4825],\n",
      "        [1.7168],\n",
      "        [1.5563],\n",
      "        [0.9858],\n",
      "        [1.8238],\n",
      "        [0.5372],\n",
      "        [2.1244],\n",
      "        [1.1343],\n",
      "        [1.5114],\n",
      "        [2.3310],\n",
      "        [1.2460],\n",
      "        [1.7137],\n",
      "        [1.3385],\n",
      "        [2.6244],\n",
      "        [2.0722],\n",
      "        [1.6729],\n",
      "        [2.2946],\n",
      "        [2.0757],\n",
      "        [1.3763],\n",
      "        [0.9285],\n",
      "        [2.3526],\n",
      "        [2.0483],\n",
      "        [1.8832],\n",
      "        [1.5019],\n",
      "        [1.2757],\n",
      "        [1.4203],\n",
      "        [1.6667],\n",
      "        [1.0909],\n",
      "        [2.2898],\n",
      "        [1.6287],\n",
      "        [1.8417],\n",
      "        [2.3807],\n",
      "        [1.6533],\n",
      "        [1.1770],\n",
      "        [2.9506],\n",
      "        [1.9340],\n",
      "        [1.1596],\n",
      "        [0.9877],\n",
      "        [1.4852],\n",
      "        [1.1225],\n",
      "        [1.2789],\n",
      "        [1.9838],\n",
      "        [1.5081],\n",
      "        [2.2148],\n",
      "        [1.4665],\n",
      "        [2.4102],\n",
      "        [1.6146],\n",
      "        [1.3025],\n",
      "        [1.8157],\n",
      "        [1.6598],\n",
      "        [0.9710],\n",
      "        [2.2757],\n",
      "        [1.5725],\n",
      "        [1.6502],\n",
      "        [1.6695],\n",
      "        [2.1570],\n",
      "        [1.3872],\n",
      "        [0.9778],\n",
      "        [2.6458],\n",
      "        [1.9345],\n",
      "        [2.3444],\n",
      "        [1.2295],\n",
      "        [1.4074],\n",
      "        [1.8905],\n",
      "        [1.3777],\n",
      "        [2.0389],\n",
      "        [1.5645],\n",
      "        [2.7117],\n",
      "        [1.7320],\n",
      "        [1.2699],\n",
      "        [1.8390],\n",
      "        [1.6573],\n",
      "        [1.4913],\n",
      "        [1.4411],\n",
      "        [1.9562],\n",
      "        [1.3952],\n",
      "        [2.0006],\n",
      "        [2.2289],\n",
      "        [1.4615],\n",
      "        [1.5775],\n",
      "        [1.8008],\n",
      "        [1.9176],\n",
      "        [1.2277],\n",
      "        [0.6382],\n",
      "        [1.0362],\n",
      "        [1.7080],\n",
      "        [1.7239],\n",
      "        [1.8879],\n",
      "        [1.1732],\n",
      "        [1.6106],\n",
      "        [1.1038],\n",
      "        [2.0814],\n",
      "        [1.5365],\n",
      "        [1.3717],\n",
      "        [1.5631],\n",
      "        [1.3590],\n",
      "        [1.0113],\n",
      "        [1.6380],\n",
      "        [1.5064],\n",
      "        [2.4720],\n",
      "        [1.9458],\n",
      "        [2.3202],\n",
      "        [0.7492],\n",
      "        [2.1539],\n",
      "        [1.5892],\n",
      "        [1.8096],\n",
      "        [1.3746]], grad_fn=<AddmmBackward>)\n",
      "Updating actor...\n",
      "V_trg.shape:  torch.Size([121])\n",
      "V_trg:  tensor([0.3612, 0.4307, 0.9453, 0.4989, 0.2707, 0.3141, 0.1740, 0.4893, 0.8671,\n",
      "        0.5208, 0.6291, 0.7122, 0.2660, 0.6381, 0.5462, 0.6974, 0.9506, 0.2714,\n",
      "        0.3121, 0.6476, 0.9161, 0.5573, 0.3795, 0.9002, 1.4947, 0.4120, 0.6631,\n",
      "        0.5022, 0.4818, 0.5722, 0.2855, 0.8900, 0.7223, 0.9112, 0.0621, 0.8863,\n",
      "        0.7394, 0.2561, 0.5005, 0.4859, 0.8350, 0.8135, 0.4685, 0.9236, 0.3972,\n",
      "        0.8566, 0.3763, 0.6225, 0.8036, 0.5670, 0.6898, 0.6651, 0.4082, 0.8704,\n",
      "        0.6252, 0.5129, 0.3858, 1.1618, 0.0085, 0.5280, 0.6685, 0.8294, 0.5646,\n",
      "        0.3007, 0.4729, 0.7526, 0.5608, 1.0218, 0.9620, 0.4823, 0.6023, 0.8468,\n",
      "        0.5459, 0.4859, 0.5619, 0.6364, 1.2051, 0.5389, 0.5842, 0.9751, 0.8049,\n",
      "        0.7235, 0.9612, 0.6664, 0.7771, 0.4646, 0.6721, 0.3100, 0.5513, 0.2892,\n",
      "        0.8528, 1.0981, 0.7555, 0.5992, 0.7222, 0.4363, 0.7117, 1.4418, 0.5678,\n",
      "        0.5892, 0.9275, 0.5724, 0.0814, 0.3859, 0.9991, 0.6950, 1.2349, 0.8151,\n",
      "        1.3259, 0.8659, 0.2438, 0.6176, 0.4996, 1.1913, 0.9273, 0.9388, 1.2193,\n",
      "        0.6595, 0.9461, 1.0973, 1.1020])\n",
      "V_pred.shape:  torch.Size([121])\n",
      "V_pred:  tensor([ 1.0054,  0.7204,  0.5100,  1.2330,  0.8145,  0.7569,  0.6381,  1.6001,\n",
      "         0.9892,  1.0985,  0.9808,  1.3697,  1.1053,  0.2554,  1.5643,  0.8411,\n",
      "         0.7522,  1.1225,  0.5702,  1.4353,  1.0132,  0.9120,  0.6092,  0.8113,\n",
      "         0.8200,  1.1572,  0.8074,  0.4071,  1.1635,  0.8081,  1.0184,  0.3571,\n",
      "         1.3665,  1.2007,  0.8886,  1.0630,  0.6905,  0.6661,  0.3158,  0.8453,\n",
      "         0.8449,  0.5254,  0.7368,  1.2987,  0.9649,  0.7155,  1.1883,  0.4750,\n",
      "         0.5718,  1.6900,  1.2110,  1.0812,  1.1294,  0.9371,  0.1072,  1.3776,\n",
      "         0.4467,  0.7850,  0.7804,  0.5925,  0.4105,  0.6073,  1.2886,  0.0469,\n",
      "         0.6150,  1.2259,  0.4928,  1.2650, -0.0505,  1.0009,  0.3448,  1.2283,\n",
      "         0.9761,  0.7558,  0.7169,  1.4640,  0.4520,  0.8841,  0.6564,  1.7591,\n",
      "         1.0102,  0.4487,  0.8299,  0.1496,  0.6791,  0.8848,  0.9912,  0.9953,\n",
      "         1.5193,  0.7606,  0.4520,  1.4171,  0.6047,  0.7171,  1.1370,  1.0278,\n",
      "         0.9818,  0.3098,  1.2579,  1.0735,  0.3935,  1.7280,  0.3989,  0.6573,\n",
      "         1.4607,  1.0819,  1.5096,  1.4814,  0.4775,  1.3246,  1.3296,  1.2358,\n",
      "         0.0499,  1.1096,  0.7585,  1.2491,  1.0296,  0.4576,  0.8266,  1.3244,\n",
      "        -0.0496])\n",
      "A.shape:  torch.Size([121])\n",
      "A:  tensor([-0.6442, -0.2897,  0.4353, -0.7341, -0.5438, -0.4428, -0.4641, -1.1108,\n",
      "        -0.1220, -0.5776, -0.3517, -0.6575, -0.8393,  0.3828, -1.0181, -0.1437,\n",
      "         0.1984, -0.8511, -0.2581, -0.7877, -0.0971, -0.3547, -0.2297,  0.0889,\n",
      "         0.6747, -0.7452, -0.1443,  0.0951, -0.6817, -0.2359, -0.7329,  0.5329,\n",
      "        -0.6442, -0.2895, -0.8265, -0.1767,  0.0489, -0.4100,  0.1847, -0.3594,\n",
      "        -0.0098,  0.2881, -0.2682, -0.3751, -0.5677,  0.1411, -0.8120,  0.1475,\n",
      "         0.2318, -1.1229, -0.5211, -0.4162, -0.7211, -0.0667,  0.5180, -0.8647,\n",
      "        -0.0610,  0.3767, -0.7718, -0.0645,  0.2580,  0.2221, -0.7240,  0.2538,\n",
      "        -0.1421, -0.4732,  0.0680, -0.2432,  1.0125, -0.5187,  0.2575, -0.3814,\n",
      "        -0.4302, -0.2699, -0.1550, -0.8276,  0.7531, -0.3452, -0.0722, -0.7840,\n",
      "        -0.2053,  0.2748,  0.1313,  0.5168,  0.0979, -0.4202, -0.3191, -0.6853,\n",
      "        -0.9681, -0.4714,  0.4008, -0.3190,  0.1508, -0.1179, -0.4148, -0.5914,\n",
      "        -0.2700,  1.1320, -0.6901, -0.4842,  0.5340, -1.1556, -0.3175, -0.2714,\n",
      "        -0.4616, -0.3869, -0.2747, -0.6663,  0.8484, -0.4587, -1.0858, -0.6181,\n",
      "         0.4496,  0.0817,  0.1689, -0.3103,  0.1897,  0.2018,  0.1194, -0.2271,\n",
      "         1.1516])\n",
      "policy_gradient.shape:  torch.Size([121])\n",
      "policy_gradient:  tensor([-0.9785, -0.4382,  0.0325, -0.1743, -0.0887, -0.0718, -0.8061, -1.7590,\n",
      "        -0.1259, -0.1286, -0.0797, -0.9579, -0.0551,  0.9634, -0.3619, -0.0613,\n",
      "         0.5910, -2.5290, -0.2726, -0.1614, -0.0102, -0.3164, -0.0703,  0.0343,\n",
      "         0.5675, -0.2072, -0.0652,  0.1540, -0.1226, -0.0513, -0.2916,  0.3819,\n",
      "        -1.8778, -0.0644, -0.6677, -0.2664,  0.0644, -0.5985,  0.0215, -0.4976,\n",
      "        -0.0027,  0.1265, -0.1844, -0.1045, -0.3511,  0.5358, -0.0716,  0.0613,\n",
      "         0.2952, -1.3844, -0.0833, -0.4719, -1.4378, -0.1314,  0.3808, -0.7662,\n",
      "        -0.0324,  0.4106, -0.1124, -0.0733,  0.0238,  0.2342, -0.2425,  0.1103,\n",
      "        -0.0149, -0.0707,  0.0200, -0.5237,  0.1764, -0.2739,  0.1356, -0.6983,\n",
      "        -0.6226, -0.3458, -0.0622, -0.3367,  0.2408, -0.1986, -0.0502, -0.6289,\n",
      "        -0.1353,  0.0957,  0.0287,  0.3857,  0.0597, -0.0412, -0.0247, -0.9876,\n",
      "        -0.2903, -0.8504,  0.1250, -0.2460,  0.0653, -0.0484, -0.0250, -0.2014,\n",
      "        -0.0424,  0.3179, -1.2622, -0.2800,  2.1768, -0.2509, -0.2764, -0.1071,\n",
      "        -0.3636, -0.8184, -0.2367, -0.4780,  0.1353, -0.0501, -1.2825, -0.1352,\n",
      "         0.2767,  0.0494,  0.0946, -0.0599,  0.0416,  0.6757,  0.1100, -0.3767,\n",
      "         1.9744], grad_fn=<MulBackward0>)\n",
      "policy_grad:  tensor(-20.1297, grad_fn=<SumBackward0>)\n",
      "Negative entropy:  tensor(-103.4843, grad_fn=<SumBackward0>)\n",
      "Actor loss:  tensor(-21.1645, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distribution:  tensor([[0.0086, 0.6765, 0.1725, 0.1424]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0101, 0.6443, 0.2498, 0.0959]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0028, 0.4219, 0.0306, 0.5447]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0103, 0.4773, 0.2700, 0.2424]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0484, 0.1290, 0.0952, 0.7274]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0158, 0.8202, 0.0496, 0.1143]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0173, 0.2714, 0.1020, 0.6093]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0178, 0.3822, 0.0756, 0.5245]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0437, 0.7786, 0.0830, 0.0947]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0345, 0.4950, 0.1490, 0.3215]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0172, 0.7416, 0.0514, 0.1898]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0119, 0.6436, 0.0818, 0.2627]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0167, 0.4493, 0.1931, 0.3409]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0113, 0.6460, 0.1195, 0.2232]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0155, 0.5982, 0.1371, 0.2492]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0324, 0.6122, 0.0507, 0.3047]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0203, 0.4069, 0.0621, 0.5106]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0066, 0.7247, 0.0400, 0.2287]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0215, 0.5645, 0.1674, 0.2466]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0115, 0.6454, 0.0634, 0.2798]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0107, 0.6603, 0.1329, 0.1961]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0055, 0.3804, 0.0131, 0.6010]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0222, 0.7887, 0.0819, 0.1072]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0103, 0.7856, 0.1317, 0.0724]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0144, 0.5069, 0.1823, 0.2965]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0431, 0.5134, 0.1646, 0.2789]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0294, 0.5912, 0.0919, 0.2875]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0087, 0.8131, 0.0353, 0.1429]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0170, 0.8024, 0.0790, 0.1017]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0179, 0.7915, 0.0410, 0.1496]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0120, 0.7680, 0.0878, 0.1321]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0073, 0.9432, 0.0363, 0.0132]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0227, 0.4940, 0.0300, 0.4533]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0247, 0.2150, 0.2132, 0.5472]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0056, 0.7560, 0.0785, 0.1600]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0232, 0.2082, 0.0387, 0.7299]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0079, 0.7829, 0.0570, 0.1522]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0191, 0.1425, 0.0715, 0.7669]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0263, 0.4366, 0.1252, 0.4118]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0632, 0.6996, 0.0854, 0.1518]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0371, 0.5219, 0.0372, 0.4037]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0050, 0.8347, 0.0418, 0.1185]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0382, 0.5229, 0.1563, 0.2826]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0128, 0.3299, 0.1231, 0.5342]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0046, 0.7267, 0.0546, 0.2141]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0101, 0.5436, 0.0454, 0.4009]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0115, 0.6326, 0.1095, 0.2464]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0367, 0.6999, 0.1222, 0.1412]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0046, 0.8532, 0.0601, 0.0822]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0133, 0.3948, 0.0330, 0.5589]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0073, 0.7416, 0.0845, 0.1667]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0227, 0.8837, 0.0615, 0.0320]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0485, 0.5103, 0.2316, 0.2097]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0141, 0.5294, 0.1078, 0.3487]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0201, 0.3940, 0.1766, 0.4094]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0459, 0.4140, 0.1381, 0.4020]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0159, 0.7857, 0.0753, 0.1230]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0102, 0.4899, 0.0581, 0.4418]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0443, 0.6297, 0.1057, 0.2203]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0223, 0.4658, 0.1946, 0.3173]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0335, 0.4987, 0.1167, 0.3510]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0250, 0.8359, 0.0219, 0.1171]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0268, 0.3605, 0.3047, 0.3080]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0204, 0.5989, 0.0993, 0.2814]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0139, 0.8015, 0.1150, 0.0696]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0146, 0.2231, 0.1033, 0.6590]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0078, 0.3311, 0.0776, 0.5834]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0069, 0.8858, 0.0399, 0.0674]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0058, 0.8519, 0.0388, 0.1034]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0156, 0.7293, 0.0687, 0.1864]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0108, 0.7255, 0.1338, 0.1299]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0245, 0.3973, 0.2401, 0.3381]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0289, 0.2960, 0.2351, 0.4400]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0141, 0.1457, 0.0700, 0.7701]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0109, 0.4007, 0.0220, 0.5664]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0143, 0.3956, 0.1482, 0.4419]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0121, 0.7454, 0.0915, 0.1510]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0061, 0.6464, 0.0815, 0.2660]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0111, 0.8877, 0.0509, 0.0502]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0321, 0.4761, 0.0854, 0.4065]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0208, 0.5128, 0.0908, 0.3756]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0085, 0.7215, 0.1070, 0.1629]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0548, 0.1128, 0.1969, 0.6355]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0425, 0.5269, 0.1198, 0.3107]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0092, 0.3490, 0.0332, 0.6086]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0137, 0.6029, 0.2213, 0.1621]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0423, 0.4818, 0.2275, 0.2484]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0245, 0.5962, 0.0629, 0.3164]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0060, 0.8404, 0.0975, 0.0562]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0208, 0.5571, 0.0603, 0.3617]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0306, 0.2444, 0.1062, 0.6188]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0140, 0.5916, 0.0805, 0.3140]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0096, 0.7110, 0.0199, 0.2595]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0547, 0.3664, 0.2443, 0.3346]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0182, 0.8595, 0.0600, 0.0624]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0129, 0.8137, 0.0703, 0.1031]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0137, 0.6694, 0.0875, 0.2294]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0304, 0.3371, 0.0723, 0.5602]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0078, 0.8788, 0.0492, 0.0642]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0216, 0.7321, 0.1330, 0.1133]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0107, 0.8640, 0.0610, 0.0643]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0199, 0.5355, 0.1110, 0.3335]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0229, 0.6537, 0.0457, 0.2776]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0437, 0.1883, 0.1434, 0.6245]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0158, 0.6799, 0.0656, 0.2388]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0260, 0.2170, 0.0747, 0.6823]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0170, 0.4336, 0.2385, 0.3108]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0078, 0.7362, 0.0727, 0.1833]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0363, 0.5271, 0.2711, 0.1656]], grad_fn=<ExpBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distribution:  tensor([[0.0417, 0.5725, 0.1130, 0.2728]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0460, 0.3279, 0.0913, 0.5348]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0170, 0.3854, 0.0511, 0.5465]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0194, 0.5878, 0.0770, 0.3158]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0094, 0.7249, 0.0232, 0.2425]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0128, 0.8643, 0.0619, 0.0610]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0237, 0.3697, 0.2389, 0.3676]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0232, 0.8238, 0.0744, 0.0787]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0158, 0.6084, 0.1324, 0.2434]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0118, 0.7916, 0.0367, 0.1599]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0069, 0.7238, 0.0896, 0.1797]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0318, 0.5124, 0.2104, 0.2455]], grad_fn=<ExpBackward>)\n",
      "n_step_rewards.shape:  (121,)\n",
      "rewards.shape:  (121,)\n",
      "n_step_rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0.]\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0.]\n",
      "bootstrap:  [False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      "  True]\n",
      "done.shape: (before n_steps) (121,)\n",
      "done: (before n_steps) [False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False]\n",
      "done.shape: (after n_steps) (121,)\n",
      "Gamma_V.shape:  (121,)\n",
      "done: (after n_steps) [False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False]\n",
      "Gamma_V:  [0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.67572905 0.6825546\n",
      " 0.68944909 0.69641322 0.70344769 0.71055323 0.71773053 0.72498034\n",
      " 0.73230337 0.73970037 0.74717209 0.75471929 0.76234271 0.77004315\n",
      " 0.77782136 0.78567814 0.79361428 0.80163059 0.80972787 0.81790694\n",
      " 0.82616862 0.83451376 0.84294319 0.85145777 0.86005835 0.86874581\n",
      " 0.87752102 0.88638487 0.89533825 0.90438208 0.91351725 0.92274469\n",
      " 0.93206535 0.94148015 0.95099005 0.96059601 0.970299   0.9801\n",
      " 0.99      ]\n",
      "old_states.shape:  torch.Size([121, 1, 9, 9])\n",
      "new_states.shape:  torch.Size([121, 1, 9, 9])\n",
      "log_probs:  [tensor(-1.7574, grad_fn=<SelectBackward>), tensor(-1.3872, grad_fn=<SelectBackward>), tensor(-0.6075, grad_fn=<SelectBackward>), tensor(-0.7395, grad_fn=<SelectBackward>), tensor(-0.3182, grad_fn=<SelectBackward>), tensor(-0.1982, grad_fn=<SelectBackward>), tensor(-1.3043, grad_fn=<SelectBackward>), tensor(-0.9619, grad_fn=<SelectBackward>), tensor(-2.3570, grad_fn=<SelectBackward>), tensor(-3.3678, grad_fn=<SelectBackward>), tensor(-1.6618, grad_fn=<SelectBackward>), tensor(-1.3367, grad_fn=<SelectBackward>), tensor(-0.8000, grad_fn=<SelectBackward>), tensor(-1.4997, grad_fn=<SelectBackward>), tensor(-0.5139, grad_fn=<SelectBackward>), tensor(-0.4907, grad_fn=<SelectBackward>), tensor(-2.7787, grad_fn=<SelectBackward>), tensor(-0.3220, grad_fn=<SelectBackward>), tensor(-3.8420, grad_fn=<SelectBackward>), tensor(-0.4380, grad_fn=<SelectBackward>), tensor(-1.6294, grad_fn=<SelectBackward>), tensor(-0.9666, grad_fn=<SelectBackward>), tensor(-0.2374, grad_fn=<SelectBackward>), tensor(-0.2413, grad_fn=<SelectBackward>), tensor(-1.2158, grad_fn=<SelectBackward>), tensor(-1.2768, grad_fn=<SelectBackward>), tensor(-0.5256, grad_fn=<SelectBackward>), tensor(-0.2069, grad_fn=<SelectBackward>), tensor(-2.5385, grad_fn=<SelectBackward>), tensor(-0.2338, grad_fn=<SelectBackward>), tensor(-2.0239, grad_fn=<SelectBackward>), tensor(-0.0584, grad_fn=<SelectBackward>), tensor(-0.7911, grad_fn=<SelectBackward>), tensor(-1.5456, grad_fn=<SelectBackward>), tensor(-0.2798, grad_fn=<SelectBackward>), tensor(-0.3149, grad_fn=<SelectBackward>), tensor(-4.8417, grad_fn=<SelectBackward>), tensor(-0.2654, grad_fn=<SelectBackward>), tensor(-0.8287, grad_fn=<SelectBackward>), tensor(-0.3572, grad_fn=<SelectBackward>), tensor(-0.9071, grad_fn=<SelectBackward>), tensor(-0.1807, grad_fn=<SelectBackward>), tensor(-1.8558, grad_fn=<SelectBackward>), tensor(-1.1090, grad_fn=<SelectBackward>), tensor(-0.3192, grad_fn=<SelectBackward>), tensor(-0.9140, grad_fn=<SelectBackward>), tensor(-2.2118, grad_fn=<SelectBackward>), tensor(-1.9579, grad_fn=<SelectBackward>), tensor(-0.1588, grad_fn=<SelectBackward>), tensor(-0.9295, grad_fn=<SelectBackward>), tensor(-0.2990, grad_fn=<SelectBackward>), tensor(-0.1237, grad_fn=<SelectBackward>), tensor(-1.5622, grad_fn=<SelectBackward>), tensor(-1.0536, grad_fn=<SelectBackward>), tensor(-0.8931, grad_fn=<SelectBackward>), tensor(-0.9112, grad_fn=<SelectBackward>), tensor(-2.0953, grad_fn=<SelectBackward>), tensor(-0.7136, grad_fn=<SelectBackward>), tensor(-0.4625, grad_fn=<SelectBackward>), tensor(-0.7640, grad_fn=<SelectBackward>), tensor(-2.1478, grad_fn=<SelectBackward>), tensor(-0.1792, grad_fn=<SelectBackward>), tensor(-1.0202, grad_fn=<SelectBackward>), tensor(-2.3096, grad_fn=<SelectBackward>), tensor(-0.2213, grad_fn=<SelectBackward>), tensor(-4.2277, grad_fn=<SelectBackward>), tensor(-1.1052, grad_fn=<SelectBackward>), tensor(-0.1212, grad_fn=<SelectBackward>), tensor(-2.2687, grad_fn=<SelectBackward>), tensor(-0.3157, grad_fn=<SelectBackward>), tensor(-2.0408, grad_fn=<SelectBackward>), tensor(-1.4267, grad_fn=<SelectBackward>), tensor(-1.2174, grad_fn=<SelectBackward>), tensor(-0.2612, grad_fn=<SelectBackward>), tensor(-0.5685, grad_fn=<SelectBackward>), tensor(-0.8167, grad_fn=<SelectBackward>), tensor(-0.2938, grad_fn=<SelectBackward>), tensor(-1.3241, grad_fn=<SelectBackward>), tensor(-0.1191, grad_fn=<SelectBackward>), tensor(-0.9002, grad_fn=<SelectBackward>), tensor(-0.9792, grad_fn=<SelectBackward>), tensor(-0.3264, grad_fn=<SelectBackward>), tensor(-0.4533, grad_fn=<SelectBackward>), tensor(-0.6407, grad_fn=<SelectBackward>), tensor(-1.0527, grad_fn=<SelectBackward>), tensor(-0.5059, grad_fn=<SelectBackward>), tensor(-0.7303, grad_fn=<SelectBackward>), tensor(-2.7664, grad_fn=<SelectBackward>), tensor(-0.1739, grad_fn=<SelectBackward>), tensor(-3.8713, grad_fn=<SelectBackward>), tensor(-0.4800, grad_fn=<SelectBackward>), tensor(-4.2717, grad_fn=<SelectBackward>), tensor(-3.9171, grad_fn=<SelectBackward>), tensor(-1.0040, grad_fn=<SelectBackward>), tensor(-0.1514, grad_fn=<SelectBackward>), tensor(-0.2061, grad_fn=<SelectBackward>), tensor(-1.4723, grad_fn=<SelectBackward>), tensor(-1.0872, grad_fn=<SelectBackward>), tensor(-0.1292, grad_fn=<SelectBackward>), tensor(-0.3118, grad_fn=<SelectBackward>), tensor(-0.1462, grad_fn=<SelectBackward>), tensor(-1.0981, grad_fn=<SelectBackward>), tensor(-0.4251, grad_fn=<SelectBackward>), tensor(-0.4708, grad_fn=<SelectBackward>), tensor(-0.3859, grad_fn=<SelectBackward>), tensor(-0.3823, grad_fn=<SelectBackward>), tensor(-0.8355, grad_fn=<SelectBackward>), tensor(-2.6213, grad_fn=<SelectBackward>), tensor(-0.6404, grad_fn=<SelectBackward>), tensor(-0.5577, grad_fn=<SelectBackward>), tensor(-1.1151, grad_fn=<SelectBackward>), tensor(-0.6042, grad_fn=<SelectBackward>), tensor(-1.1525, grad_fn=<SelectBackward>), tensor(-0.3217, grad_fn=<SelectBackward>), tensor(-0.1458, grad_fn=<SelectBackward>), tensor(-1.0007, grad_fn=<SelectBackward>), tensor(-0.1939, grad_fn=<SelectBackward>), tensor(-0.4969, grad_fn=<SelectBackward>), tensor(-0.2337, grad_fn=<SelectBackward>), tensor(-0.3233, grad_fn=<SelectBackward>), tensor(-0.6687, grad_fn=<SelectBackward>)]\n",
      "log_probs:  tensor([-1.7574, -1.3872, -0.6075, -0.7395, -0.3182, -0.1982, -1.3043, -0.9619,\n",
      "        -2.3570, -3.3678, -1.6618, -1.3367, -0.8000, -1.4997, -0.5139, -0.4907,\n",
      "        -2.7787, -0.3220, -3.8420, -0.4380, -1.6294, -0.9666, -0.2374, -0.2413,\n",
      "        -1.2158, -1.2768, -0.5256, -0.2069, -2.5385, -0.2338, -2.0239, -0.0584,\n",
      "        -0.7911, -1.5456, -0.2798, -0.3149, -4.8417, -0.2654, -0.8287, -0.3572,\n",
      "        -0.9071, -0.1807, -1.8558, -1.1090, -0.3192, -0.9140, -2.2118, -1.9579,\n",
      "        -0.1588, -0.9295, -0.2990, -0.1237, -1.5622, -1.0536, -0.8931, -0.9112,\n",
      "        -2.0953, -0.7136, -0.4625, -0.7640, -2.1478, -0.1792, -1.0202, -2.3096,\n",
      "        -0.2213, -4.2277, -1.1052, -0.1212, -2.2687, -0.3157, -2.0408, -1.4267,\n",
      "        -1.2174, -0.2612, -0.5685, -0.8167, -0.2938, -1.3241, -0.1191, -0.9002,\n",
      "        -0.9792, -0.3264, -0.4533, -0.6407, -1.0527, -0.5059, -0.7303, -2.7664,\n",
      "        -0.1739, -3.8713, -0.4800, -4.2717, -3.9171, -1.0040, -0.1514, -0.2061,\n",
      "        -1.4723, -1.0872, -0.1292, -0.3118, -0.1462, -1.0981, -0.4251, -0.4708,\n",
      "        -0.3859, -0.3823, -0.8355, -2.6213, -0.6404, -0.5577, -1.1151, -0.6042,\n",
      "        -1.1525, -0.3217, -0.1458, -1.0007, -0.1939, -0.4969, -0.2337, -0.3233,\n",
      "        -0.6687], grad_fn=<StackBackward>)\n",
      "distributions.shape:  torch.Size([1, 121, 4])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distributions:  tensor([[[0.0086, 0.6765, 0.1725, 0.1424],\n",
      "         [0.0101, 0.6443, 0.2498, 0.0959],\n",
      "         [0.0028, 0.4219, 0.0306, 0.5447],\n",
      "         [0.0103, 0.4773, 0.2700, 0.2424],\n",
      "         [0.0484, 0.1290, 0.0952, 0.7274],\n",
      "         [0.0158, 0.8202, 0.0496, 0.1143],\n",
      "         [0.0173, 0.2714, 0.1020, 0.6093],\n",
      "         [0.0178, 0.3822, 0.0756, 0.5245],\n",
      "         [0.0437, 0.7786, 0.0830, 0.0947],\n",
      "         [0.0345, 0.4950, 0.1490, 0.3215],\n",
      "         [0.0172, 0.7416, 0.0514, 0.1898],\n",
      "         [0.0119, 0.6436, 0.0818, 0.2627],\n",
      "         [0.0167, 0.4493, 0.1931, 0.3409],\n",
      "         [0.0113, 0.6460, 0.1195, 0.2232],\n",
      "         [0.0155, 0.5982, 0.1371, 0.2492],\n",
      "         [0.0324, 0.6122, 0.0507, 0.3047],\n",
      "         [0.0203, 0.4069, 0.0621, 0.5106],\n",
      "         [0.0066, 0.7247, 0.0400, 0.2287],\n",
      "         [0.0215, 0.5645, 0.1674, 0.2466],\n",
      "         [0.0115, 0.6454, 0.0634, 0.2798],\n",
      "         [0.0107, 0.6603, 0.1329, 0.1961],\n",
      "         [0.0055, 0.3804, 0.0131, 0.6010],\n",
      "         [0.0222, 0.7887, 0.0819, 0.1072],\n",
      "         [0.0103, 0.7856, 0.1317, 0.0724],\n",
      "         [0.0144, 0.5069, 0.1823, 0.2965],\n",
      "         [0.0431, 0.5134, 0.1646, 0.2789],\n",
      "         [0.0294, 0.5912, 0.0919, 0.2875],\n",
      "         [0.0087, 0.8131, 0.0353, 0.1429],\n",
      "         [0.0170, 0.8024, 0.0790, 0.1017],\n",
      "         [0.0179, 0.7915, 0.0410, 0.1496],\n",
      "         [0.0120, 0.7680, 0.0878, 0.1321],\n",
      "         [0.0073, 0.9432, 0.0363, 0.0132],\n",
      "         [0.0227, 0.4940, 0.0300, 0.4533],\n",
      "         [0.0247, 0.2150, 0.2132, 0.5472],\n",
      "         [0.0056, 0.7560, 0.0785, 0.1600],\n",
      "         [0.0232, 0.2082, 0.0387, 0.7299],\n",
      "         [0.0079, 0.7829, 0.0570, 0.1522],\n",
      "         [0.0191, 0.1425, 0.0715, 0.7669],\n",
      "         [0.0263, 0.4366, 0.1252, 0.4118],\n",
      "         [0.0632, 0.6996, 0.0854, 0.1518],\n",
      "         [0.0371, 0.5219, 0.0372, 0.4037],\n",
      "         [0.0050, 0.8347, 0.0418, 0.1185],\n",
      "         [0.0382, 0.5229, 0.1563, 0.2826],\n",
      "         [0.0128, 0.3299, 0.1231, 0.5342],\n",
      "         [0.0046, 0.7267, 0.0546, 0.2141],\n",
      "         [0.0101, 0.5436, 0.0454, 0.4009],\n",
      "         [0.0115, 0.6326, 0.1095, 0.2464],\n",
      "         [0.0367, 0.6999, 0.1222, 0.1412],\n",
      "         [0.0046, 0.8532, 0.0601, 0.0822],\n",
      "         [0.0133, 0.3948, 0.0330, 0.5589],\n",
      "         [0.0073, 0.7416, 0.0845, 0.1667],\n",
      "         [0.0227, 0.8837, 0.0615, 0.0320],\n",
      "         [0.0485, 0.5103, 0.2316, 0.2097],\n",
      "         [0.0141, 0.5294, 0.1078, 0.3487],\n",
      "         [0.0201, 0.3940, 0.1766, 0.4094],\n",
      "         [0.0459, 0.4140, 0.1381, 0.4020],\n",
      "         [0.0159, 0.7857, 0.0753, 0.1230],\n",
      "         [0.0102, 0.4899, 0.0581, 0.4418],\n",
      "         [0.0443, 0.6297, 0.1057, 0.2203],\n",
      "         [0.0223, 0.4658, 0.1946, 0.3173],\n",
      "         [0.0335, 0.4987, 0.1167, 0.3510],\n",
      "         [0.0250, 0.8359, 0.0219, 0.1171],\n",
      "         [0.0268, 0.3605, 0.3047, 0.3080],\n",
      "         [0.0204, 0.5989, 0.0993, 0.2814],\n",
      "         [0.0139, 0.8015, 0.1150, 0.0696],\n",
      "         [0.0146, 0.2231, 0.1033, 0.6590],\n",
      "         [0.0078, 0.3311, 0.0776, 0.5834],\n",
      "         [0.0069, 0.8858, 0.0399, 0.0674],\n",
      "         [0.0058, 0.8519, 0.0388, 0.1034],\n",
      "         [0.0156, 0.7293, 0.0687, 0.1864],\n",
      "         [0.0108, 0.7255, 0.1338, 0.1299],\n",
      "         [0.0245, 0.3973, 0.2401, 0.3381],\n",
      "         [0.0289, 0.2960, 0.2351, 0.4400],\n",
      "         [0.0141, 0.1457, 0.0700, 0.7701],\n",
      "         [0.0109, 0.4007, 0.0220, 0.5664],\n",
      "         [0.0143, 0.3956, 0.1482, 0.4419],\n",
      "         [0.0121, 0.7454, 0.0915, 0.1510],\n",
      "         [0.0061, 0.6464, 0.0815, 0.2660],\n",
      "         [0.0111, 0.8877, 0.0509, 0.0502],\n",
      "         [0.0321, 0.4761, 0.0854, 0.4065],\n",
      "         [0.0208, 0.5128, 0.0908, 0.3756],\n",
      "         [0.0085, 0.7215, 0.1070, 0.1629],\n",
      "         [0.0548, 0.1128, 0.1969, 0.6355],\n",
      "         [0.0425, 0.5269, 0.1198, 0.3107],\n",
      "         [0.0092, 0.3490, 0.0332, 0.6086],\n",
      "         [0.0137, 0.6029, 0.2213, 0.1621],\n",
      "         [0.0423, 0.4818, 0.2275, 0.2484],\n",
      "         [0.0245, 0.5962, 0.0629, 0.3164],\n",
      "         [0.0060, 0.8404, 0.0975, 0.0562],\n",
      "         [0.0208, 0.5571, 0.0603, 0.3617],\n",
      "         [0.0306, 0.2444, 0.1062, 0.6188],\n",
      "         [0.0140, 0.5916, 0.0805, 0.3140],\n",
      "         [0.0096, 0.7110, 0.0199, 0.2595],\n",
      "         [0.0547, 0.3664, 0.2443, 0.3346],\n",
      "         [0.0182, 0.8595, 0.0600, 0.0624],\n",
      "         [0.0129, 0.8137, 0.0703, 0.1031],\n",
      "         [0.0137, 0.6694, 0.0875, 0.2294],\n",
      "         [0.0304, 0.3371, 0.0723, 0.5602],\n",
      "         [0.0078, 0.8788, 0.0492, 0.0642],\n",
      "         [0.0216, 0.7321, 0.1330, 0.1133],\n",
      "         [0.0107, 0.8640, 0.0610, 0.0643],\n",
      "         [0.0199, 0.5355, 0.1110, 0.3335],\n",
      "         [0.0229, 0.6537, 0.0457, 0.2776],\n",
      "         [0.0437, 0.1883, 0.1434, 0.6245],\n",
      "         [0.0158, 0.6799, 0.0656, 0.2388],\n",
      "         [0.0260, 0.2170, 0.0747, 0.6823],\n",
      "         [0.0170, 0.4336, 0.2385, 0.3108],\n",
      "         [0.0078, 0.7362, 0.0727, 0.1833],\n",
      "         [0.0363, 0.5271, 0.2711, 0.1656],\n",
      "         [0.0417, 0.5725, 0.1130, 0.2728],\n",
      "         [0.0460, 0.3279, 0.0913, 0.5348],\n",
      "         [0.0170, 0.3854, 0.0511, 0.5465],\n",
      "         [0.0194, 0.5878, 0.0770, 0.3158],\n",
      "         [0.0094, 0.7249, 0.0232, 0.2425],\n",
      "         [0.0128, 0.8643, 0.0619, 0.0610],\n",
      "         [0.0237, 0.3697, 0.2389, 0.3676],\n",
      "         [0.0232, 0.8238, 0.0744, 0.0787],\n",
      "         [0.0158, 0.6084, 0.1324, 0.2434],\n",
      "         [0.0118, 0.7916, 0.0367, 0.1599],\n",
      "         [0.0069, 0.7238, 0.0896, 0.1797],\n",
      "         [0.0318, 0.5124, 0.2104, 0.2455]]], grad_fn=<StackBackward>)\n",
      "Updating critic...\n",
      "V_trg.shape (after critic):  torch.Size([121])\n",
      "V_trg.shape (after sum):  torch.Size([121])\n",
      "V_trg.shape (after squeeze):  torch.Size([121])\n",
      "V_trg.shape (after squeeze):  tensor([ 1.0808,  0.4750,  0.1216,  0.5924,  0.8446,  0.5156,  0.5128,  0.5365,\n",
      "         0.4817,  0.7575,  0.6758,  0.4949,  0.8124,  0.4126,  0.6868,  0.5952,\n",
      "         0.3258,  0.8550,  0.0323,  0.3754,  0.9660,  0.1131,  0.9615,  0.9453,\n",
      "         0.2078,  0.6625,  0.5516,  0.6063,  0.6218,  0.5836,  0.5423,  0.6845,\n",
      "         0.5775,  0.8542,  0.7248,  0.3987,  0.2649,  0.2161,  0.0603,  0.0158,\n",
      "         0.6014,  0.5075,  0.4793,  0.4734,  0.4093,  0.2342,  0.8204,  0.6804,\n",
      "         0.4696,  0.3102,  1.1122,  0.6501,  0.4207,  0.5489,  0.5132,  1.1981,\n",
      "         0.4446,  0.5801,  0.4948,  0.5779,  0.0516,  0.1817,  0.4559,  0.7637,\n",
      "         0.5091,  0.6512,  0.5953,  0.6539,  0.9660,  0.1929,  0.4487,  0.7903,\n",
      "         0.5745,  0.3810,  0.0385,  0.3476,  0.4845,  0.0723,  0.5638,  0.5065,\n",
      "         0.1487,  0.7148,  0.5360,  1.1122,  0.8736,  0.2683,  0.6876,  0.0279,\n",
      "         0.3385,  0.5000, -0.0031,  0.6090,  0.4053,  0.9262,  0.2364,  0.2747,\n",
      "         0.6627,  0.8475,  0.4738,  0.8136,  1.1818,  0.8470,  0.5904,  0.5563,\n",
      "         0.8928,  1.2580,  0.5265,  0.8609,  0.7754,  0.6863,  0.3461,  0.6692,\n",
      "         0.5737,  1.2798,  0.3499,  1.4727,  1.2806,  0.6528,  1.1847,  0.7935,\n",
      "         0.7021])\n",
      "V1.shape:  torch.Size([121])\n",
      "V1:  tensor([[1.3663],\n",
      "        [1.4522],\n",
      "        [2.5337],\n",
      "        [1.3243],\n",
      "        [1.6002],\n",
      "        [0.7591],\n",
      "        [1.4481],\n",
      "        [1.2942],\n",
      "        [1.8801],\n",
      "        [1.5286],\n",
      "        [2.4899],\n",
      "        [0.7125],\n",
      "        [1.6986],\n",
      "        [2.2683],\n",
      "        [1.7489],\n",
      "        [1.6460],\n",
      "        [1.8448],\n",
      "        [1.1073],\n",
      "        [1.3694],\n",
      "        [2.2221],\n",
      "        [1.1389],\n",
      "        [1.6399],\n",
      "        [2.0304],\n",
      "        [0.4972],\n",
      "        [1.7773],\n",
      "        [2.0160],\n",
      "        [1.6304],\n",
      "        [1.7690],\n",
      "        [1.3198],\n",
      "        [1.5909],\n",
      "        [1.4007],\n",
      "        [1.1950],\n",
      "        [1.7483],\n",
      "        [1.1970],\n",
      "        [1.7171],\n",
      "        [2.0958],\n",
      "        [1.5424],\n",
      "        [0.7004],\n",
      "        [1.0005],\n",
      "        [1.7254],\n",
      "        [1.7315],\n",
      "        [1.0186],\n",
      "        [1.0400],\n",
      "        [1.5333],\n",
      "        [1.7666],\n",
      "        [0.6716],\n",
      "        [1.8410],\n",
      "        [0.8741],\n",
      "        [0.7519],\n",
      "        [1.5855],\n",
      "        [1.6462],\n",
      "        [1.7341],\n",
      "        [0.9103],\n",
      "        [1.5960],\n",
      "        [1.4976],\n",
      "        [1.0751],\n",
      "        [1.1810],\n",
      "        [0.7496],\n",
      "        [2.1975],\n",
      "        [0.8226],\n",
      "        [0.1951],\n",
      "        [0.7125],\n",
      "        [1.7463],\n",
      "        [2.0508],\n",
      "        [1.2204],\n",
      "        [1.5749],\n",
      "        [1.6646],\n",
      "        [1.8725],\n",
      "        [1.7815],\n",
      "        [1.1356],\n",
      "        [1.3587],\n",
      "        [1.4139],\n",
      "        [1.2350],\n",
      "        [1.1770],\n",
      "        [0.7668],\n",
      "        [1.0447],\n",
      "        [2.2859],\n",
      "        [2.2118],\n",
      "        [1.8901],\n",
      "        [0.6198],\n",
      "        [2.3134],\n",
      "        [0.8246],\n",
      "        [1.2467],\n",
      "        [1.2129],\n",
      "        [0.6599],\n",
      "        [1.2281],\n",
      "        [1.3438],\n",
      "        [1.3236],\n",
      "        [1.7070],\n",
      "        [1.2077],\n",
      "        [1.2806],\n",
      "        [2.8342],\n",
      "        [1.1018],\n",
      "        [1.6757],\n",
      "        [1.0505],\n",
      "        [1.7490],\n",
      "        [0.4914],\n",
      "        [1.8705],\n",
      "        [1.9581],\n",
      "        [1.3966],\n",
      "        [1.4628],\n",
      "        [1.6585],\n",
      "        [1.1035],\n",
      "        [0.9524],\n",
      "        [1.8479],\n",
      "        [2.2540],\n",
      "        [2.2574],\n",
      "        [0.8361],\n",
      "        [0.6942],\n",
      "        [1.8441],\n",
      "        [1.5035],\n",
      "        [1.6029],\n",
      "        [1.6453],\n",
      "        [2.6402],\n",
      "        [1.5744],\n",
      "        [1.2863],\n",
      "        [1.9423],\n",
      "        [0.9414],\n",
      "        [1.6277],\n",
      "        [0.9270],\n",
      "        [1.5292]], grad_fn=<AddmmBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating actor...\n",
      "V_trg.shape:  torch.Size([121])\n",
      "V_trg:  tensor([ 0.7877,  0.6252,  0.6084,  0.4328,  0.6751,  0.3080,  0.6060,  0.6187,\n",
      "         0.9163,  0.3512,  0.0694,  0.6070,  0.1658,  0.3665,  0.5509,  0.3520,\n",
      "         0.4976,  0.2073,  0.5805,  0.9061,  0.4036,  0.5312,  0.9205,  0.7904,\n",
      "         0.7873, -0.2200,  0.5297,  0.4026,  0.5305,  0.3743,  0.8038,  0.2984,\n",
      "         0.5264,  0.2920,  0.7706,  0.5850,  0.7887,  0.2196,  0.3252,  0.6935,\n",
      "         0.4145,  0.2629,  0.6057,  0.1738,  0.8565,  0.6073,  0.3433,  0.9763,\n",
      "         0.5796,  0.4530,  0.6586,  0.2016,  0.3084,  0.5687,  0.6980,  0.8844,\n",
      "         0.9527,  0.4560,  0.6909,  0.9366,  0.2795,  0.5318,  0.6623,  0.4997,\n",
      "         0.6522,  0.3971,  0.3030,  0.5393,  0.3894,  0.6299,  0.6985,  0.4324,\n",
      "         0.6001,  0.3121,  0.7017,  0.6144,  0.5015,  0.8701,  0.3389,  0.2263,\n",
      "         0.8082,  0.5128,  0.5293,  0.6253,  0.6716,  0.4833,  0.6909,  0.6404,\n",
      "         0.7825,  0.2521,  0.5218,  0.4531,  0.3452,  1.0266,  0.6864,  0.5157,\n",
      "         0.4575,  0.5177,  0.5022,  0.0545,  0.4818, -0.0186,  0.1810,  0.6558,\n",
      "         0.4761,  0.9858,  0.6487,  0.6920,  1.3175,  1.0175,  0.4502,  1.1352,\n",
      "         0.5476,  0.3542,  1.3354,  0.6377,  0.3844,  0.3908,  1.3096,  0.4515,\n",
      "         0.7342])\n",
      "V_pred.shape:  torch.Size([121])\n",
      "V_pred:  tensor([0.9496, 0.4438, 1.1245, 1.4443, 0.0595, 0.4337, 0.8470, 0.6835, 0.4473,\n",
      "        0.1141, 1.2302, 1.4338, 1.5069, 1.2403, 0.6805, 0.1160, 0.2197, 0.7167,\n",
      "        0.5989, 1.3597, 0.9720, 0.9971, 0.5574, 1.4346, 0.8638, 0.6781, 0.8294,\n",
      "        0.6486, 0.8008, 1.1452, 0.3951, 0.7935, 0.5937, 0.8581, 1.0332, 0.9838,\n",
      "        1.0798, 0.7047, 0.4592, 0.6168, 0.7152, 1.3126, 0.7370, 0.8452, 0.9524,\n",
      "        0.7018, 0.5070, 0.3921, 0.6635, 0.8351, 1.0287, 1.1585, 0.3225, 0.5499,\n",
      "        0.5807, 0.9628, 1.5619, 1.1435, 0.9384, 0.5563, 1.2326, 1.2216, 0.7428,\n",
      "        0.8554, 0.9033, 1.1215, 1.1602, 0.6049, 0.6527, 1.1528, 0.4927, 0.8463,\n",
      "        0.5286, 0.7457, 0.7549, 0.3394, 0.1419, 1.0751, 0.6265, 0.9455, 1.1356,\n",
      "        0.7824, 0.6323, 0.2281, 0.8800, 0.5405, 0.4726, 0.9090, 0.6882, 0.8192,\n",
      "        0.7923, 0.0395, 0.9380, 1.0572, 0.9423, 0.7370, 0.3310, 0.2150, 0.3683,\n",
      "        1.2480, 0.7873, 0.1817, 0.2704, 1.5343, 0.8320, 0.7214, 1.0923, 0.4912,\n",
      "        0.7828, 1.1502, 0.5339, 0.7898, 0.6558, 0.4184, 0.7588, 0.3556, 0.5469,\n",
      "        1.0933, 0.8819, 0.6844, 1.0754])\n",
      "A.shape:  torch.Size([121])\n",
      "A:  tensor([-0.1620,  0.1814, -0.5161, -1.0116,  0.6155, -0.1257, -0.2409, -0.0648,\n",
      "         0.4690,  0.2371, -1.1609, -0.8268, -1.3410, -0.8738, -0.1296,  0.2361,\n",
      "         0.2779, -0.5095, -0.0184, -0.4535, -0.5683, -0.4659,  0.3631, -0.6442,\n",
      "        -0.0765, -0.8981, -0.2997, -0.2460, -0.2703, -0.7709,  0.4087, -0.4951,\n",
      "        -0.0673, -0.5661, -0.2626, -0.3989, -0.2912, -0.4851, -0.1340,  0.0768,\n",
      "        -0.3007, -1.0498, -0.1313, -0.6714, -0.0959, -0.0945, -0.1637,  0.5842,\n",
      "        -0.0840, -0.3820, -0.3701, -0.9569, -0.0141,  0.0189,  0.1173, -0.0784,\n",
      "        -0.6091, -0.6875, -0.2474,  0.3803, -0.9531, -0.6898, -0.0805, -0.3557,\n",
      "        -0.2510, -0.7244, -0.8571, -0.0657, -0.2633, -0.5229,  0.2058, -0.4138,\n",
      "         0.0715, -0.4336, -0.0532,  0.2750,  0.3596, -0.2050, -0.2876, -0.7191,\n",
      "        -0.3274, -0.2696, -0.1031,  0.3971, -0.2084, -0.0572,  0.2184, -0.2686,\n",
      "         0.0942, -0.5671, -0.2706,  0.4136, -0.5929, -0.0306, -0.2559, -0.2213,\n",
      "         0.1265,  0.3028,  0.1339, -1.1935, -0.3054, -0.2002, -0.0894, -0.8785,\n",
      "        -0.3559,  0.2644, -0.4436,  0.2008,  0.5347, -0.1327, -0.0837,  0.3455,\n",
      "        -0.1081, -0.0642,  0.5766,  0.2820, -0.1625, -0.7025,  0.4277, -0.2329,\n",
      "        -0.3412])\n",
      "policy_gradient.shape:  torch.Size([121])\n",
      "policy_gradient:  tensor([-0.2847,  0.2517, -0.3135, -0.7481,  0.1959, -0.0249, -0.3143, -0.0624,\n",
      "         1.1055,  0.7986, -1.9291, -1.1052, -1.0729, -1.3104, -0.0666,  0.1158,\n",
      "         0.7722, -0.1640, -0.0707, -0.1986, -0.9260, -0.4503,  0.0862, -0.1554,\n",
      "        -0.0930, -1.1467, -0.1575, -0.0509, -0.6862, -0.1803,  0.8271, -0.0289,\n",
      "        -0.0532, -0.8750, -0.0735, -0.1256, -1.4098, -0.1288, -0.1111,  0.0274,\n",
      "        -0.2728, -0.1897, -0.2436, -0.7446, -0.0306, -0.0864, -0.3622,  1.1438,\n",
      "        -0.0133, -0.3551, -0.1107, -0.1183, -0.0221,  0.0199,  0.1047, -0.0714,\n",
      "        -1.2763, -0.4906, -0.1144,  0.2906, -2.0471, -0.1236, -0.0821, -0.8215,\n",
      "        -0.0556, -3.0627, -0.9473, -0.0080, -0.5974, -0.1651,  0.4200, -0.5904,\n",
      "         0.0871, -0.1133, -0.0303,  0.2246,  0.1057, -0.2714, -0.0343, -0.6473,\n",
      "        -0.3206, -0.0880, -0.0467,  0.2544, -0.2193, -0.0290,  0.1595, -0.7430,\n",
      "         0.0164, -2.1955, -0.1299,  1.7669, -2.3224, -0.0307, -0.0387, -0.0456,\n",
      "         0.1863,  0.3292,  0.0173, -0.3722, -0.0446, -0.2199, -0.0380, -0.4136,\n",
      "        -0.1373,  0.1011, -0.3706,  0.5263,  0.3425, -0.0740, -0.0933,  0.2087,\n",
      "        -0.1246, -0.0207,  0.0841,  0.2822, -0.0315, -0.3490,  0.1000, -0.0753,\n",
      "        -0.2281], grad_fn=<MulBackward0>)\n",
      "policy_grad:  tensor(-26.2674, grad_fn=<SumBackward0>)\n",
      "Negative entropy:  tensor(-105.6020, grad_fn=<SumBackward0>)\n",
      "Actor loss:  tensor(-27.3234, grad_fn=<AddBackward0>)\n",
      "distribution:  tensor([[0.0379, 0.0897, 0.1522, 0.7202]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0021, 0.8893, 0.0331, 0.0756]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0347, 0.5227, 0.1569, 0.2857]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0126, 0.1518, 0.0481, 0.7876]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0018, 0.9070, 0.0418, 0.0493]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0171, 0.7714, 0.0899, 0.1217]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0128, 0.3942, 0.0840, 0.5090]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0174, 0.9044, 0.0302, 0.0480]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0207, 0.6604, 0.0595, 0.2593]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0245, 0.7898, 0.0795, 0.1062]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0175, 0.6211, 0.0691, 0.2923]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0180, 0.6248, 0.1186, 0.2387]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0092, 0.7869, 0.0760, 0.1279]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0161, 0.6496, 0.1511, 0.1832]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0033, 0.8786, 0.0555, 0.0626]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0118, 0.6051, 0.1479, 0.2352]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0089, 0.8879, 0.0376, 0.0657]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0080, 0.8742, 0.0265, 0.0913]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0139, 0.6215, 0.0689, 0.2956]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0132, 0.8357, 0.0460, 0.1050]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0171, 0.6644, 0.1505, 0.1680]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0121, 0.7067, 0.0770, 0.2042]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0076, 0.8949, 0.0268, 0.0707]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0316, 0.2323, 0.0568, 0.6792]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0222, 0.6926, 0.0406, 0.2446]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0108, 0.5910, 0.0439, 0.3543]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0023, 0.8173, 0.0623, 0.1181]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0264, 0.4898, 0.1134, 0.3705]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0096, 0.7548, 0.0528, 0.1827]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0054, 0.3817, 0.0277, 0.5852]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0368, 0.2861, 0.0639, 0.6132]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0205, 0.5733, 0.0569, 0.3494]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0061, 0.8001, 0.0949, 0.0989]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0075, 0.7514, 0.0856, 0.1555]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0062, 0.8633, 0.0374, 0.0932]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0118, 0.4335, 0.0895, 0.4652]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0067, 0.5276, 0.0862, 0.3795]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0238, 0.8554, 0.0674, 0.0534]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0379, 0.4731, 0.0366, 0.4525]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0159, 0.3246, 0.0935, 0.5660]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0105, 0.7333, 0.1030, 0.1533]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0223, 0.5097, 0.0988, 0.3692]], grad_fn=<ExpBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distribution:  tensor([[0.0179, 0.2634, 0.0503, 0.6684]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0252, 0.7109, 0.1388, 0.1251]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0150, 0.3340, 0.0304, 0.6206]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0341, 0.7483, 0.0987, 0.1189]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0055, 0.9153, 0.0366, 0.0426]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0052, 0.9389, 0.0362, 0.0198]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0063, 0.5997, 0.0411, 0.3528]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0310, 0.2321, 0.0836, 0.6533]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0238, 0.2919, 0.1821, 0.5022]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0075, 0.5694, 0.0735, 0.3496]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0075, 0.4982, 0.0590, 0.4353]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0806, 0.1271, 0.0713, 0.7210]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0081, 0.1651, 0.1501, 0.6766]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0131, 0.3753, 0.1095, 0.5020]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0227, 0.5810, 0.0960, 0.3003]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0127, 0.5281, 0.0231, 0.4362]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0328, 0.5889, 0.1470, 0.2313]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0136, 0.7345, 0.0729, 0.1790]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0168, 0.5414, 0.0697, 0.3721]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0066, 0.8638, 0.0372, 0.0924]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0163, 0.8320, 0.0470, 0.1047]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0331, 0.4779, 0.1191, 0.3699]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.1017, 0.6388, 0.0931, 0.1664]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0313, 0.7195, 0.0617, 0.1875]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0165, 0.7366, 0.1484, 0.0985]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0284, 0.6213, 0.1580, 0.1924]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0092, 0.4731, 0.0468, 0.4708]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0754, 0.2448, 0.0868, 0.5930]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0400, 0.4496, 0.0365, 0.4740]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0090, 0.8122, 0.0766, 0.1023]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0568, 0.2497, 0.1244, 0.5691]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0093, 0.8292, 0.0408, 0.1207]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0313, 0.4610, 0.1303, 0.3774]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0262, 0.6191, 0.1201, 0.2347]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0171, 0.4186, 0.1625, 0.4017]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0195, 0.7331, 0.0451, 0.2023]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0091, 0.2227, 0.0852, 0.6830]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0070, 0.9145, 0.0435, 0.0350]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0128, 0.6620, 0.0447, 0.2806]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0120, 0.2093, 0.0345, 0.7442]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0187, 0.8636, 0.0538, 0.0639]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0070, 0.4423, 0.0217, 0.5290]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0130, 0.8842, 0.0548, 0.0481]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0035, 0.8751, 0.0620, 0.0594]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0230, 0.7619, 0.0482, 0.1669]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0120, 0.4341, 0.0647, 0.4892]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0175, 0.6529, 0.0847, 0.2450]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0117, 0.8123, 0.0726, 0.1033]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0429, 0.4147, 0.2396, 0.3029]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0087, 0.3999, 0.1377, 0.4538]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0181, 0.1075, 0.0849, 0.7896]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0027, 0.9620, 0.0156, 0.0197]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0086, 0.6534, 0.1427, 0.1953]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0125, 0.4772, 0.1687, 0.3416]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0045, 0.3860, 0.0817, 0.5279]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0097, 0.8367, 0.0913, 0.0623]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0236, 0.1991, 0.0380, 0.7393]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0128, 0.8139, 0.0831, 0.0901]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0120, 0.8438, 0.0732, 0.0711]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0108, 0.8177, 0.0659, 0.1055]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0045, 0.8153, 0.0154, 0.1647]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0202, 0.6187, 0.1476, 0.2135]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0139, 0.2568, 0.0236, 0.7058]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0238, 0.7766, 0.1068, 0.0928]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0148, 0.7996, 0.0414, 0.1443]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0413, 0.1890, 0.1910, 0.5787]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0069, 0.8784, 0.0487, 0.0661]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0491, 0.3417, 0.2013, 0.4078]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0051, 0.4872, 0.0671, 0.4407]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0180, 0.6965, 0.1118, 0.1737]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0208, 0.3313, 0.0588, 0.5892]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0072, 0.7440, 0.1356, 0.1132]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0180, 0.6983, 0.0598, 0.2240]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0433, 0.4862, 0.0686, 0.4019]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0100, 0.8032, 0.0718, 0.1149]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0059, 0.7073, 0.0885, 0.1983]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0529, 0.5276, 0.1912, 0.2283]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0034, 0.9539, 0.0198, 0.0229]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0075, 0.4370, 0.0371, 0.5185]], grad_fn=<ExpBackward>)\n",
      "n_step_rewards.shape:  (121,)\n",
      "rewards.shape:  (121,)\n",
      "n_step_rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0.]\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0.]\n",
      "bootstrap:  [False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      "  True]\n",
      "done.shape: (before n_steps) (121,)\n",
      "done: (before n_steps) [False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False]\n",
      "done.shape: (after n_steps) (121,)\n",
      "Gamma_V.shape:  (121,)\n",
      "done: (after n_steps) [False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False]\n",
      "Gamma_V:  [0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.67572905 0.6825546\n",
      " 0.68944909 0.69641322 0.70344769 0.71055323 0.71773053 0.72498034\n",
      " 0.73230337 0.73970037 0.74717209 0.75471929 0.76234271 0.77004315\n",
      " 0.77782136 0.78567814 0.79361428 0.80163059 0.80972787 0.81790694\n",
      " 0.82616862 0.83451376 0.84294319 0.85145777 0.86005835 0.86874581\n",
      " 0.87752102 0.88638487 0.89533825 0.90438208 0.91351725 0.92274469\n",
      " 0.93206535 0.94148015 0.95099005 0.96059601 0.970299   0.9801\n",
      " 0.99      ]\n",
      "old_states.shape:  torch.Size([121, 1, 9, 9])\n",
      "new_states.shape:  torch.Size([121, 1, 9, 9])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_probs:  [tensor(-0.3282, grad_fn=<SelectBackward>), tensor(-0.1174, grad_fn=<SelectBackward>), tensor(-1.2529, grad_fn=<SelectBackward>), tensor(-0.2388, grad_fn=<SelectBackward>), tensor(-0.0976, grad_fn=<SelectBackward>), tensor(-2.4095, grad_fn=<SelectBackward>), tensor(-0.6753, grad_fn=<SelectBackward>), tensor(-0.1004, grad_fn=<SelectBackward>), tensor(-1.3499, grad_fn=<SelectBackward>), tensor(-0.2360, grad_fn=<SelectBackward>), tensor(-0.4763, grad_fn=<SelectBackward>), tensor(-0.4703, grad_fn=<SelectBackward>), tensor(-0.2397, grad_fn=<SelectBackward>), tensor(-0.4314, grad_fn=<SelectBackward>), tensor(-0.1294, grad_fn=<SelectBackward>), tensor(-1.9115, grad_fn=<SelectBackward>), tensor(-0.1189, grad_fn=<SelectBackward>), tensor(-0.1345, grad_fn=<SelectBackward>), tensor(-1.2186, grad_fn=<SelectBackward>), tensor(-0.1795, grad_fn=<SelectBackward>), tensor(-0.4088, grad_fn=<SelectBackward>), tensor(-0.3472, grad_fn=<SelectBackward>), tensor(-0.1111, grad_fn=<SelectBackward>), tensor(-1.4596, grad_fn=<SelectBackward>), tensor(-0.3673, grad_fn=<SelectBackward>), tensor(-1.0376, grad_fn=<SelectBackward>), tensor(-2.1365, grad_fn=<SelectBackward>), tensor(-0.7138, grad_fn=<SelectBackward>), tensor(-0.2813, grad_fn=<SelectBackward>), tensor(-0.5358, grad_fn=<SelectBackward>), tensor(-0.4891, grad_fn=<SelectBackward>), tensor(-0.5564, grad_fn=<SelectBackward>), tensor(-2.3550, grad_fn=<SelectBackward>), tensor(-2.4583, grad_fn=<SelectBackward>), tensor(-0.1470, grad_fn=<SelectBackward>), tensor(-0.7653, grad_fn=<SelectBackward>), tensor(-0.6395, grad_fn=<SelectBackward>), tensor(-2.9303, grad_fn=<SelectBackward>), tensor(-0.7485, grad_fn=<SelectBackward>), tensor(-2.3701, grad_fn=<SelectBackward>), tensor(-0.3102, grad_fn=<SelectBackward>), tensor(-0.9965, grad_fn=<SelectBackward>), tensor(-1.3341, grad_fn=<SelectBackward>), tensor(-0.3412, grad_fn=<SelectBackward>), tensor(-0.4770, grad_fn=<SelectBackward>), tensor(-0.2900, grad_fn=<SelectBackward>), tensor(-0.0885, grad_fn=<SelectBackward>), tensor(-0.0631, grad_fn=<SelectBackward>), tensor(-0.5113, grad_fn=<SelectBackward>), tensor(-0.4258, grad_fn=<SelectBackward>), tensor(-1.2313, grad_fn=<SelectBackward>), tensor(-2.6109, grad_fn=<SelectBackward>), tensor(-0.8317, grad_fn=<SelectBackward>), tensor(-2.5180, grad_fn=<SelectBackward>), tensor(-1.8963, grad_fn=<SelectBackward>), tensor(-0.9799, grad_fn=<SelectBackward>), tensor(-0.5429, grad_fn=<SelectBackward>), tensor(-0.6384, grad_fn=<SelectBackward>), tensor(-0.5295, grad_fn=<SelectBackward>), tensor(-0.3085, grad_fn=<SelectBackward>), tensor(-0.9887, grad_fn=<SelectBackward>), tensor(-0.1465, grad_fn=<SelectBackward>), tensor(-2.2567, grad_fn=<SelectBackward>), tensor(-0.9946, grad_fn=<SelectBackward>), tensor(-1.7931, grad_fn=<SelectBackward>), tensor(-1.6739, grad_fn=<SelectBackward>), tensor(-1.9079, grad_fn=<SelectBackward>), tensor(-1.8455, grad_fn=<SelectBackward>), tensor(-0.7484, grad_fn=<SelectBackward>), tensor(-1.4072, grad_fn=<SelectBackward>), tensor(-3.3117, grad_fn=<SelectBackward>), tensor(-0.2081, grad_fn=<SelectBackward>), tensor(-2.0844, grad_fn=<SelectBackward>), tensor(-0.1873, grad_fn=<SelectBackward>), tensor(-2.0378, grad_fn=<SelectBackward>), tensor(-0.4795, grad_fn=<SelectBackward>), tensor(-0.9120, grad_fn=<SelectBackward>), tensor(-0.3105, grad_fn=<SelectBackward>), tensor(-0.3812, grad_fn=<SelectBackward>), tensor(-3.3528, grad_fn=<SelectBackward>), tensor(-0.4126, grad_fn=<SelectBackward>), tensor(-0.2955, grad_fn=<SelectBackward>), tensor(-0.1466, grad_fn=<SelectBackward>), tensor(-0.8157, grad_fn=<SelectBackward>), tensor(-2.9038, grad_fn=<SelectBackward>), tensor(-0.1334, grad_fn=<SelectBackward>), tensor(-0.2719, grad_fn=<SelectBackward>), tensor(-0.7151, grad_fn=<SelectBackward>), tensor(-0.4264, grad_fn=<SelectBackward>), tensor(-0.2078, grad_fn=<SelectBackward>), tensor(-0.8803, grad_fn=<SelectBackward>), tensor(-0.9165, grad_fn=<SelectBackward>), tensor(-4.0139, grad_fn=<SelectBackward>), tensor(-0.0387, grad_fn=<SelectBackward>), tensor(-1.6333, grad_fn=<SelectBackward>), tensor(-1.0740, grad_fn=<SelectBackward>), tensor(-0.6389, grad_fn=<SelectBackward>), tensor(-0.1782, grad_fn=<SelectBackward>), tensor(-0.3021, grad_fn=<SelectBackward>), tensor(-0.2059, grad_fn=<SelectBackward>), tensor(-0.1699, grad_fn=<SelectBackward>), tensor(-0.2012, grad_fn=<SelectBackward>), tensor(-0.2042, grad_fn=<SelectBackward>), tensor(-1.5442, grad_fn=<SelectBackward>), tensor(-0.3485, grad_fn=<SelectBackward>), tensor(-0.2528, grad_fn=<SelectBackward>), tensor(-1.9362, grad_fn=<SelectBackward>), tensor(-3.1867, grad_fn=<SelectBackward>), tensor(-0.1297, grad_fn=<SelectBackward>), tensor(-1.0738, grad_fn=<SelectBackward>), tensor(-0.7190, grad_fn=<SelectBackward>), tensor(-0.3616, grad_fn=<SelectBackward>), tensor(-2.8343, grad_fn=<SelectBackward>), tensor(-0.2957, grad_fn=<SelectBackward>), tensor(-0.3592, grad_fn=<SelectBackward>), tensor(-0.9114, grad_fn=<SelectBackward>), tensor(-0.2191, grad_fn=<SelectBackward>), tensor(-0.3464, grad_fn=<SelectBackward>), tensor(-0.6395, grad_fn=<SelectBackward>), tensor(-0.0472, grad_fn=<SelectBackward>), tensor(-0.6567, grad_fn=<SelectBackward>)]\n",
      "log_probs:  tensor([-0.3282, -0.1174, -1.2529, -0.2388, -0.0976, -2.4095, -0.6753, -0.1004,\n",
      "        -1.3499, -0.2360, -0.4763, -0.4703, -0.2397, -0.4314, -0.1294, -1.9115,\n",
      "        -0.1189, -0.1345, -1.2186, -0.1795, -0.4088, -0.3472, -0.1111, -1.4596,\n",
      "        -0.3673, -1.0376, -2.1365, -0.7138, -0.2813, -0.5358, -0.4891, -0.5564,\n",
      "        -2.3550, -2.4583, -0.1470, -0.7653, -0.6395, -2.9303, -0.7485, -2.3701,\n",
      "        -0.3102, -0.9965, -1.3341, -0.3412, -0.4770, -0.2900, -0.0885, -0.0631,\n",
      "        -0.5113, -0.4258, -1.2313, -2.6109, -0.8317, -2.5180, -1.8963, -0.9799,\n",
      "        -0.5429, -0.6384, -0.5295, -0.3085, -0.9887, -0.1465, -2.2567, -0.9946,\n",
      "        -1.7931, -1.6739, -1.9079, -1.8455, -0.7484, -1.4072, -3.3117, -0.2081,\n",
      "        -2.0844, -0.1873, -2.0378, -0.4795, -0.9120, -0.3105, -0.3812, -3.3528,\n",
      "        -0.4126, -0.2955, -0.1466, -0.8157, -2.9038, -0.1334, -0.2719, -0.7151,\n",
      "        -0.4264, -0.2078, -0.8803, -0.9165, -4.0139, -0.0387, -1.6333, -1.0740,\n",
      "        -0.6389, -0.1782, -0.3021, -0.2059, -0.1699, -0.2012, -0.2042, -1.5442,\n",
      "        -0.3485, -0.2528, -1.9362, -3.1867, -0.1297, -1.0738, -0.7190, -0.3616,\n",
      "        -2.8343, -0.2957, -0.3592, -0.9114, -0.2191, -0.3464, -0.6395, -0.0472,\n",
      "        -0.6567], grad_fn=<StackBackward>)\n",
      "distributions.shape:  torch.Size([1, 121, 4])\n",
      "distributions:  tensor([[[0.0379, 0.0897, 0.1522, 0.7202],\n",
      "         [0.0021, 0.8893, 0.0331, 0.0756],\n",
      "         [0.0347, 0.5227, 0.1569, 0.2857],\n",
      "         [0.0126, 0.1518, 0.0481, 0.7876],\n",
      "         [0.0018, 0.9070, 0.0418, 0.0493],\n",
      "         [0.0171, 0.7714, 0.0899, 0.1217],\n",
      "         [0.0128, 0.3942, 0.0840, 0.5090],\n",
      "         [0.0174, 0.9044, 0.0302, 0.0480],\n",
      "         [0.0207, 0.6604, 0.0595, 0.2593],\n",
      "         [0.0245, 0.7898, 0.0795, 0.1062],\n",
      "         [0.0175, 0.6211, 0.0691, 0.2923],\n",
      "         [0.0180, 0.6248, 0.1186, 0.2387],\n",
      "         [0.0092, 0.7869, 0.0760, 0.1279],\n",
      "         [0.0161, 0.6496, 0.1511, 0.1832],\n",
      "         [0.0033, 0.8786, 0.0555, 0.0626],\n",
      "         [0.0118, 0.6051, 0.1479, 0.2352],\n",
      "         [0.0089, 0.8879, 0.0376, 0.0657],\n",
      "         [0.0080, 0.8742, 0.0265, 0.0913],\n",
      "         [0.0139, 0.6215, 0.0689, 0.2956],\n",
      "         [0.0132, 0.8357, 0.0460, 0.1050],\n",
      "         [0.0171, 0.6644, 0.1505, 0.1680],\n",
      "         [0.0121, 0.7067, 0.0770, 0.2042],\n",
      "         [0.0076, 0.8949, 0.0268, 0.0707],\n",
      "         [0.0316, 0.2323, 0.0568, 0.6792],\n",
      "         [0.0222, 0.6926, 0.0406, 0.2446],\n",
      "         [0.0108, 0.5910, 0.0439, 0.3543],\n",
      "         [0.0023, 0.8173, 0.0623, 0.1181],\n",
      "         [0.0264, 0.4898, 0.1134, 0.3705],\n",
      "         [0.0096, 0.7548, 0.0528, 0.1827],\n",
      "         [0.0054, 0.3817, 0.0277, 0.5852],\n",
      "         [0.0368, 0.2861, 0.0639, 0.6132],\n",
      "         [0.0205, 0.5733, 0.0569, 0.3494],\n",
      "         [0.0061, 0.8001, 0.0949, 0.0989],\n",
      "         [0.0075, 0.7514, 0.0856, 0.1555],\n",
      "         [0.0062, 0.8633, 0.0374, 0.0932],\n",
      "         [0.0118, 0.4335, 0.0895, 0.4652],\n",
      "         [0.0067, 0.5276, 0.0862, 0.3795],\n",
      "         [0.0238, 0.8554, 0.0674, 0.0534],\n",
      "         [0.0379, 0.4731, 0.0366, 0.4525],\n",
      "         [0.0159, 0.3246, 0.0935, 0.5660],\n",
      "         [0.0105, 0.7333, 0.1030, 0.1533],\n",
      "         [0.0223, 0.5097, 0.0988, 0.3692],\n",
      "         [0.0179, 0.2634, 0.0503, 0.6684],\n",
      "         [0.0252, 0.7109, 0.1388, 0.1251],\n",
      "         [0.0150, 0.3340, 0.0304, 0.6206],\n",
      "         [0.0341, 0.7483, 0.0987, 0.1189],\n",
      "         [0.0055, 0.9153, 0.0366, 0.0426],\n",
      "         [0.0052, 0.9389, 0.0362, 0.0198],\n",
      "         [0.0063, 0.5997, 0.0411, 0.3528],\n",
      "         [0.0310, 0.2321, 0.0836, 0.6533],\n",
      "         [0.0238, 0.2919, 0.1821, 0.5022],\n",
      "         [0.0075, 0.5694, 0.0735, 0.3496],\n",
      "         [0.0075, 0.4982, 0.0590, 0.4353],\n",
      "         [0.0806, 0.1271, 0.0713, 0.7210],\n",
      "         [0.0081, 0.1651, 0.1501, 0.6766],\n",
      "         [0.0131, 0.3753, 0.1095, 0.5020],\n",
      "         [0.0227, 0.5810, 0.0960, 0.3003],\n",
      "         [0.0127, 0.5281, 0.0231, 0.4362],\n",
      "         [0.0328, 0.5889, 0.1470, 0.2313],\n",
      "         [0.0136, 0.7345, 0.0729, 0.1790],\n",
      "         [0.0168, 0.5414, 0.0697, 0.3721],\n",
      "         [0.0066, 0.8638, 0.0372, 0.0924],\n",
      "         [0.0163, 0.8320, 0.0470, 0.1047],\n",
      "         [0.0331, 0.4779, 0.1191, 0.3699],\n",
      "         [0.1017, 0.6388, 0.0931, 0.1664],\n",
      "         [0.0313, 0.7195, 0.0617, 0.1875],\n",
      "         [0.0165, 0.7366, 0.1484, 0.0985],\n",
      "         [0.0284, 0.6213, 0.1580, 0.1924],\n",
      "         [0.0092, 0.4731, 0.0468, 0.4708],\n",
      "         [0.0754, 0.2448, 0.0868, 0.5930],\n",
      "         [0.0400, 0.4496, 0.0365, 0.4740],\n",
      "         [0.0090, 0.8122, 0.0766, 0.1023],\n",
      "         [0.0568, 0.2497, 0.1244, 0.5691],\n",
      "         [0.0093, 0.8292, 0.0408, 0.1207],\n",
      "         [0.0313, 0.4610, 0.1303, 0.3774],\n",
      "         [0.0262, 0.6191, 0.1201, 0.2347],\n",
      "         [0.0171, 0.4186, 0.1625, 0.4017],\n",
      "         [0.0195, 0.7331, 0.0451, 0.2023],\n",
      "         [0.0091, 0.2227, 0.0852, 0.6830],\n",
      "         [0.0070, 0.9145, 0.0435, 0.0350],\n",
      "         [0.0128, 0.6620, 0.0447, 0.2806],\n",
      "         [0.0120, 0.2093, 0.0345, 0.7442],\n",
      "         [0.0187, 0.8636, 0.0538, 0.0639],\n",
      "         [0.0070, 0.4423, 0.0217, 0.5290],\n",
      "         [0.0130, 0.8842, 0.0548, 0.0481],\n",
      "         [0.0035, 0.8751, 0.0620, 0.0594],\n",
      "         [0.0230, 0.7619, 0.0482, 0.1669],\n",
      "         [0.0120, 0.4341, 0.0647, 0.4892],\n",
      "         [0.0175, 0.6529, 0.0847, 0.2450],\n",
      "         [0.0117, 0.8123, 0.0726, 0.1033],\n",
      "         [0.0429, 0.4147, 0.2396, 0.3029],\n",
      "         [0.0087, 0.3999, 0.1377, 0.4538],\n",
      "         [0.0181, 0.1075, 0.0849, 0.7896],\n",
      "         [0.0027, 0.9620, 0.0156, 0.0197],\n",
      "         [0.0086, 0.6534, 0.1427, 0.1953],\n",
      "         [0.0125, 0.4772, 0.1687, 0.3416],\n",
      "         [0.0045, 0.3860, 0.0817, 0.5279],\n",
      "         [0.0097, 0.8367, 0.0913, 0.0623],\n",
      "         [0.0236, 0.1991, 0.0380, 0.7393],\n",
      "         [0.0128, 0.8139, 0.0831, 0.0901],\n",
      "         [0.0120, 0.8438, 0.0732, 0.0711],\n",
      "         [0.0108, 0.8177, 0.0659, 0.1055],\n",
      "         [0.0045, 0.8153, 0.0154, 0.1647],\n",
      "         [0.0202, 0.6187, 0.1476, 0.2135],\n",
      "         [0.0139, 0.2568, 0.0236, 0.7058],\n",
      "         [0.0238, 0.7766, 0.1068, 0.0928],\n",
      "         [0.0148, 0.7996, 0.0414, 0.1443],\n",
      "         [0.0413, 0.1890, 0.1910, 0.5787],\n",
      "         [0.0069, 0.8784, 0.0487, 0.0661],\n",
      "         [0.0491, 0.3417, 0.2013, 0.4078],\n",
      "         [0.0051, 0.4872, 0.0671, 0.4407],\n",
      "         [0.0180, 0.6965, 0.1118, 0.1737],\n",
      "         [0.0208, 0.3313, 0.0588, 0.5892],\n",
      "         [0.0072, 0.7440, 0.1356, 0.1132],\n",
      "         [0.0180, 0.6983, 0.0598, 0.2240],\n",
      "         [0.0433, 0.4862, 0.0686, 0.4019],\n",
      "         [0.0100, 0.8032, 0.0718, 0.1149],\n",
      "         [0.0059, 0.7073, 0.0885, 0.1983],\n",
      "         [0.0529, 0.5276, 0.1912, 0.2283],\n",
      "         [0.0034, 0.9539, 0.0198, 0.0229],\n",
      "         [0.0075, 0.4370, 0.0371, 0.5185]]], grad_fn=<StackBackward>)\n",
      "Updating critic...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V_trg.shape (after critic):  torch.Size([121])\n",
      "V_trg.shape (after sum):  torch.Size([121])\n",
      "V_trg.shape (after squeeze):  torch.Size([121])\n",
      "V_trg.shape (after squeeze):  tensor([ 0.6456,  0.9894,  0.8643,  0.3906,  0.3409,  0.3960,  0.2440,  0.7955,\n",
      "         0.1503,  0.4107,  0.7521,  0.7888,  0.3913,  0.5567,  0.6945,  0.0902,\n",
      "         0.3885,  0.8299,  0.2282,  0.1730,  0.2216,  0.2254,  0.7432,  0.4884,\n",
      "         0.6608,  0.1999,  0.6710,  0.5663,  0.3857,  0.1856,  0.2842,  0.3063,\n",
      "         0.3049,  0.2977,  0.2938,  0.3533,  0.5253,  0.2707,  0.7913,  0.3209,\n",
      "         0.3328,  0.3838,  0.1669,  0.7106,  0.2967,  0.5823,  0.6064,  0.6722,\n",
      "         0.2661,  0.6016,  0.8090,  0.6978,  0.6093,  0.1445,  0.2854,  0.3194,\n",
      "         0.3875,  0.5281,  0.7819,  0.5341,  0.6321,  0.7113,  0.1994,  0.5939,\n",
      "         0.1628,  0.5628,  0.5591,  0.2641,  0.4653,  0.7620,  0.5971,  0.3875,\n",
      "         0.3964, -0.0360,  0.9535,  0.6067,  0.8171,  0.2863,  0.7336,  0.7238,\n",
      "         1.0215, -0.0067,  0.6967,  0.5120,  0.3229,  0.5196,  0.1717,  0.6511,\n",
      "         0.8614,  0.6633,  0.4944, -0.1414,  0.4839,  0.7917,  0.2719,  0.7743,\n",
      "         0.3494,  1.0448,  1.2106,  0.4037,  0.5296,  0.7756,  0.3122,  0.6505,\n",
      "         1.2713,  0.6514,  1.0545,  0.0306,  0.2376,  1.1119,  0.7506,  1.4648,\n",
      "         0.7299,  0.5085,  0.5776,  0.2230,  0.2924,  1.1737,  0.9396,  0.3100,\n",
      "         1.0051])\n",
      "V1.shape:  torch.Size([121])\n",
      "V1:  tensor([[0.7520],\n",
      "        [1.2603],\n",
      "        [0.4359],\n",
      "        [0.6416],\n",
      "        [1.2543],\n",
      "        [0.7895],\n",
      "        [1.4351],\n",
      "        [1.3562],\n",
      "        [0.9021],\n",
      "        [0.5465],\n",
      "        [1.0307],\n",
      "        [0.7399],\n",
      "        [1.6051],\n",
      "        [0.9389],\n",
      "        [1.1732],\n",
      "        [0.8852],\n",
      "        [0.5361],\n",
      "        [1.5777],\n",
      "        [0.9319],\n",
      "        [1.8400],\n",
      "        [2.0314],\n",
      "        [1.8183],\n",
      "        [1.8332],\n",
      "        [0.7524],\n",
      "        [0.4026],\n",
      "        [1.1028],\n",
      "        [1.6289],\n",
      "        [1.5450],\n",
      "        [0.9220],\n",
      "        [1.1542],\n",
      "        [1.4798],\n",
      "        [1.0307],\n",
      "        [1.4624],\n",
      "        [2.1311],\n",
      "        [2.0231],\n",
      "        [0.9072],\n",
      "        [1.5640],\n",
      "        [1.7833],\n",
      "        [1.8957],\n",
      "        [1.2926],\n",
      "        [0.3748],\n",
      "        [1.2805],\n",
      "        [1.8417],\n",
      "        [1.2281],\n",
      "        [1.0644],\n",
      "        [1.7143],\n",
      "        [1.6034],\n",
      "        [0.7232],\n",
      "        [2.4430],\n",
      "        [1.1175],\n",
      "        [1.3835],\n",
      "        [1.4579],\n",
      "        [2.0059],\n",
      "        [2.0811],\n",
      "        [1.8449],\n",
      "        [0.2812],\n",
      "        [0.9209],\n",
      "        [2.3675],\n",
      "        [1.4810],\n",
      "        [0.7823],\n",
      "        [1.5750],\n",
      "        [1.3696],\n",
      "        [1.3237],\n",
      "        [1.2384],\n",
      "        [1.1112],\n",
      "        [1.5110],\n",
      "        [1.2728],\n",
      "        [1.2004],\n",
      "        [1.1034],\n",
      "        [1.3162],\n",
      "        [1.9508],\n",
      "        [1.2593],\n",
      "        [0.9143],\n",
      "        [0.9805],\n",
      "        [1.0506],\n",
      "        [1.1624],\n",
      "        [0.4427],\n",
      "        [1.1636],\n",
      "        [1.1526],\n",
      "        [1.8605],\n",
      "        [1.4382],\n",
      "        [0.8710],\n",
      "        [1.5691],\n",
      "        [1.3519],\n",
      "        [0.8961],\n",
      "        [0.2663],\n",
      "        [1.5863],\n",
      "        [1.5633],\n",
      "        [1.6831],\n",
      "        [1.8324],\n",
      "        [0.8184],\n",
      "        [0.8987],\n",
      "        [1.0429],\n",
      "        [1.9445],\n",
      "        [2.0416],\n",
      "        [1.4092],\n",
      "        [0.8080],\n",
      "        [1.8000],\n",
      "        [0.6644],\n",
      "        [0.6037],\n",
      "        [1.6006],\n",
      "        [1.9592],\n",
      "        [0.9548],\n",
      "        [1.1322],\n",
      "        [1.2147],\n",
      "        [1.0502],\n",
      "        [1.1554],\n",
      "        [1.3163],\n",
      "        [1.2434],\n",
      "        [0.8962],\n",
      "        [1.2053],\n",
      "        [1.0989],\n",
      "        [0.9184],\n",
      "        [1.4737],\n",
      "        [0.9572],\n",
      "        [0.4519],\n",
      "        [1.0673],\n",
      "        [0.9756],\n",
      "        [1.4145],\n",
      "        [1.3565],\n",
      "        [0.7433]], grad_fn=<AddmmBackward>)\n",
      "Updating actor...\n",
      "V_trg.shape:  torch.Size([121])\n",
      "V_trg:  tensor([0.2345, 0.4119, 0.2660, 0.3456, 0.4254, 0.6617, 0.2400, 0.3322, 0.0743,\n",
      "        0.3637, 0.4012, 0.4784, 0.4708, 0.1374, 0.3649, 0.6614, 0.5315, 0.5684,\n",
      "        0.1481, 0.2711, 0.8496, 0.3881, 0.3565, 0.3785, 0.1891, 0.4733, 0.4601,\n",
      "        0.5543, 0.7920, 0.4430, 0.4363, 0.4602, 0.5274, 0.6927, 0.5299, 0.5034,\n",
      "        0.5403, 0.7235, 0.3040, 0.4859, 0.3401, 0.3018, 0.6374, 0.5036, 0.4743,\n",
      "        0.7002, 0.2180, 0.4750, 0.5779, 0.3188, 0.5756, 0.1100, 0.7673, 0.2370,\n",
      "        0.5680, 0.6166, 0.4256, 0.1367, 0.5050, 0.5014, 0.3748, 0.6694, 0.5627,\n",
      "        0.4130, 0.4210, 0.2909, 0.6407, 0.4555, 0.9251, 0.3169, 0.3989, 0.3515,\n",
      "        0.6007, 0.1312, 0.5129, 0.7429, 0.2192, 0.6698, 0.5882, 0.2849, 0.3305,\n",
      "        0.5924, 0.2749, 0.5453, 0.2002, 0.2935, 0.6596, 0.5968, 0.4544, 0.5172,\n",
      "        0.4932, 0.5214, 0.0480, 0.6393, 0.6038, 0.0274, 0.1389, 0.1640, 0.1467,\n",
      "        0.0899, 0.4829, 0.3919, 0.5213, 0.3458, 0.7186, 0.3398, 0.9284, 0.5168,\n",
      "        0.7316, 0.6831, 0.5635, 0.6859, 0.4296, 0.9891, 0.3941, 0.8848, 0.5885,\n",
      "        0.8932, 0.6688, 0.8649, 0.2730])\n",
      "V_pred.shape:  torch.Size([121])\n",
      "V_pred:  tensor([ 0.8968,  0.3819,  0.5579,  0.0131, -0.3290,  0.9210,  0.1898,  0.8854,\n",
      "         0.6283,  0.6935,  1.4212,  0.7207,  0.5885,  0.4137, -0.0039,  0.4928,\n",
      "         0.4059,  0.3324,  0.5638,  0.5532,  0.6990,  0.2136,  0.3625,  0.6931,\n",
      "         0.7210,  0.0398,  0.6660,  0.9007,  0.8183,  0.2981,  1.2136,  0.7974,\n",
      "         0.8410,  0.6250,  0.1810,  0.7832,  0.5793,  0.2557,  0.2546,  0.8490,\n",
      "         1.2053,  0.8000,  0.5279,  0.6557,  0.8943,  0.3048,  0.0376,  0.8049,\n",
      "         0.2757,  0.7399,  0.7023,  0.9169,  0.6658,  0.8342,  0.0082,  0.0176,\n",
      "         0.5979,  0.5232,  0.7038,  0.8133,  0.2867,  0.5730, -0.3669,  0.3602,\n",
      "         0.8378,  0.8293,  0.1785,  0.9632,  0.5428,  0.7605,  0.9624,  0.8253,\n",
      "         1.2429,  0.9075,  1.0428,  0.1712,  0.7557,  0.3614, -0.0022,  0.7697,\n",
      "         1.0542,  0.8515,  0.4012,  0.6280,  0.5051,  0.7675,  0.5115,  0.1524,\n",
      "         1.0596,  0.6426,  0.3901,  0.6695,  0.4537, -0.0866,  0.5028,  0.4757,\n",
      "         0.3980,  1.2653,  0.4040,  0.5650,  0.3935,  0.6749,  0.5177,  0.8072,\n",
      "         0.8826,  0.3850,  0.3567,  0.9129,  0.8761,  0.7542,  0.5756,  0.7377,\n",
      "         1.0678,  0.8475,  0.3307, -0.1080,  0.6455,  0.8356,  0.9421,  0.2687,\n",
      "         1.1007])\n",
      "A.shape:  torch.Size([121])\n",
      "A:  tensor([-0.6623,  0.0299, -0.2919,  0.3325,  0.7544, -0.2593,  0.0502, -0.5533,\n",
      "        -0.5540, -0.3297, -1.0200, -0.2423, -0.1177, -0.2764,  0.3687,  0.1686,\n",
      "         0.1255,  0.2360, -0.4156, -0.2821,  0.1506,  0.1745, -0.0060, -0.3145,\n",
      "        -0.5319,  0.4334, -0.2058, -0.3464, -0.0263,  0.1449, -0.7773, -0.3372,\n",
      "        -0.3136,  0.0678,  0.3489, -0.2798, -0.0390,  0.4678,  0.0494, -0.3631,\n",
      "        -0.8652, -0.4982,  0.1094, -0.1521, -0.4200,  0.3954,  0.1804, -0.3299,\n",
      "         0.3022, -0.4212, -0.1267, -0.8069,  0.1014, -0.5972,  0.5598,  0.5990,\n",
      "        -0.1723, -0.3865, -0.1989, -0.3119,  0.0881,  0.0964,  0.9296,  0.0528,\n",
      "        -0.4168, -0.5385,  0.4623, -0.5077,  0.3823, -0.4436, -0.5635, -0.4738,\n",
      "        -0.6422, -0.7763, -0.5299,  0.5717, -0.5365,  0.3084,  0.5904, -0.4848,\n",
      "        -0.7237, -0.2592, -0.1263, -0.0827, -0.3049, -0.4740,  0.1481,  0.4445,\n",
      "        -0.6052, -0.1254,  0.1031, -0.1481, -0.4057,  0.7260,  0.1010, -0.4483,\n",
      "        -0.2591, -1.1013, -0.2572, -0.4751,  0.0894, -0.2831,  0.0037, -0.4613,\n",
      "        -0.1641, -0.0453,  0.5717, -0.3961, -0.1446, -0.0711, -0.0121, -0.0518,\n",
      "        -0.6382,  0.1416,  0.0634,  0.9929, -0.0571,  0.0576, -0.2733,  0.5961,\n",
      "        -0.8277])\n",
      "policy_gradient.shape:  torch.Size([121])\n",
      "policy_gradient:  tensor([-2.1738e-01,  3.5144e-03, -3.6578e-01,  7.9382e-02,  7.3661e-02,\n",
      "        -6.2474e-01,  3.3886e-02, -5.5567e-02, -7.4791e-01, -7.7825e-02,\n",
      "        -4.8580e-01, -1.1395e-01, -2.8218e-02, -1.1923e-01,  4.7725e-02,\n",
      "         3.2230e-01,  1.4927e-02,  3.1741e-02, -5.0651e-01, -5.0626e-02,\n",
      "         6.1562e-02,  6.0585e-02, -6.6727e-04, -4.5911e-01, -1.9537e-01,\n",
      "         4.4971e-01, -4.3978e-01, -2.4725e-01, -7.4029e-03,  7.7630e-02,\n",
      "        -3.8019e-01, -1.8761e-01, -7.3844e-01,  1.6666e-01,  5.1292e-02,\n",
      "        -2.1416e-01, -2.4918e-02,  1.3707e+00,  3.6960e-02, -8.6049e-01,\n",
      "        -2.6839e-01, -4.9649e-01,  1.4600e-01, -5.1904e-02, -2.0038e-01,\n",
      "         1.1465e-01,  1.5973e-02, -2.0805e-02,  1.5452e-01, -1.7932e-01,\n",
      "        -1.5596e-01, -2.1067e+00,  8.4362e-02, -1.5038e+00,  1.0616e+00,\n",
      "         5.8694e-01, -9.3573e-02, -2.4675e-01, -1.0530e-01, -9.6214e-02,\n",
      "         8.7110e-02,  1.4121e-02,  2.0978e+00,  5.2508e-02, -7.4735e-01,\n",
      "        -9.0140e-01,  8.8200e-01, -9.3686e-01,  2.8611e-01, -6.2423e-01,\n",
      "        -1.8662e+00, -9.8583e-02, -1.3386e+00, -1.4538e-01, -1.0798e+00,\n",
      "         2.7415e-01, -4.8934e-01,  9.5752e-02,  2.2509e-01, -1.6255e+00,\n",
      "        -2.9857e-01, -7.6582e-02, -1.8518e-02, -6.7419e-02, -8.8530e-01,\n",
      "        -6.3235e-02,  4.0262e-02,  3.1782e-01, -2.5808e-01, -2.6060e-02,\n",
      "         9.0770e-02, -1.3574e-01, -1.6284e+00,  2.8113e-02,  1.6498e-01,\n",
      "        -4.8152e-01, -1.6553e-01, -1.9629e-01, -7.7718e-02, -9.7825e-02,\n",
      "         1.5186e-02, -5.6961e-02,  7.5111e-04, -7.1242e-01, -5.7166e-02,\n",
      "        -1.1441e-02,  1.1069e+00, -1.2623e+00, -1.8745e-02, -7.6366e-02,\n",
      "        -8.6788e-03, -1.8729e-02, -1.8087e+00,  4.1866e-02,  2.2767e-02,\n",
      "         9.0491e-01, -1.2503e-02,  1.9959e-02, -1.7479e-01,  2.8147e-02,\n",
      "        -5.4362e-01], grad_fn=<MulBackward0>)\n",
      "policy_grad:  tensor(-19.9238, grad_fn=<SumBackward0>)\n",
      "Negative entropy:  tensor(-96.7029, grad_fn=<SumBackward0>)\n",
      "Actor loss:  tensor(-20.8908, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distribution:  tensor([[0.0032, 0.8930, 0.0234, 0.0804]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0121, 0.5968, 0.1828, 0.2083]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0286, 0.5465, 0.0900, 0.3349]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0350, 0.3036, 0.0431, 0.6183]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0072, 0.8994, 0.0483, 0.0451]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0185, 0.7572, 0.0767, 0.1476]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0284, 0.4135, 0.2509, 0.3072]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0114, 0.8791, 0.0269, 0.0826]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0049, 0.7099, 0.0128, 0.2723]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0191, 0.7921, 0.0571, 0.1317]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0019, 0.8435, 0.0678, 0.0868]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0245, 0.7006, 0.0322, 0.2427]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0060, 0.7084, 0.1385, 0.1470]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0102, 0.0753, 0.0071, 0.9073]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0265, 0.7628, 0.0650, 0.1456]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0110, 0.1754, 0.0110, 0.8026]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0045, 0.6920, 0.1115, 0.1920]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0247, 0.1714, 0.0561, 0.7479]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0082, 0.7373, 0.0552, 0.1993]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0277, 0.2973, 0.0486, 0.6264]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0226, 0.6750, 0.1449, 0.1575]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0140, 0.5128, 0.1526, 0.3206]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0041, 0.4183, 0.0296, 0.5480]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0044, 0.6209, 0.0117, 0.3630]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0114, 0.3643, 0.0641, 0.5602]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0206, 0.6290, 0.0749, 0.2755]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0327, 0.3290, 0.0836, 0.5547]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0082, 0.8341, 0.0276, 0.1301]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0134, 0.5411, 0.0470, 0.3986]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0044, 0.2472, 0.0784, 0.6699]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0075, 0.1609, 0.0855, 0.7462]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0081, 0.5798, 0.1030, 0.3090]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0043, 0.3565, 0.0189, 0.6203]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0088, 0.8706, 0.0433, 0.0773]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0083, 0.4703, 0.0749, 0.4465]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0138, 0.8497, 0.0443, 0.0922]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0178, 0.6780, 0.1107, 0.1934]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0029, 0.8505, 0.0238, 0.1228]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0060, 0.9207, 0.0440, 0.0292]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0291, 0.3429, 0.1146, 0.5133]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0071, 0.1827, 0.0775, 0.7326]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0448, 0.4935, 0.0536, 0.4080]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0219, 0.2831, 0.1045, 0.5904]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0355, 0.6082, 0.0875, 0.2688]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0287, 0.3570, 0.0421, 0.5722]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0050, 0.9358, 0.0252, 0.0340]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0080, 0.4601, 0.0617, 0.4702]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0224, 0.5731, 0.0536, 0.3509]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0112, 0.2207, 0.0199, 0.7482]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0283, 0.3740, 0.0888, 0.5090]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0136, 0.8860, 0.0441, 0.0563]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0210, 0.8702, 0.0591, 0.0497]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0174, 0.6810, 0.0410, 0.2606]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0048, 0.5172, 0.0764, 0.4015]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0118, 0.6009, 0.1454, 0.2419]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0099, 0.4512, 0.0417, 0.4972]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0224, 0.6384, 0.0403, 0.2989]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0156, 0.7047, 0.0741, 0.2055]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0098, 0.5851, 0.0296, 0.3755]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0090, 0.8351, 0.0306, 0.1254]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0097, 0.5792, 0.0162, 0.3948]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0265, 0.3276, 0.0911, 0.5549]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0104, 0.5026, 0.1800, 0.3070]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0196, 0.0909, 0.0426, 0.8469]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0125, 0.2711, 0.0606, 0.6558]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0105, 0.7613, 0.0812, 0.1471]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0217, 0.7290, 0.1126, 0.1368]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0039, 0.9269, 0.0305, 0.0387]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0050, 0.7721, 0.0972, 0.1256]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0190, 0.7983, 0.0843, 0.0984]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0034, 0.5580, 0.0373, 0.4013]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0419, 0.5709, 0.1474, 0.2398]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0163, 0.1271, 0.0427, 0.8140]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0332, 0.4256, 0.0928, 0.4484]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0098, 0.5857, 0.0288, 0.3757]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0093, 0.8924, 0.0525, 0.0457]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0051, 0.5552, 0.0323, 0.4074]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0306, 0.3663, 0.0474, 0.5558]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0076, 0.1175, 0.0831, 0.7918]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0077, 0.1273, 0.0209, 0.8441]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0077, 0.8449, 0.0586, 0.0887]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0049, 0.7771, 0.0225, 0.1954]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0256, 0.5202, 0.0634, 0.3908]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0078, 0.7407, 0.0555, 0.1960]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0239, 0.5537, 0.0167, 0.4057]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0194, 0.7090, 0.0966, 0.1750]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0080, 0.7615, 0.0742, 0.1563]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0360, 0.7368, 0.0535, 0.1737]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0177, 0.2664, 0.0477, 0.6683]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0422, 0.4701, 0.2318, 0.2559]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0086, 0.3193, 0.0903, 0.5818]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0031, 0.1362, 0.0165, 0.8442]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0217, 0.8858, 0.0412, 0.0513]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0060, 0.4777, 0.0873, 0.4291]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0032, 0.3705, 0.0411, 0.5852]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0035, 0.1709, 0.0360, 0.7896]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0258, 0.5194, 0.0944, 0.3604]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0072, 0.8649, 0.0541, 0.0738]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0067, 0.4186, 0.0453, 0.5293]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0133, 0.3945, 0.1055, 0.4868]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0066, 0.5222, 0.0493, 0.4219]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0161, 0.6883, 0.0752, 0.2203]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0051, 0.8886, 0.0526, 0.0537]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0271, 0.1303, 0.0185, 0.8241]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0132, 0.5918, 0.0446, 0.3504]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0165, 0.7062, 0.0552, 0.2221]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0347, 0.5898, 0.0498, 0.3256]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0245, 0.0251, 0.0326, 0.9178]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0289, 0.8277, 0.0272, 0.1163]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0026, 0.7269, 0.0324, 0.2381]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0091, 0.8815, 0.0337, 0.0758]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0086, 0.7018, 0.0330, 0.2566]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0117, 0.6722, 0.0580, 0.2581]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0216, 0.6568, 0.0968, 0.2248]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0256, 0.5912, 0.0232, 0.3601]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0108, 0.2254, 0.0411, 0.7227]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0263, 0.7152, 0.0566, 0.2018]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0376, 0.7307, 0.0956, 0.1361]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.1505, 0.4471, 0.0406, 0.3618]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0016, 0.9256, 0.0424, 0.0305]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0099, 0.3570, 0.0209, 0.6122]], grad_fn=<ExpBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_step_rewards.shape:  (121,)\n",
      "rewards.shape:  (121,)\n",
      "n_step_rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0.]\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0.]\n",
      "bootstrap:  [False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      "  True]\n",
      "done.shape: (before n_steps) (121,)\n",
      "done: (before n_steps) [False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False]\n",
      "done.shape: (after n_steps) (121,)\n",
      "Gamma_V.shape:  (121,)\n",
      "done: (after n_steps) [False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False]\n",
      "Gamma_V:  [0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.67572905 0.6825546\n",
      " 0.68944909 0.69641322 0.70344769 0.71055323 0.71773053 0.72498034\n",
      " 0.73230337 0.73970037 0.74717209 0.75471929 0.76234271 0.77004315\n",
      " 0.77782136 0.78567814 0.79361428 0.80163059 0.80972787 0.81790694\n",
      " 0.82616862 0.83451376 0.84294319 0.85145777 0.86005835 0.86874581\n",
      " 0.87752102 0.88638487 0.89533825 0.90438208 0.91351725 0.92274469\n",
      " 0.93206535 0.94148015 0.95099005 0.96059601 0.970299   0.9801\n",
      " 0.99      ]\n",
      "old_states.shape:  torch.Size([121, 1, 9, 9])\n",
      "new_states.shape:  torch.Size([121, 1, 9, 9])\n",
      "log_probs:  [tensor(-0.1132, grad_fn=<SelectBackward>), tensor(-1.6991, grad_fn=<SelectBackward>), tensor(-0.6043, grad_fn=<SelectBackward>), tensor(-0.4807, grad_fn=<SelectBackward>), tensor(-0.1060, grad_fn=<SelectBackward>), tensor(-0.2781, grad_fn=<SelectBackward>), tensor(-1.3828, grad_fn=<SelectBackward>), tensor(-0.1288, grad_fn=<SelectBackward>), tensor(-0.3426, grad_fn=<SelectBackward>), tensor(-0.2331, grad_fn=<SelectBackward>), tensor(-0.1702, grad_fn=<SelectBackward>), tensor(-0.3558, grad_fn=<SelectBackward>), tensor(-0.3447, grad_fn=<SelectBackward>), tensor(-2.5860, grad_fn=<SelectBackward>), tensor(-0.2707, grad_fn=<SelectBackward>), tensor(-1.7405, grad_fn=<SelectBackward>), tensor(-0.3682, grad_fn=<SelectBackward>), tensor(-0.2905, grad_fn=<SelectBackward>), tensor(-1.6129, grad_fn=<SelectBackward>), tensor(-0.4677, grad_fn=<SelectBackward>), tensor(-1.8485, grad_fn=<SelectBackward>), tensor(-0.6680, grad_fn=<SelectBackward>), tensor(-0.6016, grad_fn=<SelectBackward>), tensor(-1.0135, grad_fn=<SelectBackward>), tensor(-1.0099, grad_fn=<SelectBackward>), tensor(-0.4637, grad_fn=<SelectBackward>), tensor(-1.1116, grad_fn=<SelectBackward>), tensor(-0.1814, grad_fn=<SelectBackward>), tensor(-0.9198, grad_fn=<SelectBackward>), tensor(-1.3976, grad_fn=<SelectBackward>), tensor(-0.2928, grad_fn=<SelectBackward>), tensor(-0.5451, grad_fn=<SelectBackward>), tensor(-0.4775, grad_fn=<SelectBackward>), tensor(-0.1385, grad_fn=<SelectBackward>), tensor(-0.7543, grad_fn=<SelectBackward>), tensor(-0.1628, grad_fn=<SelectBackward>), tensor(-0.3886, grad_fn=<SelectBackward>), tensor(-3.7367, grad_fn=<SelectBackward>), tensor(-0.0826, grad_fn=<SelectBackward>), tensor(-0.6668, grad_fn=<SelectBackward>), tensor(-0.3111, grad_fn=<SelectBackward>), tensor(-0.7061, grad_fn=<SelectBackward>), tensor(-0.5269, grad_fn=<SelectBackward>), tensor(-0.4973, grad_fn=<SelectBackward>), tensor(-0.5582, grad_fn=<SelectBackward>), tensor(-0.0664, grad_fn=<SelectBackward>), tensor(-0.7764, grad_fn=<SelectBackward>), tensor(-1.0472, grad_fn=<SelectBackward>), tensor(-1.5108, grad_fn=<SelectBackward>), tensor(-3.5651, grad_fn=<SelectBackward>), tensor(-0.1211, grad_fn=<SelectBackward>), tensor(-0.1391, grad_fn=<SelectBackward>), tensor(-0.3841, grad_fn=<SelectBackward>), tensor(-0.9125, grad_fn=<SelectBackward>), tensor(-1.4191, grad_fn=<SelectBackward>), tensor(-0.6988, grad_fn=<SelectBackward>), tensor(-0.4487, grad_fn=<SelectBackward>), tensor(-0.3499, grad_fn=<SelectBackward>), tensor(-0.5360, grad_fn=<SelectBackward>), tensor(-2.0766, grad_fn=<SelectBackward>), tensor(-0.9293, grad_fn=<SelectBackward>), tensor(-0.5891, grad_fn=<SelectBackward>), tensor(-0.6879, grad_fn=<SelectBackward>), tensor(-0.1662, grad_fn=<SelectBackward>), tensor(-0.4220, grad_fn=<SelectBackward>), tensor(-0.2728, grad_fn=<SelectBackward>), tensor(-0.3161, grad_fn=<SelectBackward>), tensor(-0.0759, grad_fn=<SelectBackward>), tensor(-0.2587, grad_fn=<SelectBackward>), tensor(-0.2253, grad_fn=<SelectBackward>), tensor(-0.5834, grad_fn=<SelectBackward>), tensor(-1.9148, grad_fn=<SelectBackward>), tensor(-0.2059, grad_fn=<SelectBackward>), tensor(-0.8542, grad_fn=<SelectBackward>), tensor(-0.5349, grad_fn=<SelectBackward>), tensor(-0.1138, grad_fn=<SelectBackward>), tensor(-0.5884, grad_fn=<SelectBackward>), tensor(-0.5874, grad_fn=<SelectBackward>), tensor(-0.2335, grad_fn=<SelectBackward>), tensor(-0.1695, grad_fn=<SelectBackward>), tensor(-0.1685, grad_fn=<SelectBackward>), tensor(-1.6326, grad_fn=<SelectBackward>), tensor(-0.9395, grad_fn=<SelectBackward>), tensor(-0.3002, grad_fn=<SelectBackward>), tensor(-0.5912, grad_fn=<SelectBackward>), tensor(-1.7432, grad_fn=<SelectBackward>), tensor(-0.2724, grad_fn=<SelectBackward>), tensor(-0.3054, grad_fn=<SelectBackward>), tensor(-0.4030, grad_fn=<SelectBackward>), tensor(-1.4621, grad_fn=<SelectBackward>), tensor(-0.5416, grad_fn=<SelectBackward>), tensor(-0.1694, grad_fn=<SelectBackward>), tensor(-0.1213, grad_fn=<SelectBackward>), tensor(-0.7388, grad_fn=<SelectBackward>), tensor(-0.5357, grad_fn=<SelectBackward>), tensor(-3.3239, grad_fn=<SelectBackward>), tensor(-1.0205, grad_fn=<SelectBackward>), tensor(-0.1451, grad_fn=<SelectBackward>), tensor(-0.6361, grad_fn=<SelectBackward>), tensor(-0.7200, grad_fn=<SelectBackward>), tensor(-0.6496, grad_fn=<SelectBackward>), tensor(-0.3735, grad_fn=<SelectBackward>), tensor(-0.1181, grad_fn=<SelectBackward>), tensor(-0.1935, grad_fn=<SelectBackward>), tensor(-0.5246, grad_fn=<SelectBackward>), tensor(-2.8971, grad_fn=<SelectBackward>), tensor(-0.5280, grad_fn=<SelectBackward>), tensor(-0.0857, grad_fn=<SelectBackward>), tensor(-0.1891, grad_fn=<SelectBackward>), tensor(-0.3190, grad_fn=<SelectBackward>), tensor(-0.1262, grad_fn=<SelectBackward>), tensor(-1.3601, grad_fn=<SelectBackward>), tensor(-1.3545, grad_fn=<SelectBackward>), tensor(-0.4203, grad_fn=<SelectBackward>), tensor(-1.0214, grad_fn=<SelectBackward>), tensor(-0.3247, grad_fn=<SelectBackward>), tensor(-1.6003, grad_fn=<SelectBackward>), tensor(-0.3138, grad_fn=<SelectBackward>), tensor(-0.8050, grad_fn=<SelectBackward>), tensor(-0.0773, grad_fn=<SelectBackward>), tensor(-0.4906, grad_fn=<SelectBackward>)]\n",
      "log_probs:  tensor([-0.1132, -1.6991, -0.6043, -0.4807, -0.1060, -0.2781, -1.3828, -0.1288,\n",
      "        -0.3426, -0.2331, -0.1702, -0.3558, -0.3447, -2.5860, -0.2707, -1.7405,\n",
      "        -0.3682, -0.2905, -1.6129, -0.4677, -1.8485, -0.6680, -0.6016, -1.0135,\n",
      "        -1.0099, -0.4637, -1.1116, -0.1814, -0.9198, -1.3976, -0.2928, -0.5451,\n",
      "        -0.4775, -0.1385, -0.7543, -0.1628, -0.3886, -3.7367, -0.0826, -0.6668,\n",
      "        -0.3111, -0.7061, -0.5269, -0.4973, -0.5582, -0.0664, -0.7764, -1.0472,\n",
      "        -1.5108, -3.5651, -0.1211, -0.1391, -0.3841, -0.9125, -1.4191, -0.6988,\n",
      "        -0.4487, -0.3499, -0.5360, -2.0766, -0.9293, -0.5891, -0.6879, -0.1662,\n",
      "        -0.4220, -0.2728, -0.3161, -0.0759, -0.2587, -0.2253, -0.5834, -1.9148,\n",
      "        -0.2059, -0.8542, -0.5349, -0.1138, -0.5884, -0.5874, -0.2335, -0.1695,\n",
      "        -0.1685, -1.6326, -0.9395, -0.3002, -0.5912, -1.7432, -0.2724, -0.3054,\n",
      "        -0.4030, -1.4621, -0.5416, -0.1694, -0.1213, -0.7388, -0.5357, -3.3239,\n",
      "        -1.0205, -0.1451, -0.6361, -0.7200, -0.6496, -0.3735, -0.1181, -0.1935,\n",
      "        -0.5246, -2.8971, -0.5280, -0.0857, -0.1891, -0.3190, -0.1262, -1.3601,\n",
      "        -1.3545, -0.4203, -1.0214, -0.3247, -1.6003, -0.3138, -0.8050, -0.0773,\n",
      "        -0.4906], grad_fn=<StackBackward>)\n",
      "distributions.shape:  torch.Size([1, 121, 4])\n",
      "distributions:  tensor([[[0.0032, 0.8930, 0.0234, 0.0804],\n",
      "         [0.0121, 0.5968, 0.1828, 0.2083],\n",
      "         [0.0286, 0.5465, 0.0900, 0.3349],\n",
      "         [0.0350, 0.3036, 0.0431, 0.6183],\n",
      "         [0.0072, 0.8994, 0.0483, 0.0451],\n",
      "         [0.0185, 0.7572, 0.0767, 0.1476],\n",
      "         [0.0284, 0.4135, 0.2509, 0.3072],\n",
      "         [0.0114, 0.8791, 0.0269, 0.0826],\n",
      "         [0.0049, 0.7099, 0.0128, 0.2723],\n",
      "         [0.0191, 0.7921, 0.0571, 0.1317],\n",
      "         [0.0019, 0.8435, 0.0678, 0.0868],\n",
      "         [0.0245, 0.7006, 0.0322, 0.2427],\n",
      "         [0.0060, 0.7084, 0.1385, 0.1470],\n",
      "         [0.0102, 0.0753, 0.0071, 0.9073],\n",
      "         [0.0265, 0.7628, 0.0650, 0.1456],\n",
      "         [0.0110, 0.1754, 0.0110, 0.8026],\n",
      "         [0.0045, 0.6920, 0.1115, 0.1920],\n",
      "         [0.0247, 0.1714, 0.0561, 0.7479],\n",
      "         [0.0082, 0.7373, 0.0552, 0.1993],\n",
      "         [0.0277, 0.2973, 0.0486, 0.6264],\n",
      "         [0.0226, 0.6750, 0.1449, 0.1575],\n",
      "         [0.0140, 0.5128, 0.1526, 0.3206],\n",
      "         [0.0041, 0.4183, 0.0296, 0.5480],\n",
      "         [0.0044, 0.6209, 0.0117, 0.3630],\n",
      "         [0.0114, 0.3643, 0.0641, 0.5602],\n",
      "         [0.0206, 0.6290, 0.0749, 0.2755],\n",
      "         [0.0327, 0.3290, 0.0836, 0.5547],\n",
      "         [0.0082, 0.8341, 0.0276, 0.1301],\n",
      "         [0.0134, 0.5411, 0.0470, 0.3986],\n",
      "         [0.0044, 0.2472, 0.0784, 0.6699],\n",
      "         [0.0075, 0.1609, 0.0855, 0.7462],\n",
      "         [0.0081, 0.5798, 0.1030, 0.3090],\n",
      "         [0.0043, 0.3565, 0.0189, 0.6203],\n",
      "         [0.0088, 0.8706, 0.0433, 0.0773],\n",
      "         [0.0083, 0.4703, 0.0749, 0.4465],\n",
      "         [0.0138, 0.8497, 0.0443, 0.0922],\n",
      "         [0.0178, 0.6780, 0.1107, 0.1934],\n",
      "         [0.0029, 0.8505, 0.0238, 0.1228],\n",
      "         [0.0060, 0.9207, 0.0440, 0.0292],\n",
      "         [0.0291, 0.3429, 0.1146, 0.5133],\n",
      "         [0.0071, 0.1827, 0.0775, 0.7326],\n",
      "         [0.0448, 0.4935, 0.0536, 0.4080],\n",
      "         [0.0219, 0.2831, 0.1045, 0.5904],\n",
      "         [0.0355, 0.6082, 0.0875, 0.2688],\n",
      "         [0.0287, 0.3570, 0.0421, 0.5722],\n",
      "         [0.0050, 0.9358, 0.0252, 0.0340],\n",
      "         [0.0080, 0.4601, 0.0617, 0.4702],\n",
      "         [0.0224, 0.5731, 0.0536, 0.3509],\n",
      "         [0.0112, 0.2207, 0.0199, 0.7482],\n",
      "         [0.0283, 0.3740, 0.0888, 0.5090],\n",
      "         [0.0136, 0.8860, 0.0441, 0.0563],\n",
      "         [0.0210, 0.8702, 0.0591, 0.0497],\n",
      "         [0.0174, 0.6810, 0.0410, 0.2606],\n",
      "         [0.0048, 0.5172, 0.0764, 0.4015],\n",
      "         [0.0118, 0.6009, 0.1454, 0.2419],\n",
      "         [0.0099, 0.4512, 0.0417, 0.4972],\n",
      "         [0.0224, 0.6384, 0.0403, 0.2989],\n",
      "         [0.0156, 0.7047, 0.0741, 0.2055],\n",
      "         [0.0098, 0.5851, 0.0296, 0.3755],\n",
      "         [0.0090, 0.8351, 0.0306, 0.1254],\n",
      "         [0.0097, 0.5792, 0.0162, 0.3948],\n",
      "         [0.0265, 0.3276, 0.0911, 0.5549],\n",
      "         [0.0104, 0.5026, 0.1800, 0.3070],\n",
      "         [0.0196, 0.0909, 0.0426, 0.8469],\n",
      "         [0.0125, 0.2711, 0.0606, 0.6558],\n",
      "         [0.0105, 0.7613, 0.0812, 0.1471],\n",
      "         [0.0217, 0.7290, 0.1126, 0.1368],\n",
      "         [0.0039, 0.9269, 0.0305, 0.0387],\n",
      "         [0.0050, 0.7721, 0.0972, 0.1256],\n",
      "         [0.0190, 0.7983, 0.0843, 0.0984],\n",
      "         [0.0034, 0.5580, 0.0373, 0.4013],\n",
      "         [0.0419, 0.5709, 0.1474, 0.2398],\n",
      "         [0.0163, 0.1271, 0.0427, 0.8140],\n",
      "         [0.0332, 0.4256, 0.0928, 0.4484],\n",
      "         [0.0098, 0.5857, 0.0288, 0.3757],\n",
      "         [0.0093, 0.8924, 0.0525, 0.0457],\n",
      "         [0.0051, 0.5552, 0.0323, 0.4074],\n",
      "         [0.0306, 0.3663, 0.0474, 0.5558],\n",
      "         [0.0076, 0.1175, 0.0831, 0.7918],\n",
      "         [0.0077, 0.1273, 0.0209, 0.8441],\n",
      "         [0.0077, 0.8449, 0.0586, 0.0887],\n",
      "         [0.0049, 0.7771, 0.0225, 0.1954],\n",
      "         [0.0256, 0.5202, 0.0634, 0.3908],\n",
      "         [0.0078, 0.7407, 0.0555, 0.1960],\n",
      "         [0.0239, 0.5537, 0.0167, 0.4057],\n",
      "         [0.0194, 0.7090, 0.0966, 0.1750],\n",
      "         [0.0080, 0.7615, 0.0742, 0.1563],\n",
      "         [0.0360, 0.7368, 0.0535, 0.1737],\n",
      "         [0.0177, 0.2664, 0.0477, 0.6683],\n",
      "         [0.0422, 0.4701, 0.2318, 0.2559],\n",
      "         [0.0086, 0.3193, 0.0903, 0.5818],\n",
      "         [0.0031, 0.1362, 0.0165, 0.8442],\n",
      "         [0.0217, 0.8858, 0.0412, 0.0513],\n",
      "         [0.0060, 0.4777, 0.0873, 0.4291],\n",
      "         [0.0032, 0.3705, 0.0411, 0.5852],\n",
      "         [0.0035, 0.1709, 0.0360, 0.7896],\n",
      "         [0.0258, 0.5194, 0.0944, 0.3604],\n",
      "         [0.0072, 0.8649, 0.0541, 0.0738],\n",
      "         [0.0067, 0.4186, 0.0453, 0.5293],\n",
      "         [0.0133, 0.3945, 0.1055, 0.4868],\n",
      "         [0.0066, 0.5222, 0.0493, 0.4219],\n",
      "         [0.0161, 0.6883, 0.0752, 0.2203],\n",
      "         [0.0051, 0.8886, 0.0526, 0.0537],\n",
      "         [0.0271, 0.1303, 0.0185, 0.8241],\n",
      "         [0.0132, 0.5918, 0.0446, 0.3504],\n",
      "         [0.0165, 0.7062, 0.0552, 0.2221],\n",
      "         [0.0347, 0.5898, 0.0498, 0.3256],\n",
      "         [0.0245, 0.0251, 0.0326, 0.9178],\n",
      "         [0.0289, 0.8277, 0.0272, 0.1163],\n",
      "         [0.0026, 0.7269, 0.0324, 0.2381],\n",
      "         [0.0091, 0.8815, 0.0337, 0.0758],\n",
      "         [0.0086, 0.7018, 0.0330, 0.2566],\n",
      "         [0.0117, 0.6722, 0.0580, 0.2581],\n",
      "         [0.0216, 0.6568, 0.0968, 0.2248],\n",
      "         [0.0256, 0.5912, 0.0232, 0.3601],\n",
      "         [0.0108, 0.2254, 0.0411, 0.7227],\n",
      "         [0.0263, 0.7152, 0.0566, 0.2018],\n",
      "         [0.0376, 0.7307, 0.0956, 0.1361],\n",
      "         [0.1505, 0.4471, 0.0406, 0.3618],\n",
      "         [0.0016, 0.9256, 0.0424, 0.0305],\n",
      "         [0.0099, 0.3570, 0.0209, 0.6122]]], grad_fn=<StackBackward>)\n",
      "Updating critic...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V_trg.shape (after critic):  torch.Size([121])\n",
      "V_trg.shape (after sum):  torch.Size([121])\n",
      "V_trg.shape (after squeeze):  torch.Size([121])\n",
      "V_trg.shape (after squeeze):  tensor([ 0.3963,  0.4217,  0.5237,  0.4862,  0.6801,  0.6123, -0.0437,  0.6187,\n",
      "         0.3806,  0.0512,  0.9405,  0.0490,  0.0828,  0.4176,  0.5132,  0.6086,\n",
      "         0.3722,  0.4692,  0.6691,  0.3618,  0.7327, -0.0430,  0.6480,  0.6231,\n",
      "         0.0369,  0.3171,  0.3145,  0.8203,  0.7878,  0.9147,  0.4102,  0.3121,\n",
      "         0.7236,  0.6386,  0.2295,  0.3357,  0.1948,  0.6600,  0.3215, -0.0799,\n",
      "         0.4675,  0.4517,  0.4442,  0.5164,  0.8158,  0.3297,  0.2554,  0.7167,\n",
      "         0.1751,  0.3909,  0.6549,  0.3500, -0.0366,  0.3840,  0.4173, -0.0406,\n",
      "         0.2460,  0.3235,  0.5474,  0.5596,  0.3818,  0.4547,  0.6292,  0.2292,\n",
      "         0.4692,  0.8067,  0.3307,  0.8355,  0.9818,  0.3875,  0.3668,  0.5064,\n",
      "         0.6577,  0.5330,  0.7229,  0.5658,  0.2051,  0.5816,  0.4653,  0.5461,\n",
      "         0.4828,  0.5596,  0.6507,  0.2847,  0.7959,  0.5225,  0.2395,  0.6833,\n",
      "         0.7408,  0.5733,  0.2593,  0.5357,  0.4928,  0.4430,  0.6282,  1.4915,\n",
      "         0.4911,  0.3733,  0.4310,  0.3833,  0.9982,  0.8572,  0.3775,  0.8705,\n",
      "         0.3832,  1.0779,  0.6947,  0.7539,  0.4931,  0.7461,  0.4012,  0.4276,\n",
      "         0.0599,  0.8846,  0.4075,  0.9134,  0.9687,  0.5008,  1.0975,  1.1110,\n",
      "         0.6200])\n",
      "V1.shape:  torch.Size([121])\n",
      "V1:  tensor([[ 0.8671],\n",
      "        [ 0.7222],\n",
      "        [ 1.1851],\n",
      "        [ 0.8711],\n",
      "        [ 0.6708],\n",
      "        [ 0.9193],\n",
      "        [ 0.5736],\n",
      "        [ 0.7164],\n",
      "        [ 0.9773],\n",
      "        [ 1.4540],\n",
      "        [ 0.9184],\n",
      "        [ 0.6744],\n",
      "        [ 1.7962],\n",
      "        [ 0.5082],\n",
      "        [ 0.5162],\n",
      "        [ 0.4873],\n",
      "        [ 1.3529],\n",
      "        [ 0.4652],\n",
      "        [ 0.6673],\n",
      "        [ 0.4261],\n",
      "        [ 1.7723],\n",
      "        [ 1.1272],\n",
      "        [ 0.9886],\n",
      "        [ 0.1768],\n",
      "        [ 0.3713],\n",
      "        [ 0.7305],\n",
      "        [ 2.1356],\n",
      "        [ 1.4713],\n",
      "        [ 0.8016],\n",
      "        [ 1.1032],\n",
      "        [ 0.6725],\n",
      "        [ 0.6221],\n",
      "        [ 1.3302],\n",
      "        [ 0.4633],\n",
      "        [ 0.7140],\n",
      "        [ 1.1621],\n",
      "        [ 1.7123],\n",
      "        [ 1.2095],\n",
      "        [ 0.4875],\n",
      "        [ 1.5310],\n",
      "        [ 1.3000],\n",
      "        [ 1.3684],\n",
      "        [ 0.5705],\n",
      "        [ 0.4394],\n",
      "        [ 0.2779],\n",
      "        [ 0.6160],\n",
      "        [ 1.7232],\n",
      "        [ 0.7291],\n",
      "        [ 0.7052],\n",
      "        [ 1.3215],\n",
      "        [ 0.7916],\n",
      "        [ 0.6901],\n",
      "        [ 0.9814],\n",
      "        [-0.1639],\n",
      "        [ 0.4661],\n",
      "        [ 1.2940],\n",
      "        [ 1.3509],\n",
      "        [ 0.8710],\n",
      "        [ 1.2100],\n",
      "        [ 1.7635],\n",
      "        [ 0.9917],\n",
      "        [ 0.7408],\n",
      "        [ 1.2182],\n",
      "        [ 1.1681],\n",
      "        [ 1.3648],\n",
      "        [ 1.0030],\n",
      "        [ 1.5184],\n",
      "        [ 1.1251],\n",
      "        [ 1.4735],\n",
      "        [ 0.9475],\n",
      "        [ 0.5223],\n",
      "        [ 0.7368],\n",
      "        [ 1.1441],\n",
      "        [ 0.6335],\n",
      "        [ 0.3985],\n",
      "        [ 1.0828],\n",
      "        [ 0.5188],\n",
      "        [ 1.0130],\n",
      "        [ 1.1399],\n",
      "        [ 1.7934],\n",
      "        [ 0.7409],\n",
      "        [ 1.7515],\n",
      "        [ 1.3330],\n",
      "        [ 0.8188],\n",
      "        [ 1.3245],\n",
      "        [ 0.8198],\n",
      "        [ 0.3037],\n",
      "        [ 1.2508],\n",
      "        [ 0.4815],\n",
      "        [ 0.9079],\n",
      "        [ 1.2623],\n",
      "        [ 1.8004],\n",
      "        [ 0.8517],\n",
      "        [ 0.6642],\n",
      "        [ 1.1570],\n",
      "        [ 0.6838],\n",
      "        [ 1.2494],\n",
      "        [ 1.1783],\n",
      "        [ 1.4901],\n",
      "        [ 0.9523],\n",
      "        [ 1.4604],\n",
      "        [ 0.8324],\n",
      "        [ 1.1722],\n",
      "        [ 1.3427],\n",
      "        [ 1.4014],\n",
      "        [ 0.7259],\n",
      "        [ 0.8959],\n",
      "        [ 0.5896],\n",
      "        [ 0.9628],\n",
      "        [ 0.8641],\n",
      "        [ 0.6462],\n",
      "        [ 1.0767],\n",
      "        [ 0.9063],\n",
      "        [ 1.3780],\n",
      "        [ 1.4174],\n",
      "        [ 1.2233],\n",
      "        [ 1.2154],\n",
      "        [ 1.5627],\n",
      "        [ 0.6252],\n",
      "        [ 0.9788],\n",
      "        [ 1.9485]], grad_fn=<AddmmBackward>)\n",
      "Updating actor...\n",
      "V_trg.shape:  torch.Size([121])\n",
      "V_trg:  tensor([ 0.3556,  0.7044,  0.2125,  0.2346,  0.2645,  0.2605,  0.0632,  0.5398,\n",
      "         0.2900,  0.4603,  0.4223,  0.4069,  0.5623,  0.3234,  0.3711,  0.3241,\n",
      "         0.2597,  0.2704,  0.4679,  0.3180,  0.2867,  0.7062,  0.2939,  0.1145,\n",
      "         0.5227,  0.3747,  0.6429,  0.2097,  0.1132,  0.4061,  0.3391,  0.3232,\n",
      "         0.5165,  0.0081,  0.2717,  0.2755,  0.2861,  0.1450,  0.0996,  0.4418,\n",
      "         0.4437,  0.1621,  0.1268,  0.6094,  0.6392,  0.5126, -0.1131,  0.2187,\n",
      "         0.2197,  0.4988,  0.5287,  0.2375,  0.4929,  0.3495,  0.2662,  0.1185,\n",
      "         0.3831,  0.5024,  0.3802,  0.2466,  0.7679,  0.4968,  0.3507,  0.3194,\n",
      "         0.3567,  0.2917,  0.5553,  0.0925,  0.0998,  0.4179,  0.7390,  0.1870,\n",
      "         0.6935,  0.3280,  0.2341,  0.1189,  0.7060,  0.1113,  0.4434,  0.4286,\n",
      "         0.6855,  0.1234,  0.2102,  0.5364,  0.0429,  0.2705,  0.3835,  0.6270,\n",
      "         0.2789,  0.0534,  0.4050,  0.2590,  0.4200,  0.6594,  0.3769,  0.6165,\n",
      "         0.3320,  0.2732,  0.5967,  0.1823,  0.1586,  0.7499,  0.3307,  0.5814,\n",
      "         0.1230,  0.7338,  0.3464,  0.7272,  0.2982,  0.2929,  0.5496,  0.2545,\n",
      "         0.3602,  0.7164,  0.4887,  0.5291,  0.3105,  0.4719,  0.7281,  0.3768,\n",
      "         0.3975])\n",
      "V_pred.shape:  torch.Size([121])\n",
      "V_pred:  tensor([-0.0707,  0.8888,  0.3975,  0.8534,  0.4219,  0.5446,  0.6005,  0.4109,\n",
      "         0.3881,  0.1320,  0.7518,  0.7183,  0.5507,  0.3968,  0.8177,  0.5246,\n",
      "         0.4901,  1.0566,  0.6254,  0.1197,  0.4337,  0.5597,  0.5921,  0.4476,\n",
      "         0.7975,  0.8118,  0.3075,  0.4798,  0.3802,  0.6902,  0.4479,  0.6904,\n",
      "         0.5082,  0.3530,  0.3447,  0.6899,  0.4529,  0.6409,  0.5308,  0.8876,\n",
      "         0.5969,  0.6452, -0.1175,  0.6435,  0.2037,  0.6009,  0.1944,  0.6336,\n",
      "         0.1470,  0.9281,  0.6842,  0.6790,  0.6286,  0.7511,  0.4573,  0.4850,\n",
      "         0.7742,  0.5758,  0.7653,  0.1382,  0.4602,  0.2903, -0.0073,  0.4575,\n",
      "         0.6616,  0.4221,  0.4300,  0.9175,  0.6607,  0.0265,  0.2749,  0.8813,\n",
      "         0.9415,  0.4885,  0.8872,  0.4946,  0.1631,  0.4455,  0.7288,  1.0976,\n",
      "         0.4993,  0.3683,  0.2299,  0.4031,  0.3993,  0.4741,  0.7553,  0.5262,\n",
      "         0.4259, -0.1609,  0.4110,  0.7132,  0.4964,  0.5446,  0.5297,  0.1694,\n",
      "         0.1145,  0.6275,  0.9707,  0.8076,  0.4028,  0.5454,  0.7095,  0.8717,\n",
      "         0.1473,  0.9229,  0.8807,  0.6697,  0.7074, -0.0067,  0.3603,  0.2879,\n",
      "         0.5500,  0.4144,  0.2197,  0.4619,  0.0648,  0.1126,  0.0943,  0.0982,\n",
      "         1.1337])\n",
      "A.shape:  torch.Size([121])\n",
      "A:  tensor([ 0.4263, -0.1844, -0.1850, -0.6188, -0.1575, -0.2841, -0.5373,  0.1289,\n",
      "        -0.0982,  0.3282, -0.3295, -0.3114,  0.0116, -0.0734, -0.4466, -0.2005,\n",
      "        -0.2305, -0.7862, -0.1575,  0.1983, -0.1471,  0.1465, -0.2981, -0.3332,\n",
      "        -0.2749, -0.4371,  0.3355, -0.2701, -0.2670, -0.2842, -0.1088, -0.3672,\n",
      "         0.0084, -0.3449, -0.0730, -0.4144, -0.1668, -0.4959, -0.4312, -0.4458,\n",
      "        -0.1532, -0.4832,  0.2443, -0.0341,  0.4355, -0.0883, -0.3075, -0.4149,\n",
      "         0.0727, -0.4293, -0.1554, -0.4415, -0.1357, -0.4016, -0.1911, -0.3666,\n",
      "        -0.3911, -0.0734, -0.3850,  0.1084,  0.3077,  0.2064,  0.3580, -0.1381,\n",
      "        -0.3049, -0.1303,  0.1253, -0.8250, -0.5609,  0.3914,  0.4640, -0.6943,\n",
      "        -0.2480, -0.1606, -0.6531, -0.3756,  0.5430, -0.3343, -0.2855, -0.6690,\n",
      "         0.1861, -0.2449, -0.0197,  0.1333, -0.3564, -0.2036, -0.3718,  0.1009,\n",
      "        -0.1470,  0.2143, -0.0060, -0.4541, -0.0764,  0.1148, -0.1528,  0.4471,\n",
      "         0.2175, -0.3543, -0.3740, -0.6253, -0.2441,  0.2045, -0.3788, -0.2903,\n",
      "        -0.0243, -0.1891, -0.5343,  0.0574, -0.4092,  0.2997,  0.1893, -0.0334,\n",
      "        -0.1898,  0.3020,  0.2690,  0.0672,  0.2457,  0.3593,  0.6339,  0.2786,\n",
      "        -0.7363])\n",
      "policy_gradient.shape:  torch.Size([121])\n",
      "policy_gradient:  tensor([ 0.0483, -0.3133, -0.1118, -0.2975, -0.0167, -0.0790, -0.7431,  0.0166,\n",
      "        -0.0336,  0.0765, -0.0561, -0.1108,  0.0040, -0.1899, -0.1209, -0.3490,\n",
      "        -0.0848, -0.2284, -0.2540,  0.0927, -0.2718,  0.0978, -0.1794, -0.3376,\n",
      "        -0.2776, -0.2027,  0.3729, -0.0490, -0.2456, -0.3971, -0.0318, -0.2002,\n",
      "         0.0040, -0.0478, -0.0551, -0.0675, -0.0648, -1.8530, -0.0356, -0.2973,\n",
      "        -0.0477, -0.3412,  0.1287, -0.0169,  0.2431, -0.0059, -0.2387, -0.4345,\n",
      "         0.1098, -1.5305, -0.0188, -0.0614, -0.0521, -0.3665, -0.2712, -0.2562,\n",
      "        -0.1755, -0.0257, -0.2064,  0.2250,  0.2860,  0.1216,  0.2462, -0.0229,\n",
      "        -0.1286, -0.0356,  0.0396, -0.0626, -0.1451,  0.0882,  0.2707, -1.3294,\n",
      "        -0.0510, -0.1371, -0.3493, -0.0428,  0.3194, -0.1963, -0.0666, -0.1134,\n",
      "         0.0314, -0.3997, -0.0185,  0.0400, -0.2107, -0.3549, -0.1013,  0.0308,\n",
      "        -0.0593,  0.3133, -0.0033, -0.0769, -0.0093,  0.0848, -0.0819,  1.4861,\n",
      "         0.2220, -0.0514, -0.2379, -0.4502, -0.1586,  0.0764, -0.0447, -0.0562,\n",
      "        -0.0127, -0.5478, -0.2821,  0.0049, -0.0774,  0.0956,  0.0239, -0.0454,\n",
      "        -0.2571,  0.1269,  0.2748,  0.0218,  0.3932,  0.1127,  0.5103,  0.0215,\n",
      "        -0.3612], grad_fn=<MulBackward0>)\n",
      "policy_grad:  tensor(-11.5695, grad_fn=<SumBackward0>)\n",
      "Negative entropy:  tensor(-93.7841, grad_fn=<SumBackward0>)\n",
      "Actor loss:  tensor(-12.5073, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distribution:  tensor([[0.0229, 0.6764, 0.0281, 0.2725]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0369, 0.4177, 0.0324, 0.5131]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0079, 0.8398, 0.1017, 0.0506]], grad_fn=<ExpBackward>)\n",
      "n_step_rewards.shape:  (3,)\n",
      "rewards.shape:  (3,)\n",
      "n_step_rewards:  [ 9.801  9.9   10.   ]\n",
      "rewards:  [ 0.  0. 10.]\n",
      "bootstrap:  [False False False]\n",
      "done.shape: (before n_steps) (3,)\n",
      "done: (before n_steps) [False False  True]\n",
      "done.shape: (after n_steps) (3,)\n",
      "Gamma_V.shape:  (3,)\n",
      "done: (after n_steps) [ True  True  True]\n",
      "Gamma_V:  [0.970299 0.9801   0.99    ]\n",
      "old_states.shape:  torch.Size([3, 1, 9, 9])\n",
      "new_states.shape:  torch.Size([3, 1, 9, 9])\n",
      "log_probs:  [tensor(-0.3909, grad_fn=<SelectBackward>), tensor(-0.6673, grad_fn=<SelectBackward>), tensor(-2.9840, grad_fn=<SelectBackward>)]\n",
      "log_probs:  tensor([-0.3909, -0.6673, -2.9840], grad_fn=<StackBackward>)\n",
      "distributions.shape:  torch.Size([1, 3, 4])\n",
      "distributions:  tensor([[[0.0229, 0.6764, 0.0281, 0.2725],\n",
      "         [0.0369, 0.4177, 0.0324, 0.5131],\n",
      "         [0.0079, 0.8398, 0.1017, 0.0506]]], grad_fn=<StackBackward>)\n",
      "Updating critic...\n",
      "V_trg.shape (after critic):  torch.Size([3])\n",
      "V_trg.shape (after sum):  torch.Size([3])\n",
      "V_trg.shape (after squeeze):  torch.Size([3])\n",
      "V_trg.shape (after squeeze):  tensor([ 9.8010,  9.9000, 10.0000])\n",
      "V1.shape:  torch.Size([3])\n",
      "V1:  tensor([[1.4383],\n",
      "        [0.2207],\n",
      "        [0.6072]], grad_fn=<AddmmBackward>)\n",
      "Updating actor...\n",
      "V_trg.shape:  torch.Size([3])\n",
      "V_trg:  tensor([ 9.8010,  9.9000, 10.0000])\n",
      "V_pred.shape:  torch.Size([3])\n",
      "V_pred:  tensor([-0.2222,  0.4504,  0.7360])\n",
      "A.shape:  torch.Size([3])\n",
      "A:  tensor([10.0232,  9.4496,  9.2640])\n",
      "policy_gradient.shape:  torch.Size([3])\n",
      "policy_gradient:  tensor([ 3.9183,  6.3056, 27.6436], grad_fn=<MulBackward0>)\n",
      "policy_grad:  tensor(37.8676, grad_fn=<SumBackward0>)\n",
      "Negative entropy:  tensor(-2.3138, grad_fn=<SumBackward0>)\n",
      "Actor loss:  tensor(37.8444, grad_fn=<AddBackward0>)\n",
      "distribution:  tensor([[0.0084, 0.3010, 0.0943, 0.5963]], grad_fn=<ExpBackward>)\n",
      "n_step_rewards.shape:  (1,)\n",
      "rewards.shape:  (1,)\n",
      "n_step_rewards:  [10.]\n",
      "rewards:  [10.]\n",
      "bootstrap:  [False]\n",
      "done.shape: (before n_steps) (1,)\n",
      "done: (before n_steps) [ True]\n",
      "done.shape: (after n_steps) (1,)\n",
      "Gamma_V.shape:  (1,)\n",
      "done: (after n_steps) [ True]\n",
      "Gamma_V:  [0.99]\n",
      "old_states.shape:  torch.Size([1, 1, 9, 9])\n",
      "new_states.shape:  torch.Size([1, 1, 9, 9])\n",
      "log_probs:  [tensor(-0.5171, grad_fn=<SelectBackward>)]\n",
      "log_probs:  tensor([-0.5171], grad_fn=<StackBackward>)\n",
      "distributions.shape:  torch.Size([1, 1, 4])\n",
      "distributions:  tensor([[[0.0084, 0.3010, 0.0943, 0.5963]]], grad_fn=<StackBackward>)\n",
      "Updating critic...\n",
      "V_trg.shape (after critic):  torch.Size([])\n",
      "V_trg.shape (after sum):  torch.Size([1])\n",
      "V_trg.shape (after squeeze):  torch.Size([])\n",
      "V_trg.shape (after squeeze):  tensor(10.)\n",
      "V1.shape:  torch.Size([])\n",
      "V1:  tensor([[1.1842]], grad_fn=<AddmmBackward>)\n",
      "Updating actor...\n",
      "V_trg.shape:  torch.Size([1])\n",
      "V_trg:  tensor([10.])\n",
      "V_pred.shape:  torch.Size([])\n",
      "V_pred:  tensor(0.6626)\n",
      "A.shape:  torch.Size([1])\n",
      "A:  tensor([9.3374])\n",
      "policy_gradient.shape:  torch.Size([1])\n",
      "policy_gradient:  tensor([4.8282], grad_fn=<MulBackward0>)\n",
      "policy_grad:  tensor(4.8282, grad_fn=<SumBackward0>)\n",
      "Negative entropy:  tensor(-0.9326, grad_fn=<SumBackward0>)\n",
      "Actor loss:  tensor(4.8188, grad_fn=<AddBackward0>)\n",
      "distribution:  tensor([[0.0074, 0.2655, 0.0273, 0.6998]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0183, 0.1265, 0.0250, 0.8301]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0111, 0.5630, 0.0384, 0.3875]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0111, 0.3938, 0.0389, 0.5562]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0024, 0.0372, 0.0037, 0.9567]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0120, 0.9020, 0.0123, 0.0737]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0082, 0.7460, 0.0358, 0.2100]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0086, 0.3571, 0.0100, 0.6243]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0128, 0.1465, 0.0294, 0.8112]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0083, 0.6636, 0.0203, 0.3077]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0063, 0.0461, 0.0122, 0.9354]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0075, 0.6862, 0.0407, 0.2655]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0029, 0.8180, 0.0731, 0.1060]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0058, 0.0388, 0.0064, 0.9490]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0045, 0.3153, 0.0222, 0.6580]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0130, 0.8727, 0.0056, 0.1087]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0042, 0.8998, 0.0228, 0.0732]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0087, 0.1624, 0.0329, 0.7960]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0090, 0.7913, 0.0160, 0.1837]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0116, 0.6615, 0.0551, 0.2718]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0195, 0.1496, 0.0614, 0.7695]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0059, 0.8185, 0.0528, 0.1228]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0041, 0.1649, 0.0146, 0.8164]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0041, 0.2518, 0.0111, 0.7331]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0071, 0.8221, 0.0502, 0.1206]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0034, 0.5666, 0.0464, 0.3835]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0081, 0.3269, 0.0153, 0.6497]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0050, 0.3427, 0.0199, 0.6324]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0065, 0.2887, 0.1479, 0.5569]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0039, 0.8160, 0.0400, 0.1400]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0084, 0.0505, 0.0106, 0.9305]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0042, 0.9159, 0.0211, 0.0588]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0043, 0.4832, 0.0221, 0.4904]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0024, 0.4537, 0.0847, 0.4592]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0108, 0.6151, 0.0350, 0.3391]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0083, 0.8934, 0.0266, 0.0717]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0079, 0.9169, 0.0097, 0.0655]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0172, 0.4676, 0.0775, 0.4376]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0035, 0.7320, 0.0162, 0.2483]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0222, 0.5930, 0.0716, 0.3132]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0022, 0.4421, 0.0281, 0.5276]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0196, 0.5260, 0.1068, 0.3476]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0027, 0.3716, 0.0969, 0.5288]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0104, 0.2616, 0.0453, 0.6828]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0471, 0.3253, 0.0219, 0.6057]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0019, 0.7776, 0.0246, 0.1959]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0028, 0.4844, 0.0419, 0.4709]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0057, 0.3409, 0.0384, 0.6150]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0103, 0.1342, 0.0147, 0.8407]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0053, 0.6323, 0.0441, 0.3183]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0892, 0.6026, 0.0784, 0.2297]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0030, 0.8178, 0.0402, 0.1390]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0083, 0.8553, 0.0347, 0.1017]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0187, 0.3166, 0.0437, 0.6211]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0067, 0.2026, 0.0234, 0.7674]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0022, 0.6801, 0.0151, 0.3027]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0060, 0.7027, 0.0349, 0.2563]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0203, 0.1909, 0.0195, 0.7694]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0146, 0.5660, 0.1142, 0.3052]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0387, 0.6898, 0.0243, 0.2471]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0148, 0.8480, 0.0279, 0.1093]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0185, 0.4038, 0.0765, 0.5012]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0040, 0.3337, 0.0094, 0.6529]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0021, 0.4351, 0.0273, 0.5355]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[6.0058e-04, 2.0244e-02, 3.6487e-03, 9.7551e-01]],\n",
      "       grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0064, 0.6686, 0.0923, 0.2327]], grad_fn=<ExpBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distribution:  tensor([[0.0051, 0.5731, 0.0281, 0.3937]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0047, 0.8337, 0.0103, 0.1514]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0114, 0.4522, 0.0327, 0.5037]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0016, 0.6118, 0.0187, 0.3679]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0059, 0.1498, 0.0277, 0.8167]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0062, 0.5841, 0.0372, 0.3726]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0057, 0.2791, 0.0423, 0.6729]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0145, 0.7838, 0.0365, 0.1651]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0060, 0.6616, 0.0357, 0.2968]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0044, 0.7524, 0.0445, 0.1987]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0061, 0.8141, 0.0336, 0.1463]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0159, 0.2874, 0.0885, 0.6082]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0015, 0.1645, 0.0146, 0.8194]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0038, 0.0876, 0.0127, 0.8959]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0121, 0.5863, 0.0370, 0.3645]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0156, 0.0899, 0.0573, 0.8372]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0040, 0.7906, 0.0163, 0.1892]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0105, 0.8098, 0.0472, 0.1326]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0414, 0.5839, 0.0634, 0.3113]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0053, 0.9238, 0.0241, 0.0468]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0115, 0.8544, 0.0225, 0.1117]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0114, 0.1558, 0.0887, 0.7441]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0066, 0.8100, 0.0491, 0.1343]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0040, 0.5760, 0.0464, 0.3736]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0034, 0.8396, 0.0517, 0.1053]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0038, 0.4328, 0.0430, 0.5204]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0313, 0.2299, 0.0204, 0.7184]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0027, 0.0841, 0.0067, 0.9065]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0064, 0.1912, 0.1104, 0.6920]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0038, 0.0344, 0.0071, 0.9548]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0068, 0.1571, 0.0085, 0.8275]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0032, 0.0705, 0.0151, 0.9112]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0199, 0.0883, 0.0193, 0.8725]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0176, 0.1764, 0.0267, 0.7794]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0036, 0.5366, 0.1022, 0.3576]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0032, 0.8995, 0.0286, 0.0686]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0106, 0.6185, 0.0770, 0.2939]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0142, 0.3289, 0.0268, 0.6301]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0014, 0.0397, 0.0052, 0.9537]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0087, 0.3906, 0.0309, 0.5699]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0136, 0.4557, 0.0268, 0.5039]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0102, 0.3894, 0.0579, 0.5425]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0110, 0.6749, 0.0643, 0.2498]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0014, 0.1001, 0.0180, 0.8805]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0083, 0.5217, 0.0244, 0.4456]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0143, 0.1747, 0.0192, 0.7919]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0102, 0.3442, 0.0683, 0.5773]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0092, 0.8240, 0.0462, 0.1205]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0051, 0.5619, 0.1033, 0.3297]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0011, 0.0267, 0.0109, 0.9614]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0022, 0.0601, 0.0124, 0.9253]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0310, 0.3569, 0.0833, 0.5288]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0144, 0.7295, 0.0417, 0.2144]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0051, 0.0593, 0.0199, 0.9157]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0060, 0.0968, 0.0224, 0.8748]], grad_fn=<ExpBackward>)\n",
      "n_step_rewards.shape:  (121,)\n",
      "rewards.shape:  (121,)\n",
      "n_step_rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0.]\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0.]\n",
      "bootstrap:  [False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      "  True]\n",
      "done.shape: (before n_steps) (121,)\n",
      "done: (before n_steps) [False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False]\n",
      "done.shape: (after n_steps) (121,)\n",
      "Gamma_V.shape:  (121,)\n",
      "done: (after n_steps) [False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False]\n",
      "Gamma_V:  [0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.67572905 0.6825546\n",
      " 0.68944909 0.69641322 0.70344769 0.71055323 0.71773053 0.72498034\n",
      " 0.73230337 0.73970037 0.74717209 0.75471929 0.76234271 0.77004315\n",
      " 0.77782136 0.78567814 0.79361428 0.80163059 0.80972787 0.81790694\n",
      " 0.82616862 0.83451376 0.84294319 0.85145777 0.86005835 0.86874581\n",
      " 0.87752102 0.88638487 0.89533825 0.90438208 0.91351725 0.92274469\n",
      " 0.93206535 0.94148015 0.95099005 0.96059601 0.970299   0.9801\n",
      " 0.99      ]\n",
      "old_states.shape:  torch.Size([121, 1, 9, 9])\n",
      "new_states.shape:  torch.Size([121, 1, 9, 9])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_probs:  [tensor(-1.3262, grad_fn=<SelectBackward>), tensor(-0.1862, grad_fn=<SelectBackward>), tensor(-0.5745, grad_fn=<SelectBackward>), tensor(-0.5867, grad_fn=<SelectBackward>), tensor(-0.0442, grad_fn=<SelectBackward>), tensor(-0.1032, grad_fn=<SelectBackward>), tensor(-0.2930, grad_fn=<SelectBackward>), tensor(-1.0297, grad_fn=<SelectBackward>), tensor(-1.9208, grad_fn=<SelectBackward>), tensor(-0.4100, grad_fn=<SelectBackward>), tensor(-0.0667, grad_fn=<SelectBackward>), tensor(-1.3261, grad_fn=<SelectBackward>), tensor(-2.2444, grad_fn=<SelectBackward>), tensor(-0.0523, grad_fn=<SelectBackward>), tensor(-0.4185, grad_fn=<SelectBackward>), tensor(-0.1361, grad_fn=<SelectBackward>), tensor(-0.1056, grad_fn=<SelectBackward>), tensor(-0.2281, grad_fn=<SelectBackward>), tensor(-0.2341, grad_fn=<SelectBackward>), tensor(-0.4133, grad_fn=<SelectBackward>), tensor(-0.2621, grad_fn=<SelectBackward>), tensor(-0.2002, grad_fn=<SelectBackward>), tensor(-0.2028, grad_fn=<SelectBackward>), tensor(-0.3105, grad_fn=<SelectBackward>), tensor(-2.1155, grad_fn=<SelectBackward>), tensor(-0.9583, grad_fn=<SelectBackward>), tensor(-1.1181, grad_fn=<SelectBackward>), tensor(-1.0708, grad_fn=<SelectBackward>), tensor(-1.2425, grad_fn=<SelectBackward>), tensor(-0.2034, grad_fn=<SelectBackward>), tensor(-0.0720, grad_fn=<SelectBackward>), tensor(-0.0879, grad_fn=<SelectBackward>), tensor(-0.7126, grad_fn=<SelectBackward>), tensor(-0.7904, grad_fn=<SelectBackward>), tensor(-0.4860, grad_fn=<SelectBackward>), tensor(-0.1127, grad_fn=<SelectBackward>), tensor(-0.0867, grad_fn=<SelectBackward>), tensor(-0.8265, grad_fn=<SelectBackward>), tensor(-0.3119, grad_fn=<SelectBackward>), tensor(-2.6369, grad_fn=<SelectBackward>), tensor(-0.8162, grad_fn=<SelectBackward>), tensor(-1.0567, grad_fn=<SelectBackward>), tensor(-0.9900, grad_fn=<SelectBackward>), tensor(-0.3816, grad_fn=<SelectBackward>), tensor(-0.5013, grad_fn=<SelectBackward>), tensor(-0.2516, grad_fn=<SelectBackward>), tensor(-0.7532, grad_fn=<SelectBackward>), tensor(-0.4861, grad_fn=<SelectBackward>), tensor(-0.1735, grad_fn=<SelectBackward>), tensor(-0.4584, grad_fn=<SelectBackward>), tensor(-0.5065, grad_fn=<SelectBackward>), tensor(-0.2011, grad_fn=<SelectBackward>), tensor(-0.1563, grad_fn=<SelectBackward>), tensor(-0.4763, grad_fn=<SelectBackward>), tensor(-0.2648, grad_fn=<SelectBackward>), tensor(-1.1950, grad_fn=<SelectBackward>), tensor(-0.3528, grad_fn=<SelectBackward>), tensor(-0.2622, grad_fn=<SelectBackward>), tensor(-0.5691, grad_fn=<SelectBackward>), tensor(-1.3978, grad_fn=<SelectBackward>), tensor(-0.1648, grad_fn=<SelectBackward>), tensor(-0.6908, grad_fn=<SelectBackward>), tensor(-0.4263, grad_fn=<SelectBackward>), tensor(-0.6246, grad_fn=<SelectBackward>), tensor(-0.0248, grad_fn=<SelectBackward>), tensor(-1.4579, grad_fn=<SelectBackward>), tensor(-0.5567, grad_fn=<SelectBackward>), tensor(-0.1819, grad_fn=<SelectBackward>), tensor(-0.7935, grad_fn=<SelectBackward>), tensor(-0.4913, grad_fn=<SelectBackward>), tensor(-0.2025, grad_fn=<SelectBackward>), tensor(-0.9873, grad_fn=<SelectBackward>), tensor(-0.3961, grad_fn=<SelectBackward>), tensor(-0.2436, grad_fn=<SelectBackward>), tensor(-0.4131, grad_fn=<SelectBackward>), tensor(-0.2845, grad_fn=<SelectBackward>), tensor(-0.2057, grad_fn=<SelectBackward>), tensor(-1.2469, grad_fn=<SelectBackward>), tensor(-1.8048, grad_fn=<SelectBackward>), tensor(-0.1099, grad_fn=<SelectBackward>), tensor(-0.5338, grad_fn=<SelectBackward>), tensor(-0.1777, grad_fn=<SelectBackward>), tensor(-0.2350, grad_fn=<SelectBackward>), tensor(-0.2110, grad_fn=<SelectBackward>), tensor(-0.5380, grad_fn=<SelectBackward>), tensor(-0.0792, grad_fn=<SelectBackward>), tensor(-0.1574, grad_fn=<SelectBackward>), tensor(-1.8590, grad_fn=<SelectBackward>), tensor(-0.2108, grad_fn=<SelectBackward>), tensor(-0.5517, grad_fn=<SelectBackward>), tensor(-0.1748, grad_fn=<SelectBackward>), tensor(-0.8376, grad_fn=<SelectBackward>), tensor(-0.3307, grad_fn=<SelectBackward>), tensor(-0.0981, grad_fn=<SelectBackward>), tensor(-0.3681, grad_fn=<SelectBackward>), tensor(-0.0462, grad_fn=<SelectBackward>), tensor(-0.1893, grad_fn=<SelectBackward>), tensor(-0.0930, grad_fn=<SelectBackward>), tensor(-0.1364, grad_fn=<SelectBackward>), tensor(-0.2492, grad_fn=<SelectBackward>), tensor(-0.6225, grad_fn=<SelectBackward>), tensor(-0.1059, grad_fn=<SelectBackward>), tensor(-0.4804, grad_fn=<SelectBackward>), tensor(-1.1120, grad_fn=<SelectBackward>), tensor(-0.0474, grad_fn=<SelectBackward>), tensor(-0.9401, grad_fn=<SelectBackward>), tensor(-0.7859, grad_fn=<SelectBackward>), tensor(-0.9432, grad_fn=<SelectBackward>), tensor(-2.7443, grad_fn=<SelectBackward>), tensor(-0.1273, grad_fn=<SelectBackward>), tensor(-0.8083, grad_fn=<SelectBackward>), tensor(-0.2334, grad_fn=<SelectBackward>), tensor(-1.0666, grad_fn=<SelectBackward>), tensor(-0.1936, grad_fn=<SelectBackward>), tensor(-0.5765, grad_fn=<SelectBackward>), tensor(-0.0394, grad_fn=<SelectBackward>), tensor(-0.0776, grad_fn=<SelectBackward>), tensor(-1.0303, grad_fn=<SelectBackward>), tensor(-0.3154, grad_fn=<SelectBackward>), tensor(-0.0880, grad_fn=<SelectBackward>), tensor(-0.1338, grad_fn=<SelectBackward>)]\n",
      "log_probs:  tensor([-1.3262, -0.1862, -0.5745, -0.5867, -0.0442, -0.1032, -0.2930, -1.0297,\n",
      "        -1.9208, -0.4100, -0.0667, -1.3261, -2.2444, -0.0523, -0.4185, -0.1361,\n",
      "        -0.1056, -0.2281, -0.2341, -0.4133, -0.2621, -0.2002, -0.2028, -0.3105,\n",
      "        -2.1155, -0.9583, -1.1181, -1.0708, -1.2425, -0.2034, -0.0720, -0.0879,\n",
      "        -0.7126, -0.7904, -0.4860, -0.1127, -0.0867, -0.8265, -0.3119, -2.6369,\n",
      "        -0.8162, -1.0567, -0.9900, -0.3816, -0.5013, -0.2516, -0.7532, -0.4861,\n",
      "        -0.1735, -0.4584, -0.5065, -0.2011, -0.1563, -0.4763, -0.2648, -1.1950,\n",
      "        -0.3528, -0.2622, -0.5691, -1.3978, -0.1648, -0.6908, -0.4263, -0.6246,\n",
      "        -0.0248, -1.4579, -0.5567, -0.1819, -0.7935, -0.4913, -0.2025, -0.9873,\n",
      "        -0.3961, -0.2436, -0.4131, -0.2845, -0.2057, -1.2469, -1.8048, -0.1099,\n",
      "        -0.5338, -0.1777, -0.2350, -0.2110, -0.5380, -0.0792, -0.1574, -1.8590,\n",
      "        -0.2108, -0.5517, -0.1748, -0.8376, -0.3307, -0.0981, -0.3681, -0.0462,\n",
      "        -0.1893, -0.0930, -0.1364, -0.2492, -0.6225, -0.1059, -0.4804, -1.1120,\n",
      "        -0.0474, -0.9401, -0.7859, -0.9432, -2.7443, -0.1273, -0.8083, -0.2334,\n",
      "        -1.0666, -0.1936, -0.5765, -0.0394, -0.0776, -1.0303, -0.3154, -0.0880,\n",
      "        -0.1338], grad_fn=<StackBackward>)\n",
      "distributions.shape:  torch.Size([1, 121, 4])\n",
      "distributions:  tensor([[[7.4221e-03, 2.6548e-01, 2.7304e-02, 6.9979e-01],\n",
      "         [1.8337e-02, 1.2655e-01, 2.5024e-02, 8.3009e-01],\n",
      "         [1.1065e-02, 5.6299e-01, 3.8406e-02, 3.8754e-01],\n",
      "         [1.1140e-02, 3.9380e-01, 3.8887e-02, 5.5617e-01],\n",
      "         [2.3756e-03, 3.7199e-02, 3.6928e-03, 9.5673e-01],\n",
      "         [1.1997e-02, 9.0199e-01, 1.2286e-02, 7.3730e-02],\n",
      "         [8.1612e-03, 7.4604e-01, 3.5762e-02, 2.1003e-01],\n",
      "         [8.5617e-03, 3.5712e-01, 1.0039e-02, 6.2428e-01],\n",
      "         [1.2850e-02, 1.4649e-01, 2.9446e-02, 8.1122e-01],\n",
      "         [8.3141e-03, 6.6363e-01, 2.0343e-02, 3.0772e-01],\n",
      "         [6.3011e-03, 4.6085e-02, 1.2172e-02, 9.3544e-01],\n",
      "         [7.5055e-03, 6.8624e-01, 4.0736e-02, 2.6552e-01],\n",
      "         [2.8701e-03, 8.1801e-01, 7.3131e-02, 1.0599e-01],\n",
      "         [5.7933e-03, 3.8795e-02, 6.3666e-03, 9.4905e-01],\n",
      "         [4.4615e-03, 3.1529e-01, 2.2217e-02, 6.5803e-01],\n",
      "         [1.3001e-02, 8.7274e-01, 5.5677e-03, 1.0869e-01],\n",
      "         [4.2405e-03, 8.9978e-01, 2.2828e-02, 7.3154e-02],\n",
      "         [8.6787e-03, 1.6236e-01, 3.2945e-02, 7.9601e-01],\n",
      "         [8.9836e-03, 7.9130e-01, 1.6017e-02, 1.8370e-01],\n",
      "         [1.1618e-02, 6.6149e-01, 5.5138e-02, 2.7175e-01],\n",
      "         [1.9483e-02, 1.4963e-01, 6.1434e-02, 7.6946e-01],\n",
      "         [5.9062e-03, 8.1853e-01, 5.2798e-02, 1.2277e-01],\n",
      "         [4.0898e-03, 1.6489e-01, 1.4577e-02, 8.1645e-01],\n",
      "         [4.0578e-03, 2.5176e-01, 1.1074e-02, 7.3311e-01],\n",
      "         [7.0688e-03, 8.2211e-01, 5.0244e-02, 1.2057e-01],\n",
      "         [3.4390e-03, 5.6658e-01, 4.6437e-02, 3.8354e-01],\n",
      "         [8.0788e-03, 3.2690e-01, 1.5315e-02, 6.4971e-01],\n",
      "         [5.0120e-03, 3.4273e-01, 1.9896e-02, 6.3236e-01],\n",
      "         [6.5375e-03, 2.8865e-01, 1.4786e-01, 5.5695e-01],\n",
      "         [3.9343e-03, 8.1599e-01, 4.0042e-02, 1.4003e-01],\n",
      "         [8.4166e-03, 5.0516e-02, 1.0553e-02, 9.3052e-01],\n",
      "         [4.1701e-03, 9.1585e-01, 2.1143e-02, 5.8834e-02],\n",
      "         [4.3182e-03, 4.8325e-01, 2.2082e-02, 4.9035e-01],\n",
      "         [2.3964e-03, 4.5368e-01, 8.4697e-02, 4.5923e-01],\n",
      "         [1.0786e-02, 6.1511e-01, 3.5009e-02, 3.3910e-01],\n",
      "         [8.3247e-03, 8.9343e-01, 2.6593e-02, 7.1651e-02],\n",
      "         [7.9356e-03, 9.1693e-01, 9.6746e-03, 6.5461e-02],\n",
      "         [1.7244e-02, 4.6762e-01, 7.7543e-02, 4.3759e-01],\n",
      "         [3.4655e-03, 7.3204e-01, 1.6213e-02, 2.4828e-01],\n",
      "         [2.2241e-02, 5.9298e-01, 7.1585e-02, 3.1319e-01],\n",
      "         [2.2261e-03, 4.4211e-01, 2.8060e-02, 5.2761e-01],\n",
      "         [1.9557e-02, 5.2604e-01, 1.0679e-01, 3.4761e-01],\n",
      "         [2.7088e-03, 3.7159e-01, 9.6939e-02, 5.2876e-01],\n",
      "         [1.0370e-02, 2.6160e-01, 4.5253e-02, 6.8278e-01],\n",
      "         [4.7105e-02, 3.2525e-01, 2.1916e-02, 6.0573e-01],\n",
      "         [1.9300e-03, 7.7755e-01, 2.4572e-02, 1.9594e-01],\n",
      "         [2.7599e-03, 4.8443e-01, 4.1932e-02, 4.7088e-01],\n",
      "         [5.6547e-03, 3.4085e-01, 3.8445e-02, 6.1505e-01],\n",
      "         [1.0337e-02, 1.3420e-01, 1.4744e-02, 8.4072e-01],\n",
      "         [5.3106e-03, 6.3227e-01, 4.4128e-02, 3.1829e-01],\n",
      "         [8.9197e-02, 6.0263e-01, 7.8449e-02, 2.2973e-01],\n",
      "         [3.0188e-03, 8.1783e-01, 4.0170e-02, 1.3898e-01],\n",
      "         [8.3473e-03, 8.5528e-01, 3.4656e-02, 1.0172e-01],\n",
      "         [1.8686e-02, 3.1656e-01, 4.3682e-02, 6.2108e-01],\n",
      "         [6.6727e-03, 2.0260e-01, 2.3357e-02, 7.6737e-01],\n",
      "         [2.1915e-03, 6.8005e-01, 1.5064e-02, 3.0269e-01],\n",
      "         [6.0418e-03, 7.0274e-01, 3.4876e-02, 2.5635e-01],\n",
      "         [2.0281e-02, 1.9085e-01, 1.9471e-02, 7.6939e-01],\n",
      "         [1.4581e-02, 5.6602e-01, 1.1420e-01, 3.0521e-01],\n",
      "         [3.8750e-02, 6.8980e-01, 2.4317e-02, 2.4714e-01],\n",
      "         [1.4816e-02, 8.4805e-01, 2.7872e-02, 1.0927e-01],\n",
      "         [1.8499e-02, 4.0381e-01, 7.6504e-02, 5.0118e-01],\n",
      "         [3.9778e-03, 3.3372e-01, 9.3781e-03, 6.5292e-01],\n",
      "         [2.1267e-03, 4.3515e-01, 2.7271e-02, 5.3545e-01],\n",
      "         [6.0058e-04, 2.0244e-02, 3.6487e-03, 9.7551e-01],\n",
      "         [6.3704e-03, 6.6858e-01, 9.2321e-02, 2.3273e-01],\n",
      "         [5.0959e-03, 5.7308e-01, 2.8089e-02, 3.9373e-01],\n",
      "         [4.6749e-03, 8.3369e-01, 1.0252e-02, 1.5139e-01],\n",
      "         [1.1372e-02, 4.5224e-01, 3.2652e-02, 5.0373e-01],\n",
      "         [1.5827e-03, 6.1181e-01, 1.8714e-02, 3.6789e-01],\n",
      "         [5.9429e-03, 1.4975e-01, 2.7651e-02, 8.1665e-01],\n",
      "         [6.1679e-03, 5.8406e-01, 3.7179e-02, 3.7259e-01],\n",
      "         [5.6810e-03, 2.7911e-01, 4.2288e-02, 6.7292e-01],\n",
      "         [1.4510e-02, 7.8382e-01, 3.6545e-02, 1.6512e-01],\n",
      "         [5.9718e-03, 6.6157e-01, 3.5689e-02, 2.9677e-01],\n",
      "         [4.4496e-03, 7.5236e-01, 4.4454e-02, 1.9874e-01],\n",
      "         [6.0624e-03, 8.1406e-01, 3.3605e-02, 1.4628e-01],\n",
      "         [1.5947e-02, 2.8738e-01, 8.8499e-02, 6.0817e-01],\n",
      "         [1.5413e-03, 1.6451e-01, 1.4569e-02, 8.1938e-01],\n",
      "         [3.8002e-03, 8.7633e-02, 1.2666e-02, 8.9590e-01],\n",
      "         [1.2085e-02, 5.8635e-01, 3.7035e-02, 3.6453e-01],\n",
      "         [1.5580e-02, 8.9894e-02, 5.7329e-02, 8.3720e-01],\n",
      "         [3.9826e-03, 7.9058e-01, 1.6257e-02, 1.8918e-01],\n",
      "         [1.0465e-02, 8.0979e-01, 4.7192e-02, 1.3255e-01],\n",
      "         [4.1354e-02, 5.8394e-01, 6.3374e-02, 3.1133e-01],\n",
      "         [5.3160e-03, 9.2382e-01, 2.4093e-02, 4.6775e-02],\n",
      "         [1.1504e-02, 8.5435e-01, 2.2478e-02, 1.1166e-01],\n",
      "         [1.1351e-02, 1.5583e-01, 8.8729e-02, 7.4409e-01],\n",
      "         [6.5935e-03, 8.0996e-01, 4.9109e-02, 1.3434e-01],\n",
      "         [4.0189e-03, 5.7598e-01, 4.6424e-02, 3.7357e-01],\n",
      "         [3.3677e-03, 8.3962e-01, 5.1668e-02, 1.0535e-01],\n",
      "         [3.8465e-03, 4.3277e-01, 4.2967e-02, 5.2042e-01],\n",
      "         [3.1272e-02, 2.2991e-01, 2.0396e-02, 7.1842e-01],\n",
      "         [2.6694e-03, 8.4125e-02, 6.6624e-03, 9.0654e-01],\n",
      "         [6.3862e-03, 1.9124e-01, 1.1035e-01, 6.9202e-01],\n",
      "         [3.7700e-03, 3.4355e-02, 7.0550e-03, 9.5482e-01],\n",
      "         [6.8487e-03, 1.5713e-01, 8.5112e-03, 8.2751e-01],\n",
      "         [3.1644e-03, 7.0512e-02, 1.5144e-02, 9.1118e-01],\n",
      "         [1.9880e-02, 8.8334e-02, 1.9289e-02, 8.7250e-01],\n",
      "         [1.7555e-02, 1.7637e-01, 2.6662e-02, 7.7941e-01],\n",
      "         [3.5531e-03, 5.3659e-01, 1.0224e-01, 3.5762e-01],\n",
      "         [3.2490e-03, 8.9948e-01, 2.8634e-02, 6.8634e-02],\n",
      "         [1.0555e-02, 6.1853e-01, 7.7026e-02, 2.9389e-01],\n",
      "         [1.4223e-02, 3.2888e-01, 2.6770e-02, 6.3012e-01],\n",
      "         [1.4269e-03, 3.9668e-02, 5.1844e-03, 9.5372e-01],\n",
      "         [8.6619e-03, 3.9059e-01, 3.0888e-02, 5.6986e-01],\n",
      "         [1.3643e-02, 4.5573e-01, 2.6770e-02, 5.0386e-01],\n",
      "         [1.0169e-02, 3.8938e-01, 5.7913e-02, 5.4254e-01],\n",
      "         [1.0996e-02, 6.7493e-01, 6.4292e-02, 2.4978e-01],\n",
      "         [1.3809e-03, 1.0009e-01, 1.8032e-02, 8.8050e-01],\n",
      "         [8.2857e-03, 5.2169e-01, 2.4401e-02, 4.4563e-01],\n",
      "         [1.4256e-02, 1.7466e-01, 1.9229e-02, 7.9185e-01],\n",
      "         [1.0224e-02, 3.4417e-01, 6.8303e-02, 5.7730e-01],\n",
      "         [9.2445e-03, 8.2399e-01, 4.6246e-02, 1.2052e-01],\n",
      "         [5.1349e-03, 5.6185e-01, 1.0329e-01, 3.2972e-01],\n",
      "         [1.1196e-03, 2.6651e-02, 1.0862e-02, 9.6137e-01],\n",
      "         [2.1981e-03, 6.0142e-02, 1.2371e-02, 9.2529e-01],\n",
      "         [3.0969e-02, 3.5690e-01, 8.3297e-02, 5.2883e-01],\n",
      "         [1.4446e-02, 7.2946e-01, 4.1687e-02, 2.1441e-01],\n",
      "         [5.1225e-03, 5.9259e-02, 1.9902e-02, 9.1572e-01],\n",
      "         [5.9796e-03, 9.6847e-02, 2.2388e-02, 8.7479e-01]]],\n",
      "       grad_fn=<StackBackward>)\n",
      "Updating critic...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V_trg.shape (after critic):  torch.Size([121])\n",
      "V_trg.shape (after sum):  torch.Size([121])\n",
      "V_trg.shape (after squeeze):  torch.Size([121])\n",
      "V_trg.shape (after squeeze):  tensor([ 0.8121,  0.5636,  0.3421,  0.1849,  0.3672,  0.4389,  0.6042,  0.6871,\n",
      "         0.9670,  0.5574,  0.4064,  0.4459,  0.2782,  0.2599,  0.3906,  0.4997,\n",
      "         0.6275,  0.2694, -0.0250,  0.6539,  0.6219,  0.0570,  0.4382,  0.5231,\n",
      "         0.3212,  0.4753,  0.4936,  0.5569,  0.6771,  0.1288,  0.3176,  0.6388,\n",
      "         0.4963,  0.6764,  0.2673,  0.1373,  0.6395,  0.4485,  0.6238,  0.2953,\n",
      "         0.4419,  0.6422,  0.2390,  0.3036,  0.5453,  0.5264,  0.8032,  0.5897,\n",
      "         0.7437,  0.6998,  0.5422,  0.7456,  0.5804,  0.5252,  0.1791,  0.4482,\n",
      "         0.2097,  0.4118,  0.5088,  0.3922,  0.9517,  0.2820,  0.3715,  0.3355,\n",
      "         0.5258,  0.7159,  0.2922,  0.4292,  0.5795,  0.1416,  0.6146,  0.2245,\n",
      "         1.0913,  0.5287,  0.7426,  0.5480,  0.1952,  0.2943,  0.2670,  0.5028,\n",
      "         0.2598,  0.2435,  0.3483,  0.5313,  0.5618,  0.3847,  0.4860,  0.4374,\n",
      "         0.4251,  0.8238,  0.6267,  0.5022,  0.3469,  0.3117,  0.4059,  0.4949,\n",
      "         0.3707,  0.2486,  0.2387,  0.5836,  0.7749,  0.9097,  0.3947,  0.3744,\n",
      "         0.6486,  0.4183,  0.7145,  0.6886,  0.3123,  0.5939,  0.7712,  0.3788,\n",
      "         0.4275,  1.2713,  1.4507,  0.5090,  0.3260,  0.6708,  0.5720,  0.7498,\n",
      "         0.5634])\n",
      "V1.shape:  torch.Size([121])\n",
      "V1:  tensor([[1.5311],\n",
      "        [1.3806],\n",
      "        [1.0408],\n",
      "        [0.4580],\n",
      "        [1.4923],\n",
      "        [1.9435],\n",
      "        [0.9643],\n",
      "        [0.9034],\n",
      "        [1.8857],\n",
      "        [2.1689],\n",
      "        [0.8579],\n",
      "        [1.5175],\n",
      "        [1.0192],\n",
      "        [0.5689],\n",
      "        [0.7376],\n",
      "        [1.0040],\n",
      "        [1.1402],\n",
      "        [0.3743],\n",
      "        [0.4143],\n",
      "        [0.7364],\n",
      "        [0.8302],\n",
      "        [2.2494],\n",
      "        [1.0914],\n",
      "        [1.0527],\n",
      "        [0.8534],\n",
      "        [0.9687],\n",
      "        [1.4295],\n",
      "        [0.9692],\n",
      "        [0.7861],\n",
      "        [1.0962],\n",
      "        [1.7253],\n",
      "        [1.4080],\n",
      "        [1.1911],\n",
      "        [0.9558],\n",
      "        [1.1620],\n",
      "        [1.4202],\n",
      "        [1.2931],\n",
      "        [1.2432],\n",
      "        [1.7354],\n",
      "        [0.7577],\n",
      "        [1.2948],\n",
      "        [0.7904],\n",
      "        [1.0856],\n",
      "        [0.7114],\n",
      "        [1.2747],\n",
      "        [1.8343],\n",
      "        [1.5624],\n",
      "        [0.4361],\n",
      "        [0.8726],\n",
      "        [0.9172],\n",
      "        [1.2786],\n",
      "        [1.0750],\n",
      "        [1.6160],\n",
      "        [1.1899],\n",
      "        [1.1269],\n",
      "        [1.2008],\n",
      "        [0.9134],\n",
      "        [1.0049],\n",
      "        [1.1932],\n",
      "        [1.1714],\n",
      "        [1.1731],\n",
      "        [1.0232],\n",
      "        [0.6314],\n",
      "        [1.4386],\n",
      "        [1.4298],\n",
      "        [0.9953],\n",
      "        [1.3373],\n",
      "        [0.5074],\n",
      "        [1.2426],\n",
      "        [1.5165],\n",
      "        [1.7145],\n",
      "        [0.7532],\n",
      "        [0.6540],\n",
      "        [0.8948],\n",
      "        [1.0813],\n",
      "        [0.8136],\n",
      "        [1.3578],\n",
      "        [0.5869],\n",
      "        [1.4084],\n",
      "        [1.7134],\n",
      "        [1.4356],\n",
      "        [1.5397],\n",
      "        [1.2897],\n",
      "        [1.1297],\n",
      "        [0.9090],\n",
      "        [0.7607],\n",
      "        [0.5432],\n",
      "        [0.8430],\n",
      "        [1.1692],\n",
      "        [0.6124],\n",
      "        [1.4484],\n",
      "        [1.5124],\n",
      "        [0.5872],\n",
      "        [1.8254],\n",
      "        [1.2012],\n",
      "        [1.6079],\n",
      "        [0.5023],\n",
      "        [0.9046],\n",
      "        [0.9832],\n",
      "        [1.4559],\n",
      "        [0.9642],\n",
      "        [1.0621],\n",
      "        [1.3132],\n",
      "        [1.4728],\n",
      "        [0.7464],\n",
      "        [1.1074],\n",
      "        [0.8758],\n",
      "        [1.3720],\n",
      "        [1.4615],\n",
      "        [0.8929],\n",
      "        [0.9780],\n",
      "        [1.0085],\n",
      "        [0.7922],\n",
      "        [1.4195],\n",
      "        [1.2501],\n",
      "        [1.6828],\n",
      "        [1.0834],\n",
      "        [1.1391],\n",
      "        [0.6237],\n",
      "        [1.3498],\n",
      "        [2.0218]], grad_fn=<AddmmBackward>)\n",
      "Updating actor...\n",
      "V_trg.shape:  torch.Size([121])\n",
      "V_trg:  tensor([0.7155, 0.8979, 0.8975, 0.5162, 0.7711, 1.3422, 0.6499, 0.8490, 0.2557,\n",
      "        0.7187, 0.8912, 0.6260, 0.7625, 0.7992, 0.6674, 0.5940, 0.5510, 0.7287,\n",
      "        0.7794, 1.1416, 0.6946, 0.1233, 0.6184, 0.7381, 0.7411, 0.6037, 0.5189,\n",
      "        0.6561, 0.8893, 0.7142, 0.8415, 0.9953, 0.7761, 0.7442, 0.9853, 0.6351,\n",
      "        0.8490, 0.5072, 0.5652, 0.2577, 0.6147, 0.9171, 0.7629, 0.5258, 0.7741,\n",
      "        0.6149, 0.7420, 0.9200, 0.8314, 0.7607, 0.5202, 0.6682, 0.5038, 0.7779,\n",
      "        0.8134, 0.6229, 0.6352, 0.9744, 0.5915, 0.8010, 0.9089, 0.8494, 0.7114,\n",
      "        0.8473, 0.8671, 0.7021, 0.4672, 0.2291, 0.7269, 0.7117, 0.6695, 0.6934,\n",
      "        0.6822, 0.3763, 0.7487, 0.5313, 1.0329, 0.3839, 1.0378, 0.9020, 0.4744,\n",
      "        0.7670, 1.0393, 0.2860, 0.9588, 0.7775, 0.8903, 0.2674, 0.9098, 0.7800,\n",
      "        0.4374, 0.4913, 0.2652, 1.0770, 0.5229, 0.6334, 1.0744, 0.6480, 0.7535,\n",
      "        0.7107, 1.3172, 0.3624, 0.8642, 1.0164, 0.3374, 0.9364, 1.5080, 0.4416,\n",
      "        1.1114, 1.1061, 1.0598, 0.5416, 0.8938, 1.2187, 0.8973, 1.1899, 1.2221,\n",
      "        0.6425, 0.9010, 0.2714, 1.1642])\n",
      "V_pred.shape:  torch.Size([121])\n",
      "V_pred:  tensor([0.8980, 0.9620, 1.0121, 0.8483, 1.5237, 0.6594, 0.4217, 1.7779, 0.4572,\n",
      "        0.2952, 0.9625, 1.4260, 1.1799, 1.0545, 0.9139, 1.7142, 1.2453, 1.2012,\n",
      "        1.5649, 0.7927, 1.3783, 0.8143, 0.4580, 0.6768, 1.6114, 1.2000, 1.8088,\n",
      "        0.8533, 1.6024, 1.4398, 0.9468, 0.9890, 0.7240, 0.9380, 1.5781, 1.4055,\n",
      "        0.9325, 1.3934, 0.8581, 1.2284, 1.1351, 1.1156, 1.5337, 1.5146, 0.4309,\n",
      "        0.6719, 1.2391, 1.4779, 0.8363, 1.2509, 0.4067, 0.8147, 1.4263, 0.8254,\n",
      "        1.1896, 0.6525, 0.4460, 1.1095, 0.8134, 1.9069, 1.2156, 1.6275, 1.5965,\n",
      "        0.5693, 1.4523, 1.3318, 1.5291, 1.1425, 1.3682, 0.5897, 0.8039, 1.5386,\n",
      "        1.2140, 1.2330, 1.1220, 1.3465, 1.3984, 0.6457, 1.1665, 0.8589, 0.7870,\n",
      "        0.6587, 1.0284, 1.6901, 0.9146, 1.3847, 0.8085, 1.2670, 1.3454, 1.2666,\n",
      "        0.7421, 1.1892, 0.9960, 1.5262, 0.7509, 1.4402, 1.3907, 0.9664, 0.8838,\n",
      "        1.7463, 0.9907, 1.0999, 1.9416, 0.6143, 1.5922, 0.9752, 0.8944, 0.6712,\n",
      "        1.5212, 0.6179, 1.5490, 0.9916, 1.1365, 0.8627, 1.0771, 1.3211, 1.3196,\n",
      "        1.0833, 1.5280, 1.2899, 0.9461])\n",
      "A.shape:  torch.Size([121])\n",
      "A:  tensor([-0.1825, -0.0641, -0.1146, -0.3321, -0.7525,  0.6829,  0.2282, -0.9288,\n",
      "        -0.2015,  0.4236, -0.0713, -0.8000, -0.4175, -0.2553, -0.2466, -1.1202,\n",
      "        -0.6943, -0.4725, -0.7854,  0.3489, -0.6837, -0.6910,  0.1605,  0.0614,\n",
      "        -0.8703, -0.5963, -1.2899, -0.1971, -0.7131, -0.7256, -0.1053,  0.0062,\n",
      "         0.0521, -0.1938, -0.5929, -0.7704, -0.0835, -0.8862, -0.2929, -0.9707,\n",
      "        -0.5204, -0.1985, -0.7708, -0.9888,  0.3432, -0.0569, -0.4970, -0.5579,\n",
      "        -0.0049, -0.4902,  0.1136, -0.1465, -0.9225, -0.0475, -0.3762, -0.0296,\n",
      "         0.1892, -0.1351, -0.2218, -1.1059, -0.3067, -0.7781, -0.8851,  0.2780,\n",
      "        -0.5852, -0.6297, -1.0619, -0.9134, -0.6413,  0.1220, -0.1343, -0.8452,\n",
      "        -0.5318, -0.8568, -0.3733, -0.8152, -0.3655, -0.2618, -0.1287,  0.0431,\n",
      "        -0.3126,  0.1082,  0.0109, -1.4041,  0.0443, -0.6072,  0.0818, -0.9996,\n",
      "        -0.4355, -0.4867, -0.3048, -0.6979, -0.7308, -0.4492, -0.2280, -0.8068,\n",
      "        -0.3162, -0.3184, -0.1302, -1.0356,  0.3265, -0.7375, -1.0774,  0.4021,\n",
      "        -1.2548, -0.0388,  0.6137, -0.2295, -0.4098,  0.4882, -0.4892, -0.4500,\n",
      "        -0.2426,  0.3560, -0.1798, -0.1312, -0.0975, -0.4408, -0.6270, -1.0185,\n",
      "         0.2181])\n",
      "policy_gradient.shape:  torch.Size([121])\n",
      "policy_gradient:  tensor([-2.4201e-01, -1.1934e-02, -6.5838e-02, -1.9484e-01, -3.3286e-02,\n",
      "         7.0442e-02,  6.6850e-02, -9.5640e-01, -3.8697e-01,  1.7368e-01,\n",
      "        -4.7573e-03, -1.0609e+00, -9.3696e-01, -1.3351e-02, -1.0319e-01,\n",
      "        -1.5249e-01, -7.3320e-02, -1.0780e-01, -1.8385e-01,  1.4417e-01,\n",
      "        -1.7917e-01, -1.3837e-01,  3.2543e-02,  1.9054e-02, -1.8410e+00,\n",
      "        -5.7144e-01, -1.4423e+00, -2.1109e-01, -8.8601e-01, -1.4755e-01,\n",
      "        -7.5859e-03,  5.4741e-04,  3.7156e-02, -1.5319e-01, -2.8810e-01,\n",
      "        -8.6814e-02, -7.2415e-03, -7.3244e-01, -9.1370e-02, -2.5596e+00,\n",
      "        -4.2471e-01, -2.0974e-01, -7.6306e-01, -3.7730e-01,  1.7206e-01,\n",
      "        -1.4327e-02, -3.7435e-01, -2.7117e-01, -8.4994e-04, -2.2472e-01,\n",
      "         5.7521e-02, -2.9468e-02, -1.4422e-01, -2.2606e-02, -9.9611e-02,\n",
      "        -3.5412e-02,  6.6735e-02, -3.5405e-02, -1.2625e-01, -1.5459e+00,\n",
      "        -5.0547e-02, -5.3750e-01, -3.7732e-01,  1.7364e-01, -1.4512e-02,\n",
      "        -9.1802e-01, -5.9117e-01, -1.6614e-01, -5.0892e-01,  5.9933e-02,\n",
      "        -2.7207e-02, -8.3446e-01, -2.1065e-01, -2.0868e-01, -1.5422e-01,\n",
      "        -2.3196e-01, -7.5195e-02, -3.2646e-01, -2.3224e-01,  4.7370e-03,\n",
      "        -1.6688e-01,  1.9234e-02,  2.5675e-03, -2.9624e-01,  2.3809e-02,\n",
      "        -4.8115e-02,  1.2876e-02, -1.8583e+00, -9.1797e-02, -2.6849e-01,\n",
      "        -5.3274e-02, -5.8454e-01, -2.4166e-01, -4.4077e-02, -8.3947e-02,\n",
      "        -3.7300e-02, -5.9874e-02, -2.9612e-02, -1.7763e-02, -2.5810e-01,\n",
      "         2.0324e-01, -7.8126e-02, -5.1758e-01,  4.4711e-01, -5.9460e-02,\n",
      "        -3.6504e-02,  4.8226e-01, -2.1648e-01, -1.1247e+00,  6.2134e-02,\n",
      "        -3.9543e-01, -1.0501e-01, -2.5881e-01,  6.8918e-02, -1.0367e-01,\n",
      "        -5.1697e-03, -7.5699e-03, -4.5418e-01, -1.9779e-01, -8.9673e-02,\n",
      "         2.9173e-02], grad_fn=<MulBackward0>)\n",
      "policy_grad:  tensor(-29.3953, grad_fn=<SumBackward0>)\n",
      "Negative entropy:  tensor(-81.7702, grad_fn=<SumBackward0>)\n",
      "Actor loss:  tensor(-30.2130, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distribution:  tensor([[0.0033, 0.8164, 0.0268, 0.1535]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0016, 0.0925, 0.0064, 0.8995]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0045, 0.5803, 0.0269, 0.3883]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0030, 0.9239, 0.0155, 0.0576]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0185, 0.7168, 0.0496, 0.2151]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0020, 0.0676, 0.0043, 0.9261]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0050, 0.2347, 0.0174, 0.7429]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0042, 0.1650, 0.0518, 0.7790]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0192, 0.7846, 0.0264, 0.1698]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0014, 0.2209, 0.0035, 0.7742]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[5.5091e-04, 5.4420e-02, 6.1773e-03, 9.3885e-01]],\n",
      "       grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0016, 0.6021, 0.0239, 0.3724]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0038, 0.7748, 0.0262, 0.1951]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0040, 0.4421, 0.0032, 0.5506]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0066, 0.7428, 0.0427, 0.2080]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0112, 0.6780, 0.1039, 0.2070]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0139, 0.3505, 0.1312, 0.5044]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0085, 0.1747, 0.0316, 0.7852]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0012, 0.0267, 0.0014, 0.9707]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0106, 0.5641, 0.0697, 0.3557]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0061, 0.3012, 0.0362, 0.6566]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0096, 0.3816, 0.0168, 0.5921]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0101, 0.5840, 0.0538, 0.3521]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0153, 0.4626, 0.0693, 0.4528]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0116, 0.2775, 0.0442, 0.6667]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0110, 0.3523, 0.0290, 0.6076]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0020, 0.1577, 0.0121, 0.8282]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0054, 0.2543, 0.0437, 0.6966]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0084, 0.2136, 0.0109, 0.7671]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0192, 0.2234, 0.0306, 0.7268]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0030, 0.1962, 0.0376, 0.7633]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0052, 0.5820, 0.0305, 0.3823]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0051, 0.1777, 0.0177, 0.7995]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0021, 0.2724, 0.0071, 0.7185]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0118, 0.5338, 0.0510, 0.4034]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0099, 0.3144, 0.0639, 0.6118]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0092, 0.8515, 0.0271, 0.1123]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0027, 0.4133, 0.0491, 0.5349]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0072, 0.2535, 0.0405, 0.6988]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0030, 0.3866, 0.0171, 0.5933]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0107, 0.2811, 0.0230, 0.6852]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0038, 0.6619, 0.0227, 0.3116]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0029, 0.1562, 0.0325, 0.8084]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0011, 0.0114, 0.0016, 0.9860]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0020, 0.0774, 0.0189, 0.9017]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0060, 0.2628, 0.0454, 0.6858]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[3.5433e-04, 2.4849e-02, 4.1510e-03, 9.7065e-01]],\n",
      "       grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0029, 0.9589, 0.0065, 0.0317]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0013, 0.0792, 0.0034, 0.9162]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0059, 0.5656, 0.0086, 0.4199]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0119, 0.8411, 0.0265, 0.1205]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0135, 0.3358, 0.0132, 0.6374]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0216, 0.4313, 0.0269, 0.5202]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0082, 0.7840, 0.0466, 0.1612]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0022, 0.0577, 0.0060, 0.9340]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0038, 0.6532, 0.0169, 0.3261]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0080, 0.2711, 0.0569, 0.6640]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0119, 0.5202, 0.0305, 0.4374]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0072, 0.0263, 0.0144, 0.9522]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0069, 0.4952, 0.0112, 0.4867]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0029, 0.2081, 0.0498, 0.7392]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0082, 0.5583, 0.0212, 0.4124]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0050, 0.3648, 0.0153, 0.6150]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0188, 0.2600, 0.0208, 0.7005]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0014, 0.8377, 0.0194, 0.1414]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0073, 0.7837, 0.0447, 0.1643]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0079, 0.7123, 0.0437, 0.2361]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0158, 0.2678, 0.0425, 0.6739]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0111, 0.4729, 0.1382, 0.3777]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0127, 0.3526, 0.0225, 0.6122]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0318, 0.1720, 0.0880, 0.7082]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0137, 0.1737, 0.0340, 0.7786]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0029, 0.0431, 0.0064, 0.9476]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0109, 0.2073, 0.0235, 0.7583]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0022, 0.5019, 0.0188, 0.4771]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0092, 0.3287, 0.0173, 0.6448]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0129, 0.1859, 0.0306, 0.7707]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0040, 0.3448, 0.0267, 0.6245]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0036, 0.0497, 0.0206, 0.9261]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0194, 0.0425, 0.0148, 0.9233]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0015, 0.1103, 0.0073, 0.8810]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0064, 0.0739, 0.0197, 0.9000]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0067, 0.1462, 0.0372, 0.8099]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0027, 0.8267, 0.0417, 0.1288]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0108, 0.1770, 0.0185, 0.7938]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0049, 0.0537, 0.0137, 0.9277]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0115, 0.1002, 0.0126, 0.8757]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0048, 0.4485, 0.0585, 0.4881]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0176, 0.3829, 0.0547, 0.5448]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0018, 0.2984, 0.0321, 0.6677]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0014, 0.9296, 0.0146, 0.0545]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0121, 0.7665, 0.0504, 0.1709]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0052, 0.0610, 0.0116, 0.9221]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0122, 0.3225, 0.0231, 0.6422]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0052, 0.3925, 0.1095, 0.4928]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0010, 0.8836, 0.0094, 0.1060]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0054, 0.7991, 0.0434, 0.1521]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0102, 0.4817, 0.0295, 0.4786]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0023, 0.1107, 0.0054, 0.8816]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0105, 0.1264, 0.0318, 0.8314]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0012, 0.1114, 0.0137, 0.8738]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0013, 0.5780, 0.0383, 0.3824]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0033, 0.1160, 0.0248, 0.8559]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0032, 0.0569, 0.0133, 0.9265]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0126, 0.4831, 0.0278, 0.4765]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0125, 0.6066, 0.1064, 0.2745]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0081, 0.6996, 0.0500, 0.2422]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0023, 0.0812, 0.0068, 0.9097]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0011, 0.0592, 0.0197, 0.9200]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0067, 0.1855, 0.0369, 0.7709]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0013, 0.0248, 0.0064, 0.9675]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0200, 0.1435, 0.0448, 0.7916]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0030, 0.6889, 0.0163, 0.2918]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0116, 0.7969, 0.0162, 0.1753]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[4.5981e-04, 5.9961e-02, 1.1990e-03, 9.3838e-01]],\n",
      "       grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0132, 0.2356, 0.0520, 0.6993]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0076, 0.2245, 0.0387, 0.7292]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0110, 0.7468, 0.0340, 0.2082]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0098, 0.4462, 0.0595, 0.4845]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0015, 0.0404, 0.0061, 0.9520]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0014, 0.0700, 0.0080, 0.9206]], grad_fn=<ExpBackward>)\n",
      "n_step_rewards.shape:  (121,)\n",
      "rewards.shape:  (121,)\n",
      "n_step_rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0.]\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0.]\n",
      "bootstrap:  [False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      "  True]\n",
      "done.shape: (before n_steps) (121,)\n",
      "done: (before n_steps) [False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False]\n",
      "done.shape: (after n_steps) (121,)\n",
      "Gamma_V.shape:  (121,)\n",
      "done: (after n_steps) [False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False]\n",
      "Gamma_V:  [0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.67572905 0.6825546\n",
      " 0.68944909 0.69641322 0.70344769 0.71055323 0.71773053 0.72498034\n",
      " 0.73230337 0.73970037 0.74717209 0.75471929 0.76234271 0.77004315\n",
      " 0.77782136 0.78567814 0.79361428 0.80163059 0.80972787 0.81790694\n",
      " 0.82616862 0.83451376 0.84294319 0.85145777 0.86005835 0.86874581\n",
      " 0.87752102 0.88638487 0.89533825 0.90438208 0.91351725 0.92274469\n",
      " 0.93206535 0.94148015 0.95099005 0.96059601 0.970299   0.9801\n",
      " 0.99      ]\n",
      "old_states.shape:  torch.Size([121, 1, 9, 9])\n",
      "new_states.shape:  torch.Size([121, 1, 9, 9])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_probs:  [tensor(-0.2028, grad_fn=<SelectBackward>), tensor(-0.1059, grad_fn=<SelectBackward>), tensor(-0.5442, grad_fn=<SelectBackward>), tensor(-0.0791, grad_fn=<SelectBackward>), tensor(-0.3330, grad_fn=<SelectBackward>), tensor(-0.0768, grad_fn=<SelectBackward>), tensor(-0.2971, grad_fn=<SelectBackward>), tensor(-1.8018, grad_fn=<SelectBackward>), tensor(-3.9526, grad_fn=<SelectBackward>), tensor(-0.2559, grad_fn=<SelectBackward>), tensor(-0.0631, grad_fn=<SelectBackward>), tensor(-0.5073, grad_fn=<SelectBackward>), tensor(-0.2551, grad_fn=<SelectBackward>), tensor(-0.8162, grad_fn=<SelectBackward>), tensor(-0.2974, grad_fn=<SelectBackward>), tensor(-2.2648, grad_fn=<SelectBackward>), tensor(-1.0483, grad_fn=<SelectBackward>), tensor(-0.2419, grad_fn=<SelectBackward>), tensor(-0.0298, grad_fn=<SelectBackward>), tensor(-0.5725, grad_fn=<SelectBackward>), tensor(-3.3193, grad_fn=<SelectBackward>), tensor(-0.5242, grad_fn=<SelectBackward>), tensor(-0.5378, grad_fn=<SelectBackward>), tensor(-0.7708, grad_fn=<SelectBackward>), tensor(-1.2819, grad_fn=<SelectBackward>), tensor(-1.0432, grad_fn=<SelectBackward>), tensor(-0.1885, grad_fn=<SelectBackward>), tensor(-0.3616, grad_fn=<SelectBackward>), tensor(-0.2651, grad_fn=<SelectBackward>), tensor(-0.3192, grad_fn=<SelectBackward>), tensor(-0.2701, grad_fn=<SelectBackward>), tensor(-0.5413, grad_fn=<SelectBackward>), tensor(-0.2238, grad_fn=<SelectBackward>), tensor(-0.3306, grad_fn=<SelectBackward>), tensor(-0.6277, grad_fn=<SelectBackward>), tensor(-0.4913, grad_fn=<SelectBackward>), tensor(-0.1608, grad_fn=<SelectBackward>), tensor(-0.6257, grad_fn=<SelectBackward>), tensor(-0.3584, grad_fn=<SelectBackward>), tensor(-0.9503, grad_fn=<SelectBackward>), tensor(-1.2691, grad_fn=<SelectBackward>), tensor(-0.4127, grad_fn=<SelectBackward>), tensor(-0.2128, grad_fn=<SelectBackward>), tensor(-0.0141, grad_fn=<SelectBackward>), tensor(-2.5587, grad_fn=<SelectBackward>), tensor(-1.3365, grad_fn=<SelectBackward>), tensor(-0.0298, grad_fn=<SelectBackward>), tensor(-0.0419, grad_fn=<SelectBackward>), tensor(-2.5359, grad_fn=<SelectBackward>), tensor(-0.5698, grad_fn=<SelectBackward>), tensor(-0.1731, grad_fn=<SelectBackward>), tensor(-0.4503, grad_fn=<SelectBackward>), tensor(-0.6535, grad_fn=<SelectBackward>), tensor(-1.8252, grad_fn=<SelectBackward>), tensor(-0.0682, grad_fn=<SelectBackward>), tensor(-1.1206, grad_fn=<SelectBackward>), tensor(-0.4095, grad_fn=<SelectBackward>), tensor(-0.8270, grad_fn=<SelectBackward>), tensor(-0.0490, grad_fn=<SelectBackward>), tensor(-0.7027, grad_fn=<SelectBackward>), tensor(-0.3022, grad_fn=<SelectBackward>), tensor(-0.5829, grad_fn=<SelectBackward>), tensor(-0.4862, grad_fn=<SelectBackward>), tensor(-1.3473, grad_fn=<SelectBackward>), tensor(-0.1771, grad_fn=<SelectBackward>), tensor(-0.2437, grad_fn=<SelectBackward>), tensor(-0.3392, grad_fn=<SelectBackward>), tensor(-0.3946, grad_fn=<SelectBackward>), tensor(-0.9736, grad_fn=<SelectBackward>), tensor(-1.0425, grad_fn=<SelectBackward>), tensor(-0.3450, grad_fn=<SelectBackward>), tensor(-0.2502, grad_fn=<SelectBackward>), tensor(-0.0538, grad_fn=<SelectBackward>), tensor(-1.5737, grad_fn=<SelectBackward>), tensor(-0.7401, grad_fn=<SelectBackward>), tensor(-0.4388, grad_fn=<SelectBackward>), tensor(-0.2605, grad_fn=<SelectBackward>), tensor(-0.4707, grad_fn=<SelectBackward>), tensor(-0.0767, grad_fn=<SelectBackward>), tensor(-0.0798, grad_fn=<SelectBackward>), tensor(-0.1267, grad_fn=<SelectBackward>), tensor(-2.6045, grad_fn=<SelectBackward>), tensor(-0.2108, grad_fn=<SelectBackward>), tensor(-0.1903, grad_fn=<SelectBackward>), tensor(-3.9892, grad_fn=<SelectBackward>), tensor(-0.0751, grad_fn=<SelectBackward>), tensor(-0.1327, grad_fn=<SelectBackward>), tensor(-0.8018, grad_fn=<SelectBackward>), tensor(-0.6074, grad_fn=<SelectBackward>), tensor(-0.4039, grad_fn=<SelectBackward>), tensor(-0.0730, grad_fn=<SelectBackward>), tensor(-1.7666, grad_fn=<SelectBackward>), tensor(-0.0811, grad_fn=<SelectBackward>), tensor(-1.1316, grad_fn=<SelectBackward>), tensor(-0.7077, grad_fn=<SelectBackward>), tensor(-0.1238, grad_fn=<SelectBackward>), tensor(-0.2242, grad_fn=<SelectBackward>), tensor(-0.7304, grad_fn=<SelectBackward>), tensor(-0.1260, grad_fn=<SelectBackward>), tensor(-0.1847, grad_fn=<SelectBackward>), tensor(-0.1349, grad_fn=<SelectBackward>), tensor(-0.9614, grad_fn=<SelectBackward>), tensor(-2.1540, grad_fn=<SelectBackward>), tensor(-0.0763, grad_fn=<SelectBackward>), tensor(-0.7414, grad_fn=<SelectBackward>), tensor(-0.4999, grad_fn=<SelectBackward>), tensor(-0.3572, grad_fn=<SelectBackward>), tensor(-0.0946, grad_fn=<SelectBackward>), tensor(-0.0834, grad_fn=<SelectBackward>), tensor(-1.6846, grad_fn=<SelectBackward>), tensor(-0.0331, grad_fn=<SelectBackward>), tensor(-3.1048, grad_fn=<SelectBackward>), tensor(-4.1140, grad_fn=<SelectBackward>), tensor(-0.2271, grad_fn=<SelectBackward>), tensor(-0.0636, grad_fn=<SelectBackward>), tensor(-0.3577, grad_fn=<SelectBackward>), tensor(-0.3158, grad_fn=<SelectBackward>), tensor(-4.5121, grad_fn=<SelectBackward>), tensor(-0.8070, grad_fn=<SelectBackward>), tensor(-0.0492, grad_fn=<SelectBackward>), tensor(-0.0827, grad_fn=<SelectBackward>)]\n",
      "log_probs:  tensor([-0.2028, -0.1059, -0.5442, -0.0791, -0.3330, -0.0768, -0.2971, -1.8018,\n",
      "        -3.9526, -0.2559, -0.0631, -0.5073, -0.2551, -0.8162, -0.2974, -2.2648,\n",
      "        -1.0483, -0.2419, -0.0298, -0.5725, -3.3193, -0.5242, -0.5378, -0.7708,\n",
      "        -1.2819, -1.0432, -0.1885, -0.3616, -0.2651, -0.3192, -0.2701, -0.5413,\n",
      "        -0.2238, -0.3306, -0.6277, -0.4913, -0.1608, -0.6257, -0.3584, -0.9503,\n",
      "        -1.2691, -0.4127, -0.2128, -0.0141, -2.5587, -1.3365, -0.0298, -0.0419,\n",
      "        -2.5359, -0.5698, -0.1731, -0.4503, -0.6535, -1.8252, -0.0682, -1.1206,\n",
      "        -0.4095, -0.8270, -0.0490, -0.7027, -0.3022, -0.5829, -0.4862, -1.3473,\n",
      "        -0.1771, -0.2437, -0.3392, -0.3946, -0.9736, -1.0425, -0.3450, -0.2502,\n",
      "        -0.0538, -1.5737, -0.7401, -0.4388, -0.2605, -0.4707, -0.0767, -0.0798,\n",
      "        -0.1267, -2.6045, -0.2108, -0.1903, -3.9892, -0.0751, -0.1327, -0.8018,\n",
      "        -0.6074, -0.4039, -0.0730, -1.7666, -0.0811, -1.1316, -0.7077, -0.1238,\n",
      "        -0.2242, -0.7304, -0.1260, -0.1847, -0.1349, -0.9614, -2.1540, -0.0763,\n",
      "        -0.7414, -0.4999, -0.3572, -0.0946, -0.0834, -1.6846, -0.0331, -3.1048,\n",
      "        -4.1140, -0.2271, -0.0636, -0.3577, -0.3158, -4.5121, -0.8070, -0.0492,\n",
      "        -0.0827], grad_fn=<StackBackward>)\n",
      "distributions.shape:  torch.Size([1, 121, 4])\n",
      "distributions:  tensor([[[3.2907e-03, 8.1641e-01, 2.6807e-02, 1.5349e-01],\n",
      "         [1.6110e-03, 9.2522e-02, 6.3748e-03, 8.9949e-01],\n",
      "         [4.5124e-03, 5.8031e-01, 2.6866e-02, 3.8831e-01],\n",
      "         [2.9643e-03, 9.2392e-01, 1.5486e-02, 5.7625e-02],\n",
      "         [1.8522e-02, 7.1680e-01, 4.9594e-02, 2.1508e-01],\n",
      "         [2.0116e-03, 6.7584e-02, 4.3223e-03, 9.2608e-01],\n",
      "         [4.9919e-03, 2.3467e-01, 1.7392e-02, 7.4294e-01],\n",
      "         [4.2021e-03, 1.6501e-01, 5.1820e-02, 7.7897e-01],\n",
      "         [1.9204e-02, 7.8457e-01, 2.6421e-02, 1.6981e-01],\n",
      "         [1.4082e-03, 2.2088e-01, 3.5031e-03, 7.7421e-01],\n",
      "         [5.5091e-04, 5.4420e-02, 6.1773e-03, 9.3885e-01],\n",
      "         [1.5845e-03, 6.0212e-01, 2.3883e-02, 3.7241e-01],\n",
      "         [3.7873e-03, 7.7484e-01, 2.6235e-02, 1.9513e-01],\n",
      "         [4.0320e-03, 4.4212e-01, 3.2185e-03, 5.5063e-01],\n",
      "         [6.5510e-03, 7.4277e-01, 4.2663e-02, 2.0801e-01],\n",
      "         [1.1207e-02, 6.7799e-01, 1.0385e-01, 2.0696e-01],\n",
      "         [1.3896e-02, 3.5054e-01, 1.3117e-01, 5.0439e-01],\n",
      "         [8.5494e-03, 1.7474e-01, 3.1555e-02, 7.8515e-01],\n",
      "         [1.2496e-03, 2.6723e-02, 1.3771e-03, 9.7065e-01],\n",
      "         [1.0552e-02, 5.6409e-01, 6.9691e-02, 3.5567e-01],\n",
      "         [6.0941e-03, 3.0117e-01, 3.6179e-02, 6.5656e-01],\n",
      "         [9.5751e-03, 3.8156e-01, 1.6812e-02, 5.9206e-01],\n",
      "         [1.0054e-02, 5.8403e-01, 5.3803e-02, 3.5211e-01],\n",
      "         [1.5276e-02, 4.6264e-01, 6.9263e-02, 4.5282e-01],\n",
      "         [1.1556e-02, 2.7752e-01, 4.4188e-02, 6.6674e-01],\n",
      "         [1.1023e-02, 3.5232e-01, 2.9040e-02, 6.0762e-01],\n",
      "         [1.9871e-03, 1.5769e-01, 1.2105e-02, 8.2822e-01],\n",
      "         [5.3912e-03, 2.5433e-01, 4.3718e-02, 6.9656e-01],\n",
      "         [8.4372e-03, 2.1356e-01, 1.0863e-02, 7.6714e-01],\n",
      "         [1.9211e-02, 2.2342e-01, 3.0615e-02, 7.2675e-01],\n",
      "         [2.9517e-03, 1.9620e-01, 3.7572e-02, 7.6327e-01],\n",
      "         [5.2495e-03, 5.8197e-01, 3.0466e-02, 3.8231e-01],\n",
      "         [5.1356e-03, 1.7766e-01, 1.7704e-02, 7.9950e-01],\n",
      "         [2.0528e-03, 2.7240e-01, 7.0888e-03, 7.1846e-01],\n",
      "         [1.1781e-02, 5.3381e-01, 5.0969e-02, 4.0344e-01],\n",
      "         [9.9302e-03, 3.1441e-01, 6.3852e-02, 6.1181e-01],\n",
      "         [9.1735e-03, 8.5148e-01, 2.7075e-02, 1.1228e-01],\n",
      "         [2.6724e-03, 4.1332e-01, 4.9107e-02, 5.3490e-01],\n",
      "         [7.1814e-03, 2.5347e-01, 4.0525e-02, 6.9883e-01],\n",
      "         [3.0206e-03, 3.8663e-01, 1.7081e-02, 5.9327e-01],\n",
      "         [1.0674e-02, 2.8107e-01, 2.3007e-02, 6.8525e-01],\n",
      "         [3.7845e-03, 6.6188e-01, 2.2701e-02, 3.1163e-01],\n",
      "         [2.9363e-03, 1.5620e-01, 3.2512e-02, 8.0836e-01],\n",
      "         [1.0783e-03, 1.1385e-02, 1.5821e-03, 9.8595e-01],\n",
      "         [1.9895e-03, 7.7408e-02, 1.8876e-02, 9.0173e-01],\n",
      "         [6.0418e-03, 2.6278e-01, 4.5387e-02, 6.8580e-01],\n",
      "         [3.5433e-04, 2.4849e-02, 4.1510e-03, 9.7065e-01],\n",
      "         [2.8569e-03, 9.5893e-01, 6.5428e-03, 3.1670e-02],\n",
      "         [1.2747e-03, 7.9187e-02, 3.3599e-03, 9.1618e-01],\n",
      "         [5.8701e-03, 5.6563e-01, 8.6365e-03, 4.1986e-01],\n",
      "         [1.1922e-02, 8.4107e-01, 2.6490e-02, 1.2051e-01],\n",
      "         [1.3547e-02, 3.3582e-01, 1.3212e-02, 6.3742e-01],\n",
      "         [2.1586e-02, 4.3127e-01, 2.6943e-02, 5.2020e-01],\n",
      "         [8.1713e-03, 7.8400e-01, 4.6649e-02, 1.6118e-01],\n",
      "         [2.2378e-03, 5.7719e-02, 6.0148e-03, 9.3403e-01],\n",
      "         [3.8162e-03, 6.5317e-01, 1.6925e-02, 3.2609e-01],\n",
      "         [8.0085e-03, 2.7113e-01, 5.6892e-02, 6.6397e-01],\n",
      "         [1.1933e-02, 5.2024e-01, 3.0451e-02, 4.3737e-01],\n",
      "         [7.1782e-03, 2.6272e-02, 1.4366e-02, 9.5218e-01],\n",
      "         [6.8750e-03, 4.9524e-01, 1.1225e-02, 4.8666e-01],\n",
      "         [2.9016e-03, 2.0812e-01, 4.9808e-02, 7.3917e-01],\n",
      "         [8.1568e-03, 5.5829e-01, 2.1189e-02, 4.1236e-01],\n",
      "         [4.9729e-03, 3.6477e-01, 1.5273e-02, 6.1498e-01],\n",
      "         [1.8784e-02, 2.5995e-01, 2.0792e-02, 7.0047e-01],\n",
      "         [1.4412e-03, 8.3773e-01, 1.9441e-02, 1.4139e-01],\n",
      "         [7.2790e-03, 7.8374e-01, 4.4660e-02, 1.6432e-01],\n",
      "         [7.8733e-03, 7.1232e-01, 4.3725e-02, 2.3609e-01],\n",
      "         [1.5797e-02, 2.6776e-01, 4.2517e-02, 6.7393e-01],\n",
      "         [1.1147e-02, 4.7291e-01, 1.3822e-01, 3.7772e-01],\n",
      "         [1.2738e-02, 3.5257e-01, 2.2490e-02, 6.1220e-01],\n",
      "         [3.1843e-02, 1.7197e-01, 8.7965e-02, 7.0822e-01],\n",
      "         [1.3678e-02, 1.7368e-01, 3.4002e-02, 7.7864e-01],\n",
      "         [2.9128e-03, 4.3136e-02, 6.3656e-03, 9.4759e-01],\n",
      "         [1.0921e-02, 2.0727e-01, 2.3543e-02, 7.5827e-01],\n",
      "         [2.1895e-03, 5.0192e-01, 1.8810e-02, 4.7708e-01],\n",
      "         [9.1956e-03, 3.2867e-01, 1.7317e-02, 6.4482e-01],\n",
      "         [1.2936e-02, 1.8586e-01, 3.0556e-02, 7.7065e-01],\n",
      "         [3.9842e-03, 3.4477e-01, 2.6712e-02, 6.2454e-01],\n",
      "         [3.6391e-03, 4.9671e-02, 2.0558e-02, 9.2613e-01],\n",
      "         [1.9450e-02, 4.2504e-02, 1.4754e-02, 9.2329e-01],\n",
      "         [1.5288e-03, 1.1025e-01, 7.2566e-03, 8.8096e-01],\n",
      "         [6.3893e-03, 7.3936e-02, 1.9716e-02, 8.9996e-01],\n",
      "         [6.7279e-03, 1.4617e-01, 3.7168e-02, 8.0994e-01],\n",
      "         [2.6612e-03, 8.2675e-01, 4.1745e-02, 1.2885e-01],\n",
      "         [1.0779e-02, 1.7695e-01, 1.8514e-02, 7.9375e-01],\n",
      "         [4.8798e-03, 5.3723e-02, 1.3717e-02, 9.2768e-01],\n",
      "         [1.1461e-02, 1.0018e-01, 1.2639e-02, 8.7572e-01],\n",
      "         [4.8075e-03, 4.4853e-01, 5.8514e-02, 4.8815e-01],\n",
      "         [1.7613e-02, 3.8290e-01, 5.4717e-02, 5.4477e-01],\n",
      "         [1.8445e-03, 2.9835e-01, 3.2088e-02, 6.6771e-01],\n",
      "         [1.3576e-03, 9.2960e-01, 1.4551e-02, 5.4488e-02],\n",
      "         [1.2126e-02, 7.6653e-01, 5.0431e-02, 1.7091e-01],\n",
      "         [5.2198e-03, 6.0994e-02, 1.1646e-02, 9.2214e-01],\n",
      "         [1.2161e-02, 3.2252e-01, 2.3111e-02, 6.4221e-01],\n",
      "         [5.2387e-03, 3.9249e-01, 1.0949e-01, 4.9279e-01],\n",
      "         [1.0488e-03, 8.8358e-01, 9.3809e-03, 1.0599e-01],\n",
      "         [5.4058e-03, 7.9913e-01, 4.3391e-02, 1.5208e-01],\n",
      "         [1.0232e-02, 4.8169e-01, 2.9477e-02, 4.7860e-01],\n",
      "         [2.2905e-03, 1.1067e-01, 5.4167e-03, 8.8162e-01],\n",
      "         [1.0475e-02, 1.2641e-01, 3.1758e-02, 8.3136e-01],\n",
      "         [1.1581e-03, 1.1140e-01, 1.3653e-02, 8.7379e-01],\n",
      "         [1.3132e-03, 5.7803e-01, 3.8294e-02, 3.8236e-01],\n",
      "         [3.2774e-03, 1.1602e-01, 2.4801e-02, 8.5591e-01],\n",
      "         [3.1902e-03, 5.6947e-02, 1.3336e-02, 9.2653e-01],\n",
      "         [1.2622e-02, 4.8315e-01, 2.7761e-02, 4.7647e-01],\n",
      "         [1.2515e-02, 6.0659e-01, 1.0643e-01, 2.7447e-01],\n",
      "         [8.1111e-03, 6.9961e-01, 5.0030e-02, 2.4225e-01],\n",
      "         [2.2939e-03, 8.1168e-02, 6.8109e-03, 9.0973e-01],\n",
      "         [1.0611e-03, 5.9246e-02, 1.9685e-02, 9.2001e-01],\n",
      "         [6.7047e-03, 1.8552e-01, 3.6899e-02, 7.7088e-01],\n",
      "         [1.3133e-03, 2.4820e-02, 6.3988e-03, 9.6747e-01],\n",
      "         [2.0013e-02, 1.4351e-01, 4.4833e-02, 7.9164e-01],\n",
      "         [2.9847e-03, 6.8891e-01, 1.6343e-02, 2.9176e-01],\n",
      "         [1.1621e-02, 7.9687e-01, 1.6179e-02, 1.7533e-01],\n",
      "         [4.5981e-04, 5.9961e-02, 1.1990e-03, 9.3838e-01],\n",
      "         [1.3189e-02, 2.3555e-01, 5.1972e-02, 6.9928e-01],\n",
      "         [7.5607e-03, 2.2451e-01, 3.8711e-02, 7.2922e-01],\n",
      "         [1.0975e-02, 7.4677e-01, 3.4014e-02, 2.0824e-01],\n",
      "         [9.7979e-03, 4.4618e-01, 5.9537e-02, 4.8449e-01],\n",
      "         [1.5067e-03, 4.0444e-02, 6.0503e-03, 9.5200e-01],\n",
      "         [1.3533e-03, 7.0031e-02, 8.0244e-03, 9.2059e-01]]],\n",
      "       grad_fn=<StackBackward>)\n",
      "Updating critic...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V_trg.shape (after critic):  torch.Size([121])\n",
      "V_trg.shape (after sum):  torch.Size([121])\n",
      "V_trg.shape (after squeeze):  torch.Size([121])\n",
      "V_trg.shape (after squeeze):  tensor([0.5000, 0.2771, 0.3944, 0.5946, 0.4264, 0.4868, 0.3649, 0.5346, 0.5763,\n",
      "        0.8304, 0.2899, 0.7460, 0.2952, 0.2950, 0.5990, 0.7806, 0.3564, 0.2777,\n",
      "        0.6107, 0.5846, 0.4284, 0.3086, 0.3097, 0.5231, 0.5183, 0.7221, 0.3971,\n",
      "        0.7014, 0.9777, 0.7623, 0.2563, 0.5925, 0.5499, 0.6113, 0.4268, 0.6667,\n",
      "        1.0365, 0.5068, 0.3890, 0.5275, 0.3169, 0.6652, 0.7204, 0.6528, 0.8387,\n",
      "        0.4201, 0.6545, 0.4366, 0.5177, 0.1632, 0.2375, 0.8678, 0.5186, 0.5357,\n",
      "        0.7564, 0.7287, 0.3964, 0.5921, 0.5756, 0.3483, 0.3151, 0.2019, 0.5097,\n",
      "        0.9189, 0.4352, 0.5530, 0.3486, 0.6507, 0.9032, 0.1763, 0.6912, 0.5733,\n",
      "        0.1352, 0.5644, 0.6572, 0.4594, 0.3922, 0.7986, 0.5901, 0.5588, 0.6101,\n",
      "        0.5522, 0.5604, 0.0293, 0.4523, 0.5801, 0.6463, 0.9080, 0.6175, 1.0564,\n",
      "        0.8697, 0.6658, 0.4335, 0.4876, 0.7254, 0.9293, 0.8028, 0.9046, 0.7430,\n",
      "        0.3019, 0.2501, 0.4676, 0.1905, 0.2349, 0.0268, 1.1101, 0.5771, 0.5612,\n",
      "        0.7108, 0.8006, 0.3825, 0.7866, 0.5041, 0.5538, 1.0370, 0.2676, 1.0545,\n",
      "        0.8191, 1.0770, 1.2104, 0.2620])\n",
      "V1.shape:  torch.Size([121])\n",
      "V1:  tensor([[1.4396],\n",
      "        [1.3032],\n",
      "        [1.2607],\n",
      "        [0.9008],\n",
      "        [2.3287],\n",
      "        [1.3976],\n",
      "        [1.6101],\n",
      "        [1.1605],\n",
      "        [1.2875],\n",
      "        [1.5143],\n",
      "        [1.1960],\n",
      "        [1.0548],\n",
      "        [0.9686],\n",
      "        [1.1879],\n",
      "        [2.0952],\n",
      "        [1.5111],\n",
      "        [1.4286],\n",
      "        [1.4192],\n",
      "        [1.6752],\n",
      "        [1.9170],\n",
      "        [1.0166],\n",
      "        [1.2626],\n",
      "        [1.5973],\n",
      "        [1.0259],\n",
      "        [1.4234],\n",
      "        [1.6474],\n",
      "        [1.0771],\n",
      "        [0.8507],\n",
      "        [1.4274],\n",
      "        [1.2574],\n",
      "        [0.9688],\n",
      "        [1.2406],\n",
      "        [1.7962],\n",
      "        [1.3586],\n",
      "        [1.8788],\n",
      "        [1.5193],\n",
      "        [1.7288],\n",
      "        [0.8014],\n",
      "        [1.9434],\n",
      "        [1.2062],\n",
      "        [1.8077],\n",
      "        [1.6955],\n",
      "        [1.2111],\n",
      "        [1.7985],\n",
      "        [1.0162],\n",
      "        [1.5040],\n",
      "        [1.1000],\n",
      "        [1.7522],\n",
      "        [1.8240],\n",
      "        [1.2031],\n",
      "        [1.3506],\n",
      "        [1.3123],\n",
      "        [1.7138],\n",
      "        [1.2658],\n",
      "        [1.2623],\n",
      "        [1.6317],\n",
      "        [0.6966],\n",
      "        [0.9826],\n",
      "        [1.5787],\n",
      "        [1.5815],\n",
      "        [1.4684],\n",
      "        [1.6905],\n",
      "        [1.7271],\n",
      "        [0.6954],\n",
      "        [1.0106],\n",
      "        [1.7990],\n",
      "        [0.9259],\n",
      "        [1.5333],\n",
      "        [1.0359],\n",
      "        [2.1338],\n",
      "        [0.7939],\n",
      "        [1.0182],\n",
      "        [1.6969],\n",
      "        [1.0748],\n",
      "        [0.8569],\n",
      "        [0.8518],\n",
      "        [0.2299],\n",
      "        [0.9787],\n",
      "        [1.0789],\n",
      "        [1.1674],\n",
      "        [1.5289],\n",
      "        [1.5042],\n",
      "        [1.6341],\n",
      "        [1.6033],\n",
      "        [1.4603],\n",
      "        [0.9938],\n",
      "        [0.7741],\n",
      "        [0.8355],\n",
      "        [2.3056],\n",
      "        [1.4611],\n",
      "        [1.4154],\n",
      "        [1.3569],\n",
      "        [2.1757],\n",
      "        [2.0466],\n",
      "        [0.9875],\n",
      "        [1.2071],\n",
      "        [0.9918],\n",
      "        [0.8378],\n",
      "        [1.7738],\n",
      "        [1.3662],\n",
      "        [1.1636],\n",
      "        [0.9193],\n",
      "        [2.2306],\n",
      "        [1.4305],\n",
      "        [1.0264],\n",
      "        [1.5962],\n",
      "        [1.6380],\n",
      "        [1.3949],\n",
      "        [1.3924],\n",
      "        [1.7220],\n",
      "        [1.3404],\n",
      "        [1.3036],\n",
      "        [0.8044],\n",
      "        [1.2467],\n",
      "        [1.9755],\n",
      "        [2.1330],\n",
      "        [1.0266],\n",
      "        [1.8764],\n",
      "        [1.8529],\n",
      "        [1.6316],\n",
      "        [2.6305]], grad_fn=<AddmmBackward>)\n",
      "Updating actor...\n",
      "V_trg.shape:  torch.Size([121])\n",
      "V_trg:  tensor([0.7311, 0.9239, 0.8430, 0.9612, 0.7621, 0.7334, 0.5657, 0.3806, 0.7017,\n",
      "        0.7749, 0.9612, 1.0667, 0.6566, 1.0828, 0.5171, 0.6657, 1.1991, 0.3440,\n",
      "        0.6616, 0.7241, 0.7214, 1.0046, 1.1755, 1.0350, 0.7666, 0.8750, 1.1565,\n",
      "        0.7373, 0.8372, 0.3130, 0.6327, 0.8292, 0.6783, 0.8308, 0.1839, 0.5257,\n",
      "        0.8213, 0.6557, 0.5253, 1.1366, 0.8947, 0.9296, 0.9272, 0.9477, 0.6580,\n",
      "        0.6025, 0.7663, 0.6818, 0.8348, 0.4782, 1.1939, 1.2015, 0.8147, 0.5420,\n",
      "        0.9928, 0.7591, 0.9276, 0.7621, 0.9546, 0.8141, 0.8292, 0.5799, 0.6198,\n",
      "        0.5210, 0.7168, 0.8305, 0.5908, 0.7472, 0.5618, 1.3565, 0.4706, 0.8566,\n",
      "        0.7121, 0.4875, 1.0082, 0.9129, 0.7611, 1.1298, 0.4069, 1.1058, 0.3896,\n",
      "        0.7104, 0.7052, 1.1440, 0.9203, 0.5567, 1.0079, 1.2605, 0.2975, 0.9319,\n",
      "        0.8422, 1.0579, 1.1267, 0.6043, 0.9695, 1.1076, 0.5295, 0.8238, 0.5205,\n",
      "        0.6494, 0.6566, 1.2860, 0.6254, 0.8834, 0.4533, 0.8966, 1.2822, 1.1960,\n",
      "        1.3052, 1.4104, 1.0407, 1.2733, 1.5111, 0.9544, 1.4031, 0.3570, 0.9923,\n",
      "        1.1314, 0.7894, 0.9500, 1.4225])\n",
      "V_pred.shape:  torch.Size([121])\n",
      "V_pred:  tensor([0.5359, 1.2404, 1.2483, 0.9736, 1.0772, 1.2505, 1.1162, 0.5125, 1.0077,\n",
      "        1.2542, 1.2812, 1.2935, 0.7817, 1.7269, 0.9780, 1.2723, 0.6440, 0.8912,\n",
      "        1.6160, 1.4016, 1.7987, 1.6751, 0.9383, 1.5962, 0.8849, 1.2671, 0.4830,\n",
      "        1.3831, 0.6864, 0.7567, 1.3261, 1.4544, 1.1627, 1.0766, 0.2401, 1.2463,\n",
      "        1.2058, 1.0438, 1.5144, 0.4244, 1.1518, 1.2673, 1.7762, 1.1262, 1.2408,\n",
      "        1.5063, 1.4174, 0.8223, 1.1725, 1.2432, 1.1107, 1.2298, 1.2528, 0.5952,\n",
      "        1.4368, 1.5129, 0.8904, 1.3344, 1.1268, 1.1511, 1.2423, 0.9147, 1.8196,\n",
      "        0.9351, 1.1770, 1.4812, 0.4636, 1.0787, 0.5172, 0.7855, 1.2734, 1.1604,\n",
      "        0.9167, 1.5832, 1.6573, 0.8301, 1.0039, 1.0909, 1.0472, 1.1079, 1.3369,\n",
      "        1.3801, 1.6622, 1.5147, 1.2364, 1.0803, 0.9747, 1.0828, 1.3674, 0.8423,\n",
      "        0.5732, 1.0547, 1.0668, 2.0431, 1.2646, 1.0520, 0.9726, 0.8041, 1.5109,\n",
      "        1.3948, 1.3043, 0.8510, 1.9872, 0.8933, 1.3872, 1.2602, 1.2157, 0.5497,\n",
      "        1.0515, 1.2633, 1.8584, 1.2586, 0.9560, 1.1237, 0.9105, 0.3893, 0.2638,\n",
      "        0.9907, 1.1059, 0.5983, 1.7505])\n",
      "A.shape:  torch.Size([121])\n",
      "A:  tensor([ 0.1952, -0.3164, -0.4053, -0.0125, -0.3152, -0.5172, -0.5504, -0.1318,\n",
      "        -0.3060, -0.4792, -0.3200, -0.2268, -0.1252, -0.6440, -0.4610, -0.6066,\n",
      "         0.5551, -0.5472, -0.9545, -0.6776, -1.0773, -0.6705,  0.2371, -0.5612,\n",
      "        -0.1183, -0.3921,  0.6735, -0.6458,  0.1508, -0.4437, -0.6933, -0.6251,\n",
      "        -0.4844, -0.2458, -0.0562, -0.7206, -0.3845, -0.3881, -0.9891,  0.7122,\n",
      "        -0.2571, -0.3377, -0.8490, -0.1785, -0.5828, -0.9038, -0.6511, -0.1405,\n",
      "        -0.3377, -0.7650,  0.0832, -0.0282, -0.4381, -0.0532, -0.4440, -0.7538,\n",
      "         0.0372, -0.5724, -0.1722, -0.3369, -0.4131, -0.3348, -1.1998, -0.4141,\n",
      "        -0.4601, -0.6507,  0.1272, -0.3314,  0.0446,  0.5710, -0.8027, -0.3038,\n",
      "        -0.2046, -1.0957, -0.6491,  0.0828, -0.2428,  0.0390, -0.6402, -0.0022,\n",
      "        -0.9472, -0.6696, -0.9570, -0.3707, -0.3162, -0.5236,  0.0332,  0.1777,\n",
      "        -1.0699,  0.0896,  0.2690,  0.0033,  0.0599, -1.4388, -0.2951,  0.0556,\n",
      "        -0.4431,  0.0197, -0.9904, -0.7454, -0.6477,  0.4350, -1.3618, -0.0099,\n",
      "        -0.9339, -0.3637,  0.0665,  0.6462,  0.2537,  0.1472, -0.8177,  0.0147,\n",
      "         0.5551, -0.1693,  0.4926, -0.0324,  0.7286,  0.1408, -0.3165,  0.3517,\n",
      "        -0.3280])\n",
      "policy_gradient.shape:  torch.Size([121])\n",
      "policy_gradient:  tensor([ 3.9591e-02, -3.3519e-02, -2.2054e-01, -9.8695e-04, -1.0494e-01,\n",
      "        -3.9716e-02, -1.6355e-01, -2.3750e-01, -1.2094e+00, -1.2264e-01,\n",
      "        -2.0190e-02, -1.1507e-01, -3.1929e-02, -5.2563e-01, -1.3707e-01,\n",
      "        -1.3738e+00,  5.8190e-01, -1.3235e-01, -2.8432e-02, -3.8795e-01,\n",
      "        -3.5758e+00, -3.5145e-01,  1.2753e-01, -4.3256e-01, -1.5169e-01,\n",
      "        -4.0906e-01,  1.2694e-01, -2.3353e-01,  3.9966e-02, -1.4161e-01,\n",
      "        -1.8729e-01, -3.3839e-01, -1.0839e-01, -8.1264e-02, -3.5253e-02,\n",
      "        -3.5405e-01, -6.1814e-02, -2.4283e-01, -3.5444e-01,  6.7676e-01,\n",
      "        -3.2630e-01, -1.3935e-01, -1.8064e-01, -2.5250e-03, -1.4911e+00,\n",
      "        -1.2078e+00, -1.9398e-02, -5.8917e-03, -8.5642e-01, -4.3591e-01,\n",
      "         1.4402e-02, -1.2708e-02, -2.8634e-01, -9.7047e-02, -3.0304e-02,\n",
      "        -8.4469e-01,  1.5232e-02, -4.7335e-01, -8.4375e-03, -2.3677e-01,\n",
      "        -1.2486e-01, -1.9515e-01, -5.8328e-01, -5.5794e-01, -8.1470e-02,\n",
      "        -1.5856e-01,  4.3136e-02, -1.3080e-01,  4.3429e-02,  5.9526e-01,\n",
      "        -2.7695e-01, -7.6018e-02, -1.1014e-02, -1.7243e+00, -4.8037e-01,\n",
      "         3.6327e-02, -6.3258e-02,  1.8351e-02, -4.9130e-02, -1.7316e-04,\n",
      "        -1.2005e-01, -1.7441e+00, -2.0174e-01, -7.0522e-02, -1.2612e+00,\n",
      "        -3.9307e-02,  4.4078e-03,  1.4248e-01, -6.4985e-01,  3.6209e-02,\n",
      "         1.9636e-02,  5.7739e-03,  4.8561e-03, -1.6282e+00, -2.0883e-01,\n",
      "         6.8775e-03, -9.9357e-02,  1.4399e-02, -1.2478e-01, -1.3767e-01,\n",
      "        -8.7386e-02,  4.1822e-01, -2.9333e+00, -7.5639e-04, -6.9236e-01,\n",
      "        -1.8179e-01,  2.3763e-02,  6.1141e-02,  2.1151e-02,  2.4794e-01,\n",
      "        -2.7044e-02,  4.5746e-02,  2.2838e+00, -3.8450e-02,  3.1327e-02,\n",
      "        -1.1578e-02,  2.3007e-01,  6.3518e-01, -2.5546e-01,  1.7302e-02,\n",
      "        -2.7141e-02], grad_fn=<MulBackward0>)\n",
      "policy_grad:  tensor(-27.0446, grad_fn=<SumBackward0>)\n",
      "Negative entropy:  tensor(-76.5911, grad_fn=<SumBackward0>)\n",
      "Actor loss:  tensor(-27.8105, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distribution:  tensor([[0.0146, 0.7320, 0.0108, 0.2426]], grad_fn=<ExpBackward>)\n",
      "n_step_rewards.shape:  (1,)\n",
      "rewards.shape:  (1,)\n",
      "n_step_rewards:  [10.]\n",
      "rewards:  [10.]\n",
      "bootstrap:  [False]\n",
      "done.shape: (before n_steps) (1,)\n",
      "done: (before n_steps) [ True]\n",
      "done.shape: (after n_steps) (1,)\n",
      "Gamma_V.shape:  (1,)\n",
      "done: (after n_steps) [ True]\n",
      "Gamma_V:  [0.99]\n",
      "old_states.shape:  torch.Size([1, 1, 9, 9])\n",
      "new_states.shape:  torch.Size([1, 1, 9, 9])\n",
      "log_probs:  [tensor(-1.4163, grad_fn=<SelectBackward>)]\n",
      "log_probs:  tensor([-1.4163], grad_fn=<StackBackward>)\n",
      "distributions.shape:  torch.Size([1, 1, 4])\n",
      "distributions:  tensor([[[0.0146, 0.7320, 0.0108, 0.2426]]], grad_fn=<StackBackward>)\n",
      "Updating critic...\n",
      "V_trg.shape (after critic):  torch.Size([])\n",
      "V_trg.shape (after sum):  torch.Size([1])\n",
      "V_trg.shape (after squeeze):  torch.Size([])\n",
      "V_trg.shape (after squeeze):  tensor(10.)\n",
      "V1.shape:  torch.Size([])\n",
      "V1:  tensor([[1.8007]], grad_fn=<AddmmBackward>)\n",
      "Updating actor...\n",
      "V_trg.shape:  torch.Size([1])\n",
      "V_trg:  tensor([10.])\n",
      "V_pred.shape:  torch.Size([])\n",
      "V_pred:  tensor(1.4556)\n",
      "A.shape:  torch.Size([1])\n",
      "A:  tensor([8.5444])\n",
      "policy_gradient.shape:  torch.Size([1])\n",
      "policy_gradient:  tensor([12.1013], grad_fn=<MulBackward0>)\n",
      "policy_grad:  tensor(12.1013, grad_fn=<SumBackward0>)\n",
      "Negative entropy:  tensor(-0.6826, grad_fn=<SumBackward0>)\n",
      "Actor loss:  tensor(12.0944, grad_fn=<AddBackward0>)\n",
      "distribution:  tensor([[0.0094, 0.3672, 0.0207, 0.6027]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0030, 0.0783, 0.0072, 0.9115]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0067, 0.2231, 0.0221, 0.7482]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0076, 0.5547, 0.0192, 0.4185]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0032, 0.8492, 0.0196, 0.1280]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0085, 0.7206, 0.0410, 0.2298]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[4.1970e-04, 7.6851e-03, 6.0733e-04, 9.9129e-01]],\n",
      "       grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0031, 0.7378, 0.0130, 0.2460]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0016, 0.1171, 0.0103, 0.8710]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0031, 0.8664, 0.0157, 0.1148]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0120, 0.1952, 0.0239, 0.7690]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0016, 0.0416, 0.0139, 0.9429]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[4.9696e-04, 5.6252e-02, 4.8500e-03, 9.3840e-01]],\n",
      "       grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0081, 0.3848, 0.0052, 0.6019]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0188, 0.3430, 0.0747, 0.5636]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0141, 0.2159, 0.0175, 0.7526]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[2.9453e-04, 1.4130e-02, 6.9033e-04, 9.8489e-01]],\n",
      "       grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0029, 0.0673, 0.0057, 0.9241]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0151, 0.0542, 0.0197, 0.9110]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0017, 0.7931, 0.0267, 0.1785]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0297, 0.4080, 0.0293, 0.5330]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0025, 0.0925, 0.0074, 0.8976]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0029, 0.7438, 0.0137, 0.2396]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0200, 0.4511, 0.0369, 0.4920]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[5.2232e-04, 1.3751e-02, 1.0634e-03, 9.8466e-01]],\n",
      "       grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0069, 0.1695, 0.0157, 0.8079]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0012, 0.0662, 0.0216, 0.9110]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[4.1701e-04, 3.6933e-02, 1.1802e-03, 9.6147e-01]],\n",
      "       grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0147, 0.1466, 0.0366, 0.8021]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0023, 0.1354, 0.0152, 0.8471]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0039, 0.0632, 0.0193, 0.9136]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0050, 0.0626, 0.0095, 0.9229]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0027, 0.0727, 0.0072, 0.9173]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0063, 0.8437, 0.0212, 0.1288]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0081, 0.0982, 0.0196, 0.8741]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[7.2704e-04, 2.5034e-02, 3.0996e-03, 9.7114e-01]],\n",
      "       grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0019, 0.1172, 0.0285, 0.8524]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0015, 0.0353, 0.0045, 0.9588]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0012, 0.0273, 0.0031, 0.9684]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[1.5863e-04, 2.3519e-02, 1.4648e-03, 9.7486e-01]],\n",
      "       grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0045, 0.1089, 0.0038, 0.8827]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[6.7280e-04, 3.8278e-02, 7.4238e-04, 9.6031e-01]],\n",
      "       grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0278, 0.5551, 0.0197, 0.3974]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0018, 0.0222, 0.0037, 0.9722]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0029, 0.3407, 0.0339, 0.6225]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[8.3023e-04, 5.2969e-02, 6.0391e-03, 9.4016e-01]],\n",
      "       grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0011, 0.0358, 0.0054, 0.9576]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0095, 0.6196, 0.0398, 0.3311]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0064, 0.5764, 0.0160, 0.4012]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0107, 0.5652, 0.0215, 0.4026]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0053, 0.5573, 0.0310, 0.4064]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0098, 0.2634, 0.0235, 0.7033]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0202, 0.1589, 0.0510, 0.7698]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0227, 0.2512, 0.0252, 0.7009]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0017, 0.0198, 0.0017, 0.9768]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0056, 0.0912, 0.0296, 0.8735]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0048, 0.6179, 0.0559, 0.3214]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0038, 0.8664, 0.0195, 0.1103]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0124, 0.1541, 0.0322, 0.8013]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0034, 0.3174, 0.0054, 0.6738]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[3.7512e-04, 4.3966e-03, 2.8559e-03, 9.9237e-01]],\n",
      "       grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0014, 0.0868, 0.0082, 0.9036]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0026, 0.1321, 0.0131, 0.8522]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0017, 0.0392, 0.0109, 0.9483]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0012, 0.1610, 0.0185, 0.8193]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0010, 0.0878, 0.0018, 0.9094]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[2.0295e-04, 3.4128e-02, 1.4780e-03, 9.6419e-01]],\n",
      "       grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0078, 0.3238, 0.0575, 0.6108]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0032, 0.8165, 0.0189, 0.1614]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0042, 0.0531, 0.0167, 0.9259]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0010, 0.0389, 0.0108, 0.9493]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0018, 0.0276, 0.0039, 0.9666]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0063, 0.2150, 0.0481, 0.7305]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0028, 0.1382, 0.0019, 0.8572]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0044, 0.4135, 0.0192, 0.5629]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[2.8624e-04, 1.2746e-02, 1.2117e-03, 9.8576e-01]],\n",
      "       grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0030, 0.5439, 0.0138, 0.4393]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[2.1755e-04, 5.4819e-03, 1.5382e-03, 9.9276e-01]],\n",
      "       grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0093, 0.7033, 0.0591, 0.2284]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0007, 0.4308, 0.0102, 0.5582]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0024, 0.8284, 0.0241, 0.1451]], grad_fn=<ExpBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distribution:  tensor([[1.8122e-04, 1.7409e-02, 5.0551e-04, 9.8190e-01]],\n",
      "       grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0034, 0.1182, 0.0083, 0.8701]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0131, 0.1874, 0.0288, 0.7707]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0101, 0.3975, 0.0444, 0.5480]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0048, 0.4196, 0.0234, 0.5522]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0026, 0.2064, 0.0029, 0.7881]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0011, 0.2890, 0.0038, 0.7061]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0016, 0.1323, 0.0151, 0.8509]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0029, 0.3908, 0.0658, 0.5406]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0196, 0.1509, 0.0434, 0.7861]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0018, 0.5808, 0.0219, 0.3954]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0048, 0.8546, 0.0117, 0.1290]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[3.1616e-04, 5.1973e-03, 3.2537e-04, 9.9416e-01]],\n",
      "       grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[4.4617e-04, 1.3923e-01, 4.8123e-03, 8.5551e-01]],\n",
      "       grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0059, 0.1877, 0.0343, 0.7722]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0048, 0.2779, 0.0284, 0.6888]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0068, 0.2952, 0.0273, 0.6707]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0026, 0.4424, 0.0159, 0.5391]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0028, 0.0460, 0.0169, 0.9344]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0051, 0.7967, 0.0203, 0.1779]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0020, 0.1762, 0.0040, 0.8179]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0061, 0.4120, 0.0568, 0.5252]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0022, 0.1301, 0.0071, 0.8606]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0013, 0.0451, 0.0036, 0.9501]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0115, 0.1055, 0.0967, 0.7862]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0039, 0.1258, 0.0355, 0.8348]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0024, 0.0830, 0.0086, 0.9060]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0037, 0.7959, 0.0320, 0.1683]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0023, 0.0372, 0.0065, 0.9540]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0068, 0.1330, 0.0228, 0.8374]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0038, 0.1178, 0.0082, 0.8703]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0018, 0.7225, 0.0376, 0.2381]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0134, 0.5017, 0.0436, 0.4412]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0050, 0.4054, 0.0078, 0.5819]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0062, 0.1372, 0.0046, 0.8519]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0068, 0.1068, 0.0160, 0.8704]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0018, 0.1586, 0.0040, 0.8357]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0024, 0.0808, 0.0042, 0.9126]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0082, 0.2929, 0.0504, 0.6485]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[9.2490e-04, 1.3401e-02, 2.2247e-03, 9.8345e-01]],\n",
      "       grad_fn=<ExpBackward>)\n",
      "n_step_rewards.shape:  (121,)\n",
      "rewards.shape:  (121,)\n",
      "n_step_rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0.]\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0.]\n",
      "bootstrap:  [False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      "  True]\n",
      "done.shape: (before n_steps) (121,)\n",
      "done: (before n_steps) [False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False]\n",
      "done.shape: (after n_steps) (121,)\n",
      "Gamma_V.shape:  (121,)\n",
      "done: (after n_steps) [False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False]\n",
      "Gamma_V:  [0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.67572905 0.6825546\n",
      " 0.68944909 0.69641322 0.70344769 0.71055323 0.71773053 0.72498034\n",
      " 0.73230337 0.73970037 0.74717209 0.75471929 0.76234271 0.77004315\n",
      " 0.77782136 0.78567814 0.79361428 0.80163059 0.80972787 0.81790694\n",
      " 0.82616862 0.83451376 0.84294319 0.85145777 0.86005835 0.86874581\n",
      " 0.87752102 0.88638487 0.89533825 0.90438208 0.91351725 0.92274469\n",
      " 0.93206535 0.94148015 0.95099005 0.96059601 0.970299   0.9801\n",
      " 0.99      ]\n",
      "old_states.shape:  torch.Size([121, 1, 9, 9])\n",
      "new_states.shape:  torch.Size([121, 1, 9, 9])\n",
      "log_probs:  [tensor(-4.6669, grad_fn=<SelectBackward>), tensor(-2.5470, grad_fn=<SelectBackward>), tensor(-0.2901, grad_fn=<SelectBackward>), tensor(-0.5894, grad_fn=<SelectBackward>), tensor(-2.0561, grad_fn=<SelectBackward>), tensor(-3.1941, grad_fn=<SelectBackward>), tensor(-0.0088, grad_fn=<SelectBackward>), tensor(-0.3040, grad_fn=<SelectBackward>), tensor(-0.1381, grad_fn=<SelectBackward>), tensor(-0.1435, grad_fn=<SelectBackward>), tensor(-0.2627, grad_fn=<SelectBackward>), tensor(-0.0588, grad_fn=<SelectBackward>), tensor(-0.0636, grad_fn=<SelectBackward>), tensor(-0.9550, grad_fn=<SelectBackward>), tensor(-0.5735, grad_fn=<SelectBackward>), tensor(-1.5332, grad_fn=<SelectBackward>), tensor(-0.0152, grad_fn=<SelectBackward>), tensor(-0.0789, grad_fn=<SelectBackward>), tensor(-0.0932, grad_fn=<SelectBackward>), tensor(-0.2318, grad_fn=<SelectBackward>), tensor(-0.6293, grad_fn=<SelectBackward>), tensor(-0.1080, grad_fn=<SelectBackward>), tensor(-0.2960, grad_fn=<SelectBackward>), tensor(-0.7092, grad_fn=<SelectBackward>), tensor(-0.0155, grad_fn=<SelectBackward>), tensor(-1.7748, grad_fn=<SelectBackward>), tensor(-2.7146, grad_fn=<SelectBackward>), tensor(-0.0393, grad_fn=<SelectBackward>), tensor(-0.2205, grad_fn=<SelectBackward>), tensor(-0.1660, grad_fn=<SelectBackward>), tensor(-0.0903, grad_fn=<SelectBackward>), tensor(-0.0803, grad_fn=<SelectBackward>), tensor(-0.0863, grad_fn=<SelectBackward>), tensor(-0.1700, grad_fn=<SelectBackward>), tensor(-0.1345, grad_fn=<SelectBackward>), tensor(-0.0293, grad_fn=<SelectBackward>), tensor(-0.1597, grad_fn=<SelectBackward>), tensor(-0.0421, grad_fn=<SelectBackward>), tensor(-0.0321, grad_fn=<SelectBackward>), tensor(-0.0255, grad_fn=<SelectBackward>), tensor(-0.1248, grad_fn=<SelectBackward>), tensor(-0.0405, grad_fn=<SelectBackward>), tensor(-0.5886, grad_fn=<SelectBackward>), tensor(-0.0282, grad_fn=<SelectBackward>), tensor(-0.4740, grad_fn=<SelectBackward>), tensor(-0.0617, grad_fn=<SelectBackward>), tensor(-0.0433, grad_fn=<SelectBackward>), tensor(-0.4787, grad_fn=<SelectBackward>), tensor(-0.9133, grad_fn=<SelectBackward>), tensor(-0.5705, grad_fn=<SelectBackward>), tensor(-0.5847, grad_fn=<SelectBackward>), tensor(-0.3520, grad_fn=<SelectBackward>), tensor(-0.2616, grad_fn=<SelectBackward>), tensor(-0.3554, grad_fn=<SelectBackward>), tensor(-0.0235, grad_fn=<SelectBackward>), tensor(-0.1352, grad_fn=<SelectBackward>), tensor(-1.1350, grad_fn=<SelectBackward>), tensor(-0.1434, grad_fn=<SelectBackward>), tensor(-0.2216, grad_fn=<SelectBackward>), tensor(-0.3948, grad_fn=<SelectBackward>), tensor(-0.0077, grad_fn=<SelectBackward>), tensor(-0.1014, grad_fn=<SelectBackward>), tensor(-0.1600, grad_fn=<SelectBackward>), tensor(-0.0531, grad_fn=<SelectBackward>), tensor(-0.1993, grad_fn=<SelectBackward>), tensor(-0.0950, grad_fn=<SelectBackward>), tensor(-0.0365, grad_fn=<SelectBackward>), tensor(-0.4930, grad_fn=<SelectBackward>), tensor(-0.2027, grad_fn=<SelectBackward>), tensor(-0.0770, grad_fn=<SelectBackward>), tensor(-0.0520, grad_fn=<SelectBackward>), tensor(-0.0339, grad_fn=<SelectBackward>), tensor(-0.3140, grad_fn=<SelectBackward>), tensor(-1.9794, grad_fn=<SelectBackward>), tensor(-0.8831, grad_fn=<SelectBackward>), tensor(-0.0143, grad_fn=<SelectBackward>), tensor(-0.8226, grad_fn=<SelectBackward>), tensor(-0.0073, grad_fn=<SelectBackward>), tensor(-0.3520, grad_fn=<SelectBackward>), tensor(-0.8421, grad_fn=<SelectBackward>), tensor(-0.1882, grad_fn=<SelectBackward>), tensor(-0.0183, grad_fn=<SelectBackward>), tensor(-0.1392, grad_fn=<SelectBackward>), tensor(-0.2605, grad_fn=<SelectBackward>), tensor(-0.9226, grad_fn=<SelectBackward>), tensor(-0.5939, grad_fn=<SelectBackward>), tensor(-0.2382, grad_fn=<SelectBackward>), tensor(-0.3480, grad_fn=<SelectBackward>), tensor(-0.1614, grad_fn=<SelectBackward>), tensor(-0.9395, grad_fn=<SelectBackward>), tensor(-0.2407, grad_fn=<SelectBackward>), tensor(-0.9279, grad_fn=<SelectBackward>), tensor(-0.1572, grad_fn=<SelectBackward>), tensor(-0.0059, grad_fn=<SelectBackward>), tensor(-1.9716, grad_fn=<SelectBackward>), tensor(-0.2585, grad_fn=<SelectBackward>), tensor(-1.2804, grad_fn=<SelectBackward>), tensor(-0.3994, grad_fn=<SelectBackward>), tensor(-0.8155, grad_fn=<SelectBackward>), tensor(-0.0679, grad_fn=<SelectBackward>), tensor(-0.2273, grad_fn=<SelectBackward>), tensor(-0.2010, grad_fn=<SelectBackward>), tensor(-0.8868, grad_fn=<SelectBackward>), tensor(-0.1501, grad_fn=<SelectBackward>), tensor(-0.0512, grad_fn=<SelectBackward>), tensor(-0.2405, grad_fn=<SelectBackward>), tensor(-0.1806, grad_fn=<SelectBackward>), tensor(-0.0987, grad_fn=<SelectBackward>), tensor(-0.2283, grad_fn=<SelectBackward>), tensor(-0.0470, grad_fn=<SelectBackward>), tensor(-2.0172, grad_fn=<SelectBackward>), tensor(-0.1390, grad_fn=<SelectBackward>), tensor(-0.3251, grad_fn=<SelectBackward>), tensor(-0.8182, grad_fn=<SelectBackward>), tensor(-0.5415, grad_fn=<SelectBackward>), tensor(-0.1603, grad_fn=<SelectBackward>), tensor(-0.1388, grad_fn=<SelectBackward>), tensor(-0.1795, grad_fn=<SelectBackward>), tensor(-0.0915, grad_fn=<SelectBackward>), tensor(-0.4331, grad_fn=<SelectBackward>), tensor(-0.0167, grad_fn=<SelectBackward>)]\n",
      "log_probs:  tensor([-4.6669, -2.5470, -0.2901, -0.5894, -2.0561, -3.1941, -0.0088, -0.3040,\n",
      "        -0.1381, -0.1435, -0.2627, -0.0588, -0.0636, -0.9550, -0.5735, -1.5332,\n",
      "        -0.0152, -0.0789, -0.0932, -0.2318, -0.6293, -0.1080, -0.2960, -0.7092,\n",
      "        -0.0155, -1.7748, -2.7146, -0.0393, -0.2205, -0.1660, -0.0903, -0.0803,\n",
      "        -0.0863, -0.1700, -0.1345, -0.0293, -0.1597, -0.0421, -0.0321, -0.0255,\n",
      "        -0.1248, -0.0405, -0.5886, -0.0282, -0.4740, -0.0617, -0.0433, -0.4787,\n",
      "        -0.9133, -0.5705, -0.5847, -0.3520, -0.2616, -0.3554, -0.0235, -0.1352,\n",
      "        -1.1350, -0.1434, -0.2216, -0.3948, -0.0077, -0.1014, -0.1600, -0.0531,\n",
      "        -0.1993, -0.0950, -0.0365, -0.4930, -0.2027, -0.0770, -0.0520, -0.0339,\n",
      "        -0.3140, -1.9794, -0.8831, -0.0143, -0.8226, -0.0073, -0.3520, -0.8421,\n",
      "        -0.1882, -0.0183, -0.1392, -0.2605, -0.9226, -0.5939, -0.2382, -0.3480,\n",
      "        -0.1614, -0.9395, -0.2407, -0.9279, -0.1572, -0.0059, -1.9716, -0.2585,\n",
      "        -1.2804, -0.3994, -0.8155, -0.0679, -0.2273, -0.2010, -0.8868, -0.1501,\n",
      "        -0.0512, -0.2405, -0.1806, -0.0987, -0.2283, -0.0470, -2.0172, -0.1390,\n",
      "        -0.3251, -0.8182, -0.5415, -0.1603, -0.1388, -0.1795, -0.0915, -0.4331,\n",
      "        -0.0167], grad_fn=<StackBackward>)\n",
      "distributions.shape:  torch.Size([1, 121, 4])\n",
      "distributions:  tensor([[[9.4015e-03, 3.6719e-01, 2.0731e-02, 6.0268e-01],\n",
      "         [3.0060e-03, 7.8319e-02, 7.1859e-03, 9.1149e-01],\n",
      "         [6.6509e-03, 2.2306e-01, 2.2093e-02, 7.4819e-01],\n",
      "         [7.6119e-03, 5.5468e-01, 1.9237e-02, 4.1847e-01],\n",
      "         [3.1969e-03, 8.4924e-01, 1.9602e-02, 1.2796e-01],\n",
      "         [8.5329e-03, 7.2063e-01, 4.1002e-02, 2.2984e-01],\n",
      "         [4.1970e-04, 7.6851e-03, 6.0733e-04, 9.9129e-01],\n",
      "         [3.0684e-03, 7.3784e-01, 1.3048e-02, 2.4604e-01],\n",
      "         [1.6171e-03, 1.1706e-01, 1.0329e-02, 8.7100e-01],\n",
      "         [3.1269e-03, 8.6635e-01, 1.5680e-02, 1.1484e-01],\n",
      "         [1.1998e-02, 1.9515e-01, 2.3897e-02, 7.6895e-01],\n",
      "         [1.5677e-03, 4.1605e-02, 1.3907e-02, 9.4292e-01],\n",
      "         [4.9696e-04, 5.6252e-02, 4.8500e-03, 9.3840e-01],\n",
      "         [8.1376e-03, 3.8480e-01, 5.1608e-03, 6.0190e-01],\n",
      "         [1.8752e-02, 3.4298e-01, 7.4696e-02, 5.6357e-01],\n",
      "         [1.4084e-02, 2.1585e-01, 1.7512e-02, 7.5255e-01],\n",
      "         [2.9453e-04, 1.4130e-02, 6.9033e-04, 9.8489e-01],\n",
      "         [2.8954e-03, 6.7289e-02, 5.7276e-03, 9.2409e-01],\n",
      "         [1.5068e-02, 5.4227e-02, 1.9688e-02, 9.1102e-01],\n",
      "         [1.6988e-03, 7.9313e-01, 2.6710e-02, 1.7846e-01],\n",
      "         [2.9736e-02, 4.0805e-01, 2.9254e-02, 5.3296e-01],\n",
      "         [2.5474e-03, 9.2452e-02, 7.3963e-03, 8.9760e-01],\n",
      "         [2.9474e-03, 7.4376e-01, 1.3718e-02, 2.3957e-01],\n",
      "         [2.0008e-02, 4.5107e-01, 3.6888e-02, 4.9203e-01],\n",
      "         [5.2232e-04, 1.3751e-02, 1.0634e-03, 9.8466e-01],\n",
      "         [6.9045e-03, 1.6952e-01, 1.5712e-02, 8.0786e-01],\n",
      "         [1.1946e-03, 6.6229e-02, 2.1624e-02, 9.1095e-01],\n",
      "         [4.1701e-04, 3.6933e-02, 1.1802e-03, 9.6147e-01],\n",
      "         [1.4671e-02, 1.4663e-01, 3.6591e-02, 8.0211e-01],\n",
      "         [2.3115e-03, 1.3544e-01, 1.5171e-02, 8.4708e-01],\n",
      "         [3.8899e-03, 6.3210e-02, 1.9258e-02, 9.1364e-01],\n",
      "         [5.0420e-03, 6.2592e-02, 9.4853e-03, 9.2288e-01],\n",
      "         [2.7492e-03, 7.2731e-02, 7.2363e-03, 9.1728e-01],\n",
      "         [6.3368e-03, 8.4369e-01, 2.1174e-02, 1.2880e-01],\n",
      "         [8.1050e-03, 9.8219e-02, 1.9553e-02, 8.7412e-01],\n",
      "         [7.2704e-04, 2.5034e-02, 3.0996e-03, 9.7114e-01],\n",
      "         [1.9460e-03, 1.1719e-01, 2.8453e-02, 8.5242e-01],\n",
      "         [1.4753e-03, 3.5301e-02, 4.4718e-03, 9.5875e-01],\n",
      "         [1.1618e-03, 2.7337e-02, 3.1352e-03, 9.6837e-01],\n",
      "         [1.5863e-04, 2.3519e-02, 1.4648e-03, 9.7486e-01],\n",
      "         [4.5122e-03, 1.0894e-01, 3.8425e-03, 8.8271e-01],\n",
      "         [6.7280e-04, 3.8278e-02, 7.4238e-04, 9.6031e-01],\n",
      "         [2.7768e-02, 5.5513e-01, 1.9726e-02, 3.9738e-01],\n",
      "         [1.8493e-03, 2.2226e-02, 3.7469e-03, 9.7218e-01],\n",
      "         [2.9403e-03, 3.4068e-01, 3.3888e-02, 6.2249e-01],\n",
      "         [8.3023e-04, 5.2969e-02, 6.0391e-03, 9.4016e-01],\n",
      "         [1.1336e-03, 3.5808e-02, 5.4238e-03, 9.5763e-01],\n",
      "         [9.5272e-03, 6.1957e-01, 3.9825e-02, 3.3108e-01],\n",
      "         [6.3750e-03, 5.7642e-01, 1.6002e-02, 4.0120e-01],\n",
      "         [1.0664e-02, 5.6523e-01, 2.1496e-02, 4.0261e-01],\n",
      "         [5.2654e-03, 5.5728e-01, 3.1030e-02, 4.0642e-01],\n",
      "         [9.7574e-03, 2.6343e-01, 2.3540e-02, 7.0327e-01],\n",
      "         [2.0243e-02, 1.5894e-01, 5.1001e-02, 7.6982e-01],\n",
      "         [2.2685e-02, 2.5121e-01, 2.5211e-02, 7.0089e-01],\n",
      "         [1.6991e-03, 1.9831e-02, 1.6542e-03, 9.7682e-01],\n",
      "         [5.6304e-03, 9.1243e-02, 2.9625e-02, 8.7350e-01],\n",
      "         [4.7941e-03, 6.1789e-01, 5.5910e-02, 3.2141e-01],\n",
      "         [3.8380e-03, 8.6638e-01, 1.9524e-02, 1.1026e-01],\n",
      "         [1.2439e-02, 1.5408e-01, 3.2209e-02, 8.0128e-01],\n",
      "         [3.4059e-03, 3.1744e-01, 5.3735e-03, 6.7378e-01],\n",
      "         [3.7512e-04, 4.3966e-03, 2.8559e-03, 9.9237e-01],\n",
      "         [1.3783e-03, 8.6825e-02, 8.2098e-03, 9.0359e-01],\n",
      "         [2.6267e-03, 1.3208e-01, 1.3114e-02, 8.5218e-01],\n",
      "         [1.6665e-03, 3.9200e-02, 1.0879e-02, 9.4825e-01],\n",
      "         [1.1956e-03, 1.6105e-01, 1.8474e-02, 8.1928e-01],\n",
      "         [9.8244e-04, 8.7837e-02, 1.7984e-03, 9.0938e-01],\n",
      "         [2.0295e-04, 3.4128e-02, 1.4780e-03, 9.6419e-01],\n",
      "         [7.8293e-03, 3.2381e-01, 5.7547e-02, 6.1082e-01],\n",
      "         [3.1952e-03, 8.1652e-01, 1.8858e-02, 1.6143e-01],\n",
      "         [4.2458e-03, 5.3148e-02, 1.6712e-02, 9.2589e-01],\n",
      "         [9.9034e-04, 3.8937e-02, 1.0756e-02, 9.4932e-01],\n",
      "         [1.8120e-03, 2.7627e-02, 3.9258e-03, 9.6664e-01],\n",
      "         [6.3425e-03, 2.1502e-01, 4.8114e-02, 7.3052e-01],\n",
      "         [2.7676e-03, 1.3815e-01, 1.9249e-03, 8.5715e-01],\n",
      "         [4.4033e-03, 4.1350e-01, 1.9154e-02, 5.6294e-01],\n",
      "         [2.8624e-04, 1.2746e-02, 1.2117e-03, 9.8576e-01],\n",
      "         [2.9940e-03, 5.4389e-01, 1.3814e-02, 4.3931e-01],\n",
      "         [2.1755e-04, 5.4819e-03, 1.5382e-03, 9.9276e-01],\n",
      "         [9.2996e-03, 7.0328e-01, 5.9073e-02, 2.2835e-01],\n",
      "         [7.3925e-04, 4.3081e-01, 1.0240e-02, 5.5821e-01],\n",
      "         [2.3874e-03, 8.2845e-01, 2.4053e-02, 1.4511e-01],\n",
      "         [1.8122e-04, 1.7409e-02, 5.0551e-04, 9.8190e-01],\n",
      "         [3.4022e-03, 1.1823e-01, 8.3057e-03, 8.7006e-01],\n",
      "         [1.3117e-02, 1.8745e-01, 2.8767e-02, 7.7067e-01],\n",
      "         [1.0090e-02, 3.9748e-01, 4.4386e-02, 5.4804e-01],\n",
      "         [4.8463e-03, 4.1955e-01, 2.3445e-02, 5.5216e-01],\n",
      "         [2.5565e-03, 2.0644e-01, 2.9302e-03, 7.8807e-01],\n",
      "         [1.0750e-03, 2.8896e-01, 3.8378e-03, 7.0612e-01],\n",
      "         [1.6305e-03, 1.3230e-01, 1.5129e-02, 8.5094e-01],\n",
      "         [2.8604e-03, 3.9083e-01, 6.5752e-02, 5.4056e-01],\n",
      "         [1.9616e-02, 1.5089e-01, 4.3431e-02, 7.8606e-01],\n",
      "         [1.8486e-03, 5.8085e-01, 2.1901e-02, 3.9540e-01],\n",
      "         [4.7506e-03, 8.5457e-01, 1.1689e-02, 1.2899e-01],\n",
      "         [3.1616e-04, 5.1973e-03, 3.2537e-04, 9.9416e-01],\n",
      "         [4.4617e-04, 1.3923e-01, 4.8123e-03, 8.5551e-01],\n",
      "         [5.8819e-03, 1.8768e-01, 3.4260e-02, 7.7218e-01],\n",
      "         [4.8092e-03, 2.7794e-01, 2.8437e-02, 6.8882e-01],\n",
      "         [6.8292e-03, 2.9517e-01, 2.7273e-02, 6.7073e-01],\n",
      "         [2.6190e-03, 4.4242e-01, 1.5896e-02, 5.3906e-01],\n",
      "         [2.7541e-03, 4.5996e-02, 1.6896e-02, 9.3435e-01],\n",
      "         [5.0574e-03, 7.9671e-01, 2.0290e-02, 1.7795e-01],\n",
      "         [1.9852e-03, 1.7615e-01, 3.9877e-03, 8.1787e-01],\n",
      "         [6.0776e-03, 4.1198e-01, 5.6785e-02, 5.2515e-01],\n",
      "         [2.2144e-03, 1.3011e-01, 7.0917e-03, 8.6058e-01],\n",
      "         [1.2798e-03, 4.5059e-02, 3.5545e-03, 9.5011e-01],\n",
      "         [1.1545e-02, 1.0551e-01, 9.6713e-02, 7.8623e-01],\n",
      "         [3.9286e-03, 1.2577e-01, 3.5500e-02, 8.3480e-01],\n",
      "         [2.4450e-03, 8.2957e-02, 8.5661e-03, 9.0603e-01],\n",
      "         [3.7477e-03, 7.9591e-01, 3.1995e-02, 1.6834e-01],\n",
      "         [2.2859e-03, 3.7191e-02, 6.4806e-03, 9.5404e-01],\n",
      "         [6.7625e-03, 1.3303e-01, 2.2788e-02, 8.3742e-01],\n",
      "         [3.7546e-03, 1.1780e-01, 8.1766e-03, 8.7027e-01],\n",
      "         [1.7834e-03, 7.2247e-01, 3.7643e-02, 2.3810e-01],\n",
      "         [1.3449e-02, 5.0174e-01, 4.3592e-02, 4.4122e-01],\n",
      "         [4.9626e-03, 4.0536e-01, 7.7820e-03, 5.8189e-01],\n",
      "         [6.2456e-03, 1.3721e-01, 4.6183e-03, 8.5192e-01],\n",
      "         [6.7950e-03, 1.0684e-01, 1.5979e-02, 8.7038e-01],\n",
      "         [1.7509e-03, 1.5857e-01, 3.9762e-03, 8.3570e-01],\n",
      "         [2.4043e-03, 8.0827e-02, 4.2052e-03, 9.1256e-01],\n",
      "         [8.2179e-03, 2.9293e-01, 5.0390e-02, 6.4847e-01],\n",
      "         [9.2490e-04, 1.3401e-02, 2.2247e-03, 9.8345e-01]]],\n",
      "       grad_fn=<StackBackward>)\n",
      "Updating critic...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V_trg.shape (after critic):  torch.Size([121])\n",
      "V_trg.shape (after sum):  torch.Size([121])\n",
      "V_trg.shape (after squeeze):  torch.Size([121])\n",
      "V_trg.shape (after squeeze):  tensor([ 0.5738,  0.6104,  0.5511,  0.6906,  0.8225,  0.5154,  0.8526,  0.7682,\n",
      "         0.5376,  0.6742,  0.9454,  0.6130,  0.6713,  1.0631,  0.7417,  0.8446,\n",
      "         0.8679,  0.6169,  1.0472,  0.2958, -0.0696,  0.7297,  0.7050,  0.6661,\n",
      "         0.9077,  1.2574,  0.5316,  0.4586,  0.7473,  0.2964,  0.4462,  0.7954,\n",
      "         0.4869,  0.6856,  0.3689,  0.4118,  0.4903,  0.8901,  0.4911,  0.5354,\n",
      "         1.1028,  0.6805,  0.8600,  0.6682,  0.6146,  0.7621,  0.6380,  0.8922,\n",
      "         0.8848,  0.8053,  0.8377,  0.5395,  0.5027,  0.5657,  0.9564,  0.8364,\n",
      "         0.1721,  0.2799,  0.9869,  0.2644,  0.2923,  0.7986,  0.4818,  0.4773,\n",
      "         0.6395,  0.9427,  0.4739,  0.9172,  0.8055,  0.4096,  0.5598,  0.7220,\n",
      "         0.5975,  0.4529,  1.1029,  0.8274,  1.0230,  1.0984,  0.7205,  0.4684,\n",
      "         0.2042,  0.7883,  0.9636,  0.3859,  0.8183,  0.4349,  0.2500,  1.0078,\n",
      "         0.2831,  0.4080,  1.1263,  0.7965,  0.6566,  0.7429,  0.3532,  1.0276,\n",
      "         0.7768,  0.8412,  0.7314,  0.8157,  0.7118,  0.7787,  0.7642,  0.7364,\n",
      "         0.7313,  0.8899,  0.6013,  0.7825,  0.8272,  0.7398,  0.1848,  0.7382,\n",
      "         0.6810,  0.5293,  1.1431,  1.0644,  0.7200,  1.6205,  1.3244,  1.2931,\n",
      "         1.2557])\n",
      "V1.shape:  torch.Size([121])\n",
      "V1:  tensor([[1.5554],\n",
      "        [2.1815],\n",
      "        [1.1375],\n",
      "        [0.7359],\n",
      "        [1.0213],\n",
      "        [1.9136],\n",
      "        [2.6026],\n",
      "        [2.3025],\n",
      "        [2.9043],\n",
      "        [2.0636],\n",
      "        [1.7325],\n",
      "        [1.9209],\n",
      "        [0.7341],\n",
      "        [0.8925],\n",
      "        [1.1059],\n",
      "        [2.9121],\n",
      "        [1.8808],\n",
      "        [2.5652],\n",
      "        [1.6852],\n",
      "        [1.9256],\n",
      "        [1.7430],\n",
      "        [1.4451],\n",
      "        [2.0372],\n",
      "        [1.9589],\n",
      "        [1.6473],\n",
      "        [1.1107],\n",
      "        [2.2166],\n",
      "        [0.8653],\n",
      "        [2.4042],\n",
      "        [2.6615],\n",
      "        [1.7010],\n",
      "        [1.8073],\n",
      "        [1.5694],\n",
      "        [1.9621],\n",
      "        [2.0171],\n",
      "        [1.4008],\n",
      "        [2.0855],\n",
      "        [1.6524],\n",
      "        [1.1665],\n",
      "        [2.0099],\n",
      "        [1.3889],\n",
      "        [0.9637],\n",
      "        [1.8073],\n",
      "        [0.8010],\n",
      "        [1.5489],\n",
      "        [1.4928],\n",
      "        [2.0697],\n",
      "        [0.8575],\n",
      "        [2.4920],\n",
      "        [1.9448],\n",
      "        [2.4636],\n",
      "        [0.9582],\n",
      "        [1.7083],\n",
      "        [1.7360],\n",
      "        [1.8495],\n",
      "        [1.3342],\n",
      "        [2.2847],\n",
      "        [1.4397],\n",
      "        [1.5881],\n",
      "        [1.8632],\n",
      "        [0.6405],\n",
      "        [0.8899],\n",
      "        [0.8838],\n",
      "        [1.4782],\n",
      "        [1.6275],\n",
      "        [2.4423],\n",
      "        [2.1260],\n",
      "        [1.5507],\n",
      "        [2.4107],\n",
      "        [1.0431],\n",
      "        [0.6613],\n",
      "        [2.7028],\n",
      "        [1.2982],\n",
      "        [1.2265],\n",
      "        [2.5951],\n",
      "        [0.6609],\n",
      "        [1.1469],\n",
      "        [1.9115],\n",
      "        [1.1653],\n",
      "        [1.4611],\n",
      "        [0.7614],\n",
      "        [2.3710],\n",
      "        [1.7803],\n",
      "        [1.9157],\n",
      "        [2.4459],\n",
      "        [1.4287],\n",
      "        [1.9487],\n",
      "        [1.8681],\n",
      "        [0.9446],\n",
      "        [2.0732],\n",
      "        [1.4239],\n",
      "        [1.8610],\n",
      "        [1.0844],\n",
      "        [2.1567],\n",
      "        [1.7160],\n",
      "        [2.1969],\n",
      "        [1.3202],\n",
      "        [2.0314],\n",
      "        [1.7158],\n",
      "        [1.7251],\n",
      "        [1.3969],\n",
      "        [1.4316],\n",
      "        [2.1569],\n",
      "        [1.0728],\n",
      "        [1.6754],\n",
      "        [2.2851],\n",
      "        [2.3255],\n",
      "        [2.9072],\n",
      "        [2.0237],\n",
      "        [0.6900],\n",
      "        [2.2176],\n",
      "        [0.7995],\n",
      "        [1.7499],\n",
      "        [0.6895],\n",
      "        [1.0169],\n",
      "        [1.7192],\n",
      "        [0.8908],\n",
      "        [2.2093],\n",
      "        [1.6584],\n",
      "        [2.1827],\n",
      "        [1.4905]], grad_fn=<AddmmBackward>)\n",
      "Updating actor...\n",
      "V_trg.shape:  torch.Size([121])\n",
      "V_trg:  tensor([0.5123, 1.0896, 1.1760, 0.6588, 1.0981, 0.4223, 0.8878, 0.9031, 1.3161,\n",
      "        0.9981, 1.1012, 0.8468, 1.1386, 1.3988, 1.0917, 0.9674, 1.0225, 0.8277,\n",
      "        0.7172, 0.8034, 0.9293, 1.2315, 1.0819, 1.5387, 0.8463, 1.0562, 1.2180,\n",
      "        0.8320, 0.7626, 0.7191, 1.2196, 1.0534, 0.9346, 0.9033, 0.7251, 1.3073,\n",
      "        1.1646, 1.2100, 1.3615, 1.0636, 0.9774, 0.8462, 1.0442, 1.4727, 0.8387,\n",
      "        0.9105, 1.2021, 0.9780, 1.0613, 1.1403, 0.4731, 0.7270, 0.5422, 0.8851,\n",
      "        1.0605, 0.5410, 0.7397, 1.0928, 0.9153, 0.5996, 0.6495, 1.2517, 1.2924,\n",
      "        0.9335, 0.9776, 1.3247, 0.8289, 1.1075, 1.5881, 1.4269, 0.7297, 1.4020,\n",
      "        1.2984, 0.5529, 1.4405, 1.4545, 0.9165, 1.2830, 0.7170, 1.2738, 1.5784,\n",
      "        0.4929, 0.8925, 1.2879, 1.1277, 0.9942, 0.9998, 0.9049, 1.4313, 0.8905,\n",
      "        1.3986, 1.9875, 1.1225, 1.4081, 0.6433, 1.7173, 1.8026, 1.3616, 1.6362,\n",
      "        1.2376, 1.0665, 2.1686, 1.2062, 1.3776, 1.2480, 1.2531, 1.9330, 1.4474,\n",
      "        0.6956, 1.8809, 2.3448, 1.2589, 2.1774, 2.0377, 0.7351, 0.7585, 1.4500,\n",
      "        0.6267, 1.4007, 1.9704, 0.8600])\n",
      "V_pred.shape:  torch.Size([121])\n",
      "V_pred:  tensor([1.7493, 1.1742, 1.8486, 2.2208, 2.0926, 1.5504, 1.9621, 2.1759, 0.8383,\n",
      "        1.0416, 1.4963, 2.1645, 1.5208, 1.9532, 1.3659, 1.7295, 1.9522, 0.7561,\n",
      "        1.3727, 1.6482, 2.1188, 1.9641, 1.0008, 1.3323, 2.1196, 1.3897, 1.7339,\n",
      "        1.9473, 2.0181, 1.9876, 1.7450, 1.4530, 1.3314, 1.9967, 1.6080, 1.4491,\n",
      "        2.1532, 0.7135, 1.8149, 0.7268, 0.7679, 1.5514, 1.9426, 1.4803, 2.1033,\n",
      "        1.9555, 1.0543, 1.4723, 1.8799, 1.4569, 1.8952, 1.5894, 2.2722, 1.5108,\n",
      "        1.0473, 0.6807, 1.4341, 1.8454, 1.6213, 1.9022, 1.1746, 1.1464, 1.7943,\n",
      "        1.5814, 0.8360, 1.4413, 0.9827, 1.2944, 2.1236, 1.6488, 1.0933, 0.6690,\n",
      "        1.9229, 2.1427, 1.2821, 2.0336, 1.1909, 1.5067, 1.1954, 1.5480, 1.4007,\n",
      "        1.3119, 1.9488, 1.5682, 1.9807, 1.9079, 1.4471, 1.2167, 1.2831, 1.9103,\n",
      "        1.5679, 2.2843, 1.7906, 1.3838, 1.1553, 0.6415, 2.1778, 1.2309, 1.7177,\n",
      "        1.5758, 2.2564, 0.8293, 0.8604, 1.8014, 1.2726, 1.2769, 1.8162, 1.5928,\n",
      "        0.8422, 1.2061, 1.8008, 1.5222, 0.9489, 0.8997, 1.8332, 2.1759, 1.4954,\n",
      "        2.0732, 1.6683, 1.9807, 1.5190])\n",
      "A.shape:  torch.Size([121])\n",
      "A:  tensor([-1.2370, -0.0846, -0.6726, -1.5619, -0.9945, -1.1281, -1.0743, -1.2728,\n",
      "         0.4778, -0.0435, -0.3951, -1.3177, -0.3822, -0.5544, -0.2742, -0.7620,\n",
      "        -0.9298,  0.0716, -0.6555, -0.8448, -1.1895, -0.7326,  0.0811,  0.2063,\n",
      "        -1.2733, -0.3336, -0.5160, -1.1153, -1.2555, -1.2684, -0.5253, -0.3995,\n",
      "        -0.3967, -1.0933, -0.8830, -0.1417, -0.9886,  0.4964, -0.4533,  0.3369,\n",
      "         0.2095, -0.7053, -0.8985, -0.0076, -1.2647, -1.0450,  0.1478, -0.4943,\n",
      "        -0.8186, -0.3166, -1.4221, -0.8624, -1.7300, -0.6258,  0.0132, -0.1397,\n",
      "        -0.6943, -0.7526, -0.7060, -1.3025, -0.5252,  0.1053, -0.5019, -0.6479,\n",
      "         0.1417, -0.1165, -0.1539, -0.1869, -0.5355, -0.2219, -0.3636,  0.7330,\n",
      "        -0.6245, -1.5898,  0.1584, -0.5791, -0.2744, -0.2237, -0.4784, -0.2742,\n",
      "         0.1776, -0.8190, -1.0564, -0.2803, -0.8530, -0.9136, -0.4473, -0.3117,\n",
      "         0.1482, -1.0198, -0.1693, -0.2968, -0.6681,  0.0243, -0.5120,  1.0757,\n",
      "        -0.3752,  0.1306, -0.0815, -0.3382, -1.1898,  1.3394,  0.3457, -0.4238,\n",
      "        -0.0246, -0.0238,  0.1169, -0.1453, -0.1466,  0.6748,  0.5440, -0.2633,\n",
      "         1.2285,  1.1380, -1.0980, -1.4174, -0.0454, -1.4465, -0.2676, -0.0103,\n",
      "        -0.6590])\n",
      "policy_gradient.shape:  torch.Size([121])\n",
      "policy_gradient:  tensor([-5.7730e+00, -2.1542e-01, -1.9512e-01, -9.2056e-01, -2.0448e+00,\n",
      "        -3.6034e+00, -9.4009e-03, -3.8697e-01,  6.5998e-02, -6.2383e-03,\n",
      "        -1.0381e-01, -7.7445e-02, -2.4300e-02, -5.2943e-01, -1.5725e-01,\n",
      "        -1.1683e+00, -1.4161e-02,  5.6562e-03, -6.1086e-02, -1.9579e-01,\n",
      "        -7.4853e-01, -7.9142e-02,  2.4020e-02,  1.4635e-01, -1.9679e-02,\n",
      "        -5.9201e-01, -1.4006e+00, -4.3821e-02, -2.7684e-01, -2.1052e-01,\n",
      "        -4.7446e-02, -3.2066e-02, -3.4253e-02, -1.8584e-01, -1.1879e-01,\n",
      "        -4.1511e-03, -1.5787e-01,  2.0910e-02, -1.4573e-02,  8.5782e-03,\n",
      "         2.6136e-02, -2.8565e-02, -5.2880e-01, -2.1363e-04, -5.9950e-01,\n",
      "        -6.4480e-02,  6.3984e-03, -2.3664e-01, -7.4759e-01, -1.8063e-01,\n",
      "        -8.3146e-01, -3.0358e-01, -4.5256e-01, -2.2240e-01,  3.1037e-04,\n",
      "        -1.8895e-02, -7.8810e-01, -1.0795e-01, -1.5642e-01, -5.1431e-01,\n",
      "        -4.0212e-03,  1.0678e-02, -8.0285e-02, -3.4424e-02,  2.8242e-02,\n",
      "        -1.1070e-02, -5.6120e-03, -9.2129e-02, -1.0855e-01, -1.7087e-02,\n",
      "        -1.8913e-02,  2.4874e-02, -1.9609e-01, -3.1468e+00,  1.3989e-01,\n",
      "        -8.3085e-03, -2.2572e-01, -1.6248e-03, -1.6840e-01, -2.3092e-01,\n",
      "         3.3427e-02, -1.4957e-02, -1.4703e-01, -7.3015e-02, -7.8700e-01,\n",
      "        -5.4263e-01, -1.0654e-01, -1.0847e-01,  2.3917e-02, -9.5809e-01,\n",
      "        -4.0749e-02, -2.7543e-01, -1.0499e-01,  1.4208e-04, -1.0094e+00,\n",
      "         2.7812e-01, -4.8033e-01,  5.2174e-02, -6.6436e-02, -2.2963e-02,\n",
      "        -2.7041e-01,  2.6928e-01,  3.0657e-01, -6.3627e-02, -1.2581e-03,\n",
      "        -5.7343e-03,  2.1100e-02, -1.4343e-02, -3.3459e-02,  3.1748e-02,\n",
      "         1.0973e+00, -3.6583e-02,  3.9937e-01,  9.3114e-01, -5.9456e-01,\n",
      "        -2.2715e-01, -6.3033e-03, -2.5962e-01, -2.4488e-02, -4.4781e-03,\n",
      "        -1.0998e-02], grad_fn=<MulBackward0>)\n",
      "policy_grad:  tensor(-31.9235, grad_fn=<SumBackward0>)\n",
      "Negative entropy:  tensor(-61.4726, grad_fn=<SumBackward0>)\n",
      "Actor loss:  tensor(-32.5382, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distribution:  tensor([[0.0037, 0.3403, 0.0047, 0.6513]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0026, 0.0464, 0.0038, 0.9472]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0014, 0.1026, 0.0095, 0.8865]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0098, 0.3318, 0.0335, 0.6249]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0015, 0.0534, 0.0042, 0.9409]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0192, 0.2662, 0.0579, 0.6567]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0035, 0.2827, 0.0633, 0.6506]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0078, 0.3565, 0.0318, 0.6039]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[2.6220e-04, 1.5005e-02, 4.9477e-04, 9.8424e-01]],\n",
      "       grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0082, 0.5478, 0.0139, 0.4300]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0100, 0.5620, 0.0433, 0.3847]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0048, 0.1266, 0.0016, 0.8670]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0054, 0.1148, 0.0057, 0.8741]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0117, 0.1476, 0.0146, 0.8261]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0020, 0.2189, 0.0310, 0.7481]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0034, 0.1094, 0.0257, 0.8616]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[6.3147e-04, 1.8182e-02, 4.1588e-03, 9.7703e-01]],\n",
      "       grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0023, 0.1088, 0.0175, 0.8714]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0060, 0.1325, 0.0294, 0.8322]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0015, 0.0574, 0.0037, 0.9374]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0053, 0.7805, 0.0307, 0.1835]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[6.2029e-04, 1.9834e-02, 1.1147e-03, 9.7843e-01]],\n",
      "       grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0040, 0.5366, 0.0413, 0.4182]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0013, 0.0118, 0.0064, 0.9806]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0061, 0.0469, 0.0093, 0.9377]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0010, 0.0346, 0.0046, 0.9597]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0067, 0.2030, 0.0357, 0.7547]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[2.5902e-04, 2.1954e-02, 2.3072e-03, 9.7548e-01]],\n",
      "       grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0014, 0.0549, 0.0024, 0.9413]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0054, 0.3230, 0.1063, 0.5653]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0023, 0.3289, 0.0133, 0.6555]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0033, 0.0466, 0.0056, 0.9445]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0100, 0.1519, 0.0386, 0.7995]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0084, 0.3534, 0.0295, 0.6087]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0014, 0.0484, 0.0013, 0.9489]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0449, 0.5732, 0.0589, 0.3230]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[5.5452e-04, 9.1038e-03, 2.8262e-04, 9.9006e-01]],\n",
      "       grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0205, 0.4138, 0.0174, 0.5482]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0051, 0.1501, 0.0087, 0.8361]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0021, 0.2544, 0.0022, 0.7413]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0033, 0.1160, 0.0044, 0.8762]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0130, 0.1561, 0.0381, 0.7928]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[2.4039e-04, 1.7188e-02, 1.3065e-03, 9.8126e-01]],\n",
      "       grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[2.7106e-04, 9.0370e-03, 9.9668e-04, 9.8970e-01]],\n",
      "       grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0109, 0.4641, 0.0686, 0.4564]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0058, 0.6114, 0.0694, 0.3134]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[5.0303e-04, 2.6796e-02, 1.6788e-03, 9.7102e-01]],\n",
      "       grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0039, 0.1603, 0.0595, 0.7763]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0014, 0.0739, 0.0048, 0.9200]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0019, 0.0189, 0.0060, 0.9732]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0025, 0.0975, 0.0358, 0.8643]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0046, 0.1365, 0.0064, 0.8525]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0024, 0.0302, 0.0042, 0.9632]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0030, 0.8007, 0.0237, 0.1727]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0072, 0.0561, 0.0364, 0.9003]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0196, 0.3860, 0.0317, 0.5628]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0034, 0.0419, 0.0324, 0.9224]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0012, 0.0854, 0.0065, 0.9070]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0039, 0.2031, 0.0078, 0.7852]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0068, 0.3621, 0.0343, 0.5968]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0070, 0.0607, 0.0218, 0.9104]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0052, 0.0768, 0.0148, 0.9031]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0022, 0.0114, 0.0024, 0.9840]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0079, 0.0744, 0.0255, 0.8922]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[8.1977e-04, 1.1957e-02, 2.2534e-03, 9.8497e-01]],\n",
      "       grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0021, 0.0918, 0.0089, 0.8973]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0129, 0.3998, 0.0138, 0.5735]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0016, 0.0276, 0.0013, 0.9696]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[1.2172e-04, 7.4732e-03, 4.5318e-04, 9.9195e-01]],\n",
      "       grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0033, 0.5200, 0.0238, 0.4529]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[5.7213e-04, 9.2114e-03, 3.8318e-03, 9.8638e-01]],\n",
      "       grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0017, 0.1091, 0.0070, 0.8822]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[6.6973e-04, 2.1938e-02, 2.4518e-03, 9.7494e-01]],\n",
      "       grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0041, 0.4967, 0.0932, 0.4060]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0030, 0.0681, 0.0173, 0.9116]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[1.8737e-04, 1.9377e-02, 2.3965e-03, 9.7804e-01]],\n",
      "       grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0015, 0.5273, 0.0258, 0.4455]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0189, 0.1806, 0.0603, 0.7402]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0010, 0.1426, 0.0023, 0.8541]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[5.5696e-04, 6.4392e-02, 7.7853e-04, 9.3427e-01]],\n",
      "       grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0086, 0.0489, 0.0235, 0.9190]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0133, 0.2094, 0.0397, 0.7375]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0015, 0.0736, 0.0050, 0.9199]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[2.0772e-04, 7.7542e-03, 4.1247e-04, 9.9163e-01]],\n",
      "       grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0036, 0.0871, 0.0129, 0.8965]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0011, 0.1028, 0.0055, 0.8906]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0040, 0.3099, 0.0090, 0.6771]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0104, 0.4647, 0.0330, 0.4919]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[2.3474e-04, 1.0341e-02, 1.4102e-03, 9.8801e-01]],\n",
      "       grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0035, 0.2451, 0.0112, 0.7402]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0064, 0.0820, 0.0048, 0.9068]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0009, 0.0984, 0.0122, 0.8884]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0021, 0.7189, 0.0185, 0.2605]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[5.2617e-04, 2.0800e-02, 7.7988e-04, 9.7789e-01]],\n",
      "       grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0120, 0.4939, 0.0146, 0.4795]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0062, 0.4986, 0.0366, 0.4586]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0020, 0.0384, 0.0237, 0.9359]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0063, 0.0329, 0.0137, 0.9471]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0051, 0.3836, 0.0189, 0.5924]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0073, 0.2834, 0.0503, 0.6590]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[8.3047e-04, 4.8261e-02, 7.6232e-03, 9.4329e-01]],\n",
      "       grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0032, 0.0200, 0.0073, 0.9694]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[2.5902e-04, 2.1892e-02, 1.2982e-03, 9.7655e-01]],\n",
      "       grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0054, 0.6314, 0.0475, 0.3157]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0022, 0.0748, 0.0044, 0.9187]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0075, 0.1794, 0.0241, 0.7889]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0048, 0.3299, 0.0328, 0.6326]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[6.4369e-04, 8.9791e-02, 3.7222e-03, 9.0584e-01]],\n",
      "       grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[1.1652e-04, 8.7800e-03, 7.2739e-04, 9.9038e-01]],\n",
      "       grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0110, 0.0711, 0.0037, 0.9143]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0017, 0.0290, 0.0030, 0.9663]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0031, 0.0260, 0.0029, 0.9679]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[6.8157e-04, 1.7038e-02, 1.3248e-03, 9.8096e-01]],\n",
      "       grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[4.8516e-04, 3.8205e-02, 1.2663e-03, 9.6004e-01]],\n",
      "       grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0013, 0.0087, 0.0030, 0.9869]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0039, 0.0744, 0.0019, 0.9198]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0021, 0.3607, 0.0661, 0.5711]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[4.9833e-04, 1.5636e-02, 2.2333e-03, 9.8163e-01]],\n",
      "       grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[3.6531e-04, 1.0152e-02, 6.7507e-04, 9.8881e-01]],\n",
      "       grad_fn=<ExpBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distribution:  tensor([[0.0092, 0.1052, 0.0039, 0.8817]], grad_fn=<ExpBackward>)\n",
      "distribution:  tensor([[0.0080, 0.0824, 0.0578, 0.8518]], grad_fn=<ExpBackward>)\n",
      "n_step_rewards.shape:  (121,)\n",
      "rewards.shape:  (121,)\n",
      "n_step_rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0.]\n",
      "rewards:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0.]\n",
      "bootstrap:  [False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      "  True]\n",
      "done.shape: (before n_steps) (121,)\n",
      "done: (before n_steps) [False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False]\n",
      "done.shape: (after n_steps) (121,)\n",
      "Gamma_V.shape:  (121,)\n",
      "done: (after n_steps) [False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False]\n",
      "Gamma_V:  [0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176 0.66897176\n",
      " 0.66897176 0.66897176 0.66897176 0.66897176 0.67572905 0.6825546\n",
      " 0.68944909 0.69641322 0.70344769 0.71055323 0.71773053 0.72498034\n",
      " 0.73230337 0.73970037 0.74717209 0.75471929 0.76234271 0.77004315\n",
      " 0.77782136 0.78567814 0.79361428 0.80163059 0.80972787 0.81790694\n",
      " 0.82616862 0.83451376 0.84294319 0.85145777 0.86005835 0.86874581\n",
      " 0.87752102 0.88638487 0.89533825 0.90438208 0.91351725 0.92274469\n",
      " 0.93206535 0.94148015 0.95099005 0.96059601 0.970299   0.9801\n",
      " 0.99      ]\n",
      "old_states.shape:  torch.Size([121, 1, 9, 9])\n",
      "new_states.shape:  torch.Size([121, 1, 9, 9])\n",
      "log_probs:  [tensor(-0.4288, grad_fn=<SelectBackward>), tensor(-0.0542, grad_fn=<SelectBackward>), tensor(-0.1205, grad_fn=<SelectBackward>), tensor(-1.1031, grad_fn=<SelectBackward>), tensor(-0.0609, grad_fn=<SelectBackward>), tensor(-1.3236, grad_fn=<SelectBackward>), tensor(-2.7605, grad_fn=<SelectBackward>), tensor(-1.0313, grad_fn=<SelectBackward>), tensor(-0.0159, grad_fn=<SelectBackward>), tensor(-0.6019, grad_fn=<SelectBackward>), tensor(-0.5763, grad_fn=<SelectBackward>), tensor(-0.1427, grad_fn=<SelectBackward>), tensor(-0.1345, grad_fn=<SelectBackward>), tensor(-0.1910, grad_fn=<SelectBackward>), tensor(-1.5190, grad_fn=<SelectBackward>), tensor(-0.1490, grad_fn=<SelectBackward>), tensor(-0.0232, grad_fn=<SelectBackward>), tensor(-0.1376, grad_fn=<SelectBackward>), tensor(-5.1243, grad_fn=<SelectBackward>), tensor(-0.0646, grad_fn=<SelectBackward>), tensor(-0.2478, grad_fn=<SelectBackward>), tensor(-0.0218, grad_fn=<SelectBackward>), tensor(-0.8719, grad_fn=<SelectBackward>), tensor(-0.0196, grad_fn=<SelectBackward>), tensor(-0.0643, grad_fn=<SelectBackward>), tensor(-0.0411, grad_fn=<SelectBackward>), tensor(-0.2814, grad_fn=<SelectBackward>), tensor(-0.0248, grad_fn=<SelectBackward>), tensor(-0.0605, grad_fn=<SelectBackward>), tensor(-0.5703, grad_fn=<SelectBackward>), tensor(-0.4224, grad_fn=<SelectBackward>), tensor(-0.0571, grad_fn=<SelectBackward>), tensor(-0.2237, grad_fn=<SelectBackward>), tensor(-0.4965, grad_fn=<SelectBackward>), tensor(-0.0524, grad_fn=<SelectBackward>), tensor(-0.5566, grad_fn=<SelectBackward>), tensor(-0.0100, grad_fn=<SelectBackward>), tensor(-0.6011, grad_fn=<SelectBackward>), tensor(-0.1790, grad_fn=<SelectBackward>), tensor(-0.2994, grad_fn=<SelectBackward>), tensor(-0.1321, grad_fn=<SelectBackward>), tensor(-0.2322, grad_fn=<SelectBackward>), tensor(-0.0189, grad_fn=<SelectBackward>), tensor(-0.0104, grad_fn=<SelectBackward>), tensor(-0.7843, grad_fn=<SelectBackward>), tensor(-1.1603, grad_fn=<SelectBackward>), tensor(-0.0294, grad_fn=<SelectBackward>), tensor(-0.2533, grad_fn=<SelectBackward>), tensor(-0.0834, grad_fn=<SelectBackward>), tensor(-0.0271, grad_fn=<SelectBackward>), tensor(-3.3310, grad_fn=<SelectBackward>), tensor(-0.1596, grad_fn=<SelectBackward>), tensor(-0.0375, grad_fn=<SelectBackward>), tensor(-0.2222, grad_fn=<SelectBackward>), tensor(-0.1051, grad_fn=<SelectBackward>), tensor(-0.9520, grad_fn=<SelectBackward>), tensor(-0.0808, grad_fn=<SelectBackward>), tensor(-0.0977, grad_fn=<SelectBackward>), tensor(-0.2418, grad_fn=<SelectBackward>), tensor(-0.5162, grad_fn=<SelectBackward>), tensor(-0.0938, grad_fn=<SelectBackward>), tensor(-0.1019, grad_fn=<SelectBackward>), tensor(-0.0161, grad_fn=<SelectBackward>), tensor(-0.1141, grad_fn=<SelectBackward>), tensor(-0.0151, grad_fn=<SelectBackward>), tensor(-0.1084, grad_fn=<SelectBackward>), tensor(-0.5561, grad_fn=<SelectBackward>), tensor(-0.0309, grad_fn=<SelectBackward>), tensor(-0.0081, grad_fn=<SelectBackward>), tensor(-0.6538, grad_fn=<SelectBackward>), tensor(-0.0137, grad_fn=<SelectBackward>), tensor(-0.1253, grad_fn=<SelectBackward>), tensor(-0.0254, grad_fn=<SelectBackward>), tensor(-0.6997, grad_fn=<SelectBackward>), tensor(-0.0925, grad_fn=<SelectBackward>), tensor(-0.0222, grad_fn=<SelectBackward>), tensor(-0.8086, grad_fn=<SelectBackward>), tensor(-1.7115, grad_fn=<SelectBackward>), tensor(-1.9479, grad_fn=<SelectBackward>), tensor(-0.0680, grad_fn=<SelectBackward>), tensor(-0.0845, grad_fn=<SelectBackward>), tensor(-0.3045, grad_fn=<SelectBackward>), tensor(-0.0835, grad_fn=<SelectBackward>), tensor(-0.0084, grad_fn=<SelectBackward>), tensor(-0.1093, grad_fn=<SelectBackward>), tensor(-0.1159, grad_fn=<SelectBackward>), tensor(-0.3899, grad_fn=<SelectBackward>), tensor(-0.7663, grad_fn=<SelectBackward>), tensor(-0.0121, grad_fn=<SelectBackward>), tensor(-1.4063, grad_fn=<SelectBackward>), tensor(-0.0978, grad_fn=<SelectBackward>), tensor(-0.1183, grad_fn=<SelectBackward>), tensor(-0.3300, grad_fn=<SelectBackward>), tensor(-0.0224, grad_fn=<SelectBackward>), tensor(-0.7054, grad_fn=<SelectBackward>), tensor(-3.3072, grad_fn=<SelectBackward>), tensor(-0.0662, grad_fn=<SelectBackward>), tensor(-0.0543, grad_fn=<SelectBackward>), tensor(-0.5235, grad_fn=<SelectBackward>), tensor(-0.4171, grad_fn=<SelectBackward>), tensor(-0.0584, grad_fn=<SelectBackward>), tensor(-0.0310, grad_fn=<SelectBackward>), tensor(-0.0237, grad_fn=<SelectBackward>), tensor(-0.4597, grad_fn=<SelectBackward>), tensor(-0.0848, grad_fn=<SelectBackward>), tensor(-0.2371, grad_fn=<SelectBackward>), tensor(-0.4580, grad_fn=<SelectBackward>), tensor(-0.0989, grad_fn=<SelectBackward>), tensor(-0.0097, grad_fn=<SelectBackward>), tensor(-0.0896, grad_fn=<SelectBackward>), tensor(-0.0342, grad_fn=<SelectBackward>), tensor(-0.0326, grad_fn=<SelectBackward>), tensor(-0.0192, grad_fn=<SelectBackward>), tensor(-0.0408, grad_fn=<SelectBackward>), tensor(-0.0132, grad_fn=<SelectBackward>), tensor(-2.5982, grad_fn=<SelectBackward>), tensor(-1.0197, grad_fn=<SelectBackward>), tensor(-0.0185, grad_fn=<SelectBackward>), tensor(-0.0113, grad_fn=<SelectBackward>), tensor(-0.1259, grad_fn=<SelectBackward>), tensor(-0.1604, grad_fn=<SelectBackward>)]\n",
      "log_probs:  tensor([-0.4288, -0.0542, -0.1205, -1.1031, -0.0609, -1.3236, -2.7605, -1.0313,\n",
      "        -0.0159, -0.6019, -0.5763, -0.1427, -0.1345, -0.1910, -1.5190, -0.1490,\n",
      "        -0.0232, -0.1376, -5.1243, -0.0646, -0.2478, -0.0218, -0.8719, -0.0196,\n",
      "        -0.0643, -0.0411, -0.2814, -0.0248, -0.0605, -0.5703, -0.4224, -0.0571,\n",
      "        -0.2237, -0.4965, -0.0524, -0.5566, -0.0100, -0.6011, -0.1790, -0.2994,\n",
      "        -0.1321, -0.2322, -0.0189, -0.0104, -0.7843, -1.1603, -0.0294, -0.2533,\n",
      "        -0.0834, -0.0271, -3.3310, -0.1596, -0.0375, -0.2222, -0.1051, -0.9520,\n",
      "        -0.0808, -0.0977, -0.2418, -0.5162, -0.0938, -0.1019, -0.0161, -0.1141,\n",
      "        -0.0151, -0.1084, -0.5561, -0.0309, -0.0081, -0.6538, -0.0137, -0.1253,\n",
      "        -0.0254, -0.6997, -0.0925, -0.0222, -0.8086, -1.7115, -1.9479, -0.0680,\n",
      "        -0.0845, -0.3045, -0.0835, -0.0084, -0.1093, -0.1159, -0.3899, -0.7663,\n",
      "        -0.0121, -1.4063, -0.0978, -0.1183, -0.3300, -0.0224, -0.7054, -3.3072,\n",
      "        -0.0662, -0.0543, -0.5235, -0.4171, -0.0584, -0.0310, -0.0237, -0.4597,\n",
      "        -0.0848, -0.2371, -0.4580, -0.0989, -0.0097, -0.0896, -0.0342, -0.0326,\n",
      "        -0.0192, -0.0408, -0.0132, -2.5982, -1.0197, -0.0185, -0.0113, -0.1259,\n",
      "        -0.1604], grad_fn=<StackBackward>)\n",
      "distributions.shape:  torch.Size([1, 121, 4])\n",
      "distributions:  tensor([[[3.6791e-03, 3.4031e-01, 4.7376e-03, 6.5127e-01],\n",
      "         [2.6250e-03, 4.6370e-02, 3.7773e-03, 9.4723e-01],\n",
      "         [1.4055e-03, 1.0258e-01, 9.5325e-03, 8.8649e-01],\n",
      "         [9.7571e-03, 3.3184e-01, 3.3454e-02, 6.2495e-01],\n",
      "         [1.5174e-03, 5.3359e-02, 4.1919e-03, 9.4093e-01],\n",
      "         [1.9244e-02, 2.6618e-01, 5.7898e-02, 6.5667e-01],\n",
      "         [3.4969e-03, 2.8265e-01, 6.3262e-02, 6.5059e-01],\n",
      "         [7.8251e-03, 3.5654e-01, 3.1777e-02, 6.0386e-01],\n",
      "         [2.6220e-04, 1.5005e-02, 4.9477e-04, 9.8424e-01],\n",
      "         [8.2498e-03, 5.4780e-01, 1.3906e-02, 4.3005e-01],\n",
      "         [9.9993e-03, 5.6196e-01, 4.3328e-02, 3.8471e-01],\n",
      "         [4.8040e-03, 1.2657e-01, 1.5996e-03, 8.6703e-01],\n",
      "         [5.3761e-03, 1.1482e-01, 5.6687e-03, 8.7414e-01],\n",
      "         [1.1672e-02, 1.4758e-01, 1.4615e-02, 8.2613e-01],\n",
      "         [1.9577e-03, 2.1893e-01, 3.0996e-02, 7.4812e-01],\n",
      "         [3.3590e-03, 1.0936e-01, 2.5694e-02, 8.6159e-01],\n",
      "         [6.3147e-04, 1.8182e-02, 4.1588e-03, 9.7703e-01],\n",
      "         [2.2821e-03, 1.0881e-01, 1.7469e-02, 8.7144e-01],\n",
      "         [5.9506e-03, 1.3247e-01, 2.9422e-02, 8.3216e-01],\n",
      "         [1.4652e-03, 5.7447e-02, 3.6922e-03, 9.3740e-01],\n",
      "         [5.3076e-03, 7.8050e-01, 3.0657e-02, 1.8353e-01],\n",
      "         [6.2029e-04, 1.9834e-02, 1.1147e-03, 9.7843e-01],\n",
      "         [4.0143e-03, 5.3655e-01, 4.1268e-02, 4.1817e-01],\n",
      "         [1.2574e-03, 1.1774e-02, 6.4017e-03, 9.8057e-01],\n",
      "         [6.1465e-03, 4.6908e-02, 9.2683e-03, 9.3768e-01],\n",
      "         [1.0298e-03, 3.4650e-02, 4.5741e-03, 9.5975e-01],\n",
      "         [6.6674e-03, 2.0298e-01, 3.5654e-02, 7.5470e-01],\n",
      "         [2.5902e-04, 2.1954e-02, 2.3072e-03, 9.7548e-01],\n",
      "         [1.4082e-03, 5.4875e-02, 2.3881e-03, 9.4133e-01],\n",
      "         [5.4458e-03, 3.2296e-01, 1.0626e-01, 5.6534e-01],\n",
      "         [2.2945e-03, 3.2889e-01, 1.3308e-02, 6.5550e-01],\n",
      "         [3.2946e-03, 4.6609e-02, 5.5702e-03, 9.4453e-01],\n",
      "         [1.0030e-02, 1.5187e-01, 3.8555e-02, 7.9955e-01],\n",
      "         [8.3932e-03, 3.5341e-01, 2.9536e-02, 6.0866e-01],\n",
      "         [1.3769e-03, 4.8430e-02, 1.2645e-03, 9.4893e-01],\n",
      "         [4.4938e-02, 5.7315e-01, 5.8878e-02, 3.2303e-01],\n",
      "         [5.5452e-04, 9.1038e-03, 2.8262e-04, 9.9006e-01],\n",
      "         [2.0525e-02, 4.1382e-01, 1.7432e-02, 5.4822e-01],\n",
      "         [5.0580e-03, 1.5012e-01, 8.7479e-03, 8.3607e-01],\n",
      "         [2.0751e-03, 2.5439e-01, 2.2495e-03, 7.4129e-01],\n",
      "         [3.3480e-03, 1.1604e-01, 4.3781e-03, 8.7623e-01],\n",
      "         [1.3013e-02, 1.5610e-01, 3.8097e-02, 7.9279e-01],\n",
      "         [2.4039e-04, 1.7188e-02, 1.3065e-03, 9.8126e-01],\n",
      "         [2.7106e-04, 9.0370e-03, 9.9668e-04, 9.8970e-01],\n",
      "         [1.0877e-02, 4.6412e-01, 6.8563e-02, 4.5644e-01],\n",
      "         [5.8477e-03, 6.1137e-01, 6.9394e-02, 3.1339e-01],\n",
      "         [5.0303e-04, 2.6796e-02, 1.6788e-03, 9.7102e-01],\n",
      "         [3.9256e-03, 1.6028e-01, 5.9544e-02, 7.7625e-01],\n",
      "         [1.4020e-03, 7.3857e-02, 4.7556e-03, 9.1999e-01],\n",
      "         [1.8835e-03, 1.8859e-02, 6.0340e-03, 9.7322e-01],\n",
      "         [2.4542e-03, 9.7498e-02, 3.5757e-02, 8.6429e-01],\n",
      "         [4.5698e-03, 1.3652e-01, 6.3924e-03, 8.5252e-01],\n",
      "         [2.3614e-03, 3.0209e-02, 4.2004e-03, 9.6323e-01],\n",
      "         [2.9555e-03, 8.0072e-01, 2.3671e-02, 1.7265e-01],\n",
      "         [7.2327e-03, 5.6063e-02, 3.6439e-02, 9.0027e-01],\n",
      "         [1.9572e-02, 3.8597e-01, 3.1691e-02, 5.6277e-01],\n",
      "         [3.3621e-03, 4.1880e-02, 3.2384e-02, 9.2237e-01],\n",
      "         [1.1672e-03, 8.5409e-02, 6.4575e-03, 9.0697e-01],\n",
      "         [3.8957e-03, 2.0308e-01, 7.7832e-03, 7.8524e-01],\n",
      "         [6.8240e-03, 3.6207e-01, 3.4311e-02, 5.9680e-01],\n",
      "         [7.0265e-03, 6.0720e-02, 2.1820e-02, 9.1043e-01],\n",
      "         [5.2490e-03, 7.6829e-02, 1.4818e-02, 9.0310e-01],\n",
      "         [2.2168e-03, 1.1359e-02, 2.4044e-03, 9.8402e-01],\n",
      "         [7.9329e-03, 7.4361e-02, 2.5520e-02, 8.9219e-01],\n",
      "         [8.1977e-04, 1.1957e-02, 2.2534e-03, 9.8497e-01],\n",
      "         [2.1240e-03, 9.1756e-02, 8.8625e-03, 8.9726e-01],\n",
      "         [1.2899e-02, 3.9984e-01, 1.3800e-02, 5.7346e-01],\n",
      "         [1.5511e-03, 2.7560e-02, 1.2952e-03, 9.6959e-01],\n",
      "         [1.2172e-04, 7.4732e-03, 4.5318e-04, 9.9195e-01],\n",
      "         [3.2615e-03, 5.2004e-01, 2.3844e-02, 4.5285e-01],\n",
      "         [5.7213e-04, 9.2114e-03, 3.8318e-03, 9.8638e-01],\n",
      "         [1.7412e-03, 1.0909e-01, 6.9697e-03, 8.8220e-01],\n",
      "         [6.6973e-04, 2.1938e-02, 2.4518e-03, 9.7494e-01],\n",
      "         [4.0852e-03, 4.9671e-01, 9.3172e-02, 4.0603e-01],\n",
      "         [2.9921e-03, 6.8066e-02, 1.7337e-02, 9.1161e-01],\n",
      "         [1.8737e-04, 1.9377e-02, 2.3965e-03, 9.7804e-01],\n",
      "         [1.4766e-03, 5.2729e-01, 2.5760e-02, 4.4547e-01],\n",
      "         [1.8897e-02, 1.8059e-01, 6.0329e-02, 7.4018e-01],\n",
      "         [9.5956e-04, 1.4258e-01, 2.3221e-03, 8.5414e-01],\n",
      "         [5.5696e-04, 6.4392e-02, 7.7853e-04, 9.3427e-01],\n",
      "         [8.5592e-03, 4.8919e-02, 2.3547e-02, 9.1897e-01],\n",
      "         [1.3349e-02, 2.0943e-01, 3.9708e-02, 7.3751e-01],\n",
      "         [1.4736e-03, 7.3629e-02, 5.0108e-03, 9.1989e-01],\n",
      "         [2.0772e-04, 7.7542e-03, 4.1247e-04, 9.9163e-01],\n",
      "         [3.5520e-03, 8.7107e-02, 1.2854e-02, 8.9649e-01],\n",
      "         [1.1375e-03, 1.0276e-01, 5.5130e-03, 8.9059e-01],\n",
      "         [3.9512e-03, 3.0987e-01, 9.0304e-03, 6.7715e-01],\n",
      "         [1.0389e-02, 4.6471e-01, 3.2996e-02, 4.9191e-01],\n",
      "         [2.3474e-04, 1.0341e-02, 1.4102e-03, 9.8801e-01],\n",
      "         [3.5297e-03, 2.4506e-01, 1.1230e-02, 7.4018e-01],\n",
      "         [6.4194e-03, 8.2010e-02, 4.7753e-03, 9.0680e-01],\n",
      "         [9.4960e-04, 9.8410e-02, 1.2213e-02, 8.8843e-01],\n",
      "         [2.0701e-03, 7.1891e-01, 1.8520e-02, 2.6050e-01],\n",
      "         [5.2617e-04, 2.0800e-02, 7.7988e-04, 9.7789e-01],\n",
      "         [1.1964e-02, 4.9391e-01, 1.4587e-02, 4.7954e-01],\n",
      "         [6.2294e-03, 4.9856e-01, 3.6617e-02, 4.5859e-01],\n",
      "         [2.0252e-03, 3.8378e-02, 2.3651e-02, 9.3595e-01],\n",
      "         [6.2958e-03, 3.2902e-02, 1.3656e-02, 9.4715e-01],\n",
      "         [5.1317e-03, 3.8355e-01, 1.8879e-02, 5.9244e-01],\n",
      "         [7.2768e-03, 2.8344e-01, 5.0311e-02, 6.5897e-01],\n",
      "         [8.3047e-04, 4.8261e-02, 7.6232e-03, 9.4329e-01],\n",
      "         [3.1858e-03, 2.0046e-02, 7.3381e-03, 9.6943e-01],\n",
      "         [2.5902e-04, 2.1892e-02, 1.2982e-03, 9.7655e-01],\n",
      "         [5.3613e-03, 6.3145e-01, 4.7534e-02, 3.1566e-01],\n",
      "         [2.1752e-03, 7.4766e-02, 4.3992e-03, 9.1866e-01],\n",
      "         [7.5056e-03, 1.7944e-01, 2.4133e-02, 7.8892e-01],\n",
      "         [4.8003e-03, 3.2987e-01, 3.2765e-02, 6.3256e-01],\n",
      "         [6.4369e-04, 8.9791e-02, 3.7222e-03, 9.0584e-01],\n",
      "         [1.1652e-04, 8.7800e-03, 7.2739e-04, 9.9038e-01],\n",
      "         [1.0977e-02, 7.1062e-02, 3.6586e-03, 9.1430e-01],\n",
      "         [1.6663e-03, 2.8959e-02, 3.0431e-03, 9.6633e-01],\n",
      "         [3.1288e-03, 2.6015e-02, 2.9124e-03, 9.6794e-01],\n",
      "         [6.8157e-04, 1.7038e-02, 1.3248e-03, 9.8096e-01],\n",
      "         [4.8516e-04, 3.8205e-02, 1.2663e-03, 9.6004e-01],\n",
      "         [1.3122e-03, 8.7179e-03, 3.0473e-03, 9.8692e-01],\n",
      "         [3.8960e-03, 7.4408e-02, 1.8819e-03, 9.1981e-01],\n",
      "         [2.0925e-03, 3.6069e-01, 6.6125e-02, 5.7109e-01],\n",
      "         [4.9833e-04, 1.5636e-02, 2.2333e-03, 9.8163e-01],\n",
      "         [3.6531e-04, 1.0152e-02, 6.7507e-04, 9.8881e-01],\n",
      "         [9.1934e-03, 1.0521e-01, 3.9331e-03, 8.8167e-01],\n",
      "         [8.0066e-03, 8.2381e-02, 5.7825e-02, 8.5179e-01]]],\n",
      "       grad_fn=<StackBackward>)\n",
      "Updating critic...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V_trg.shape (after critic):  torch.Size([121])\n",
      "V_trg.shape (after sum):  torch.Size([121])\n",
      "V_trg.shape (after squeeze):  torch.Size([121])\n",
      "V_trg.shape (after squeeze):  tensor([0.7212, 0.7206, 1.1046, 0.8684, 0.9098, 0.6617, 0.4136, 0.1305, 0.4089,\n",
      "        0.7743, 0.7112, 0.7051, 0.3978, 0.6012, 0.8845, 0.8841, 0.5931, 0.8899,\n",
      "        0.6572, 0.5082, 0.6362, 1.2112, 0.6973, 0.7983, 0.5948, 0.8058, 1.0111,\n",
      "        0.5564, 0.5529, 0.5691, 0.6187, 0.3627, 0.8247, 0.7986, 0.7298, 0.6381,\n",
      "        0.8043, 0.4672, 0.9362, 1.0432, 0.8461, 0.8355, 0.7464, 0.7489, 0.9878,\n",
      "        0.5901, 0.5978, 0.7857, 1.1877, 0.8350, 0.6563, 0.8072, 0.8075, 0.5594,\n",
      "        0.7860, 1.1396, 0.3381, 1.1596, 0.8587, 0.8311, 0.7036, 0.6395, 0.9760,\n",
      "        1.0463, 0.7925, 0.5898, 0.8675, 0.7353, 0.3403, 0.8559, 0.4525, 0.9463,\n",
      "        0.6892, 0.8039, 0.4847, 0.6992, 0.4918, 1.1734, 0.9529, 1.2067, 1.0017,\n",
      "        1.1834, 1.1458, 0.4891, 0.9409, 1.0515, 0.9805, 0.6454, 0.9403, 1.2870,\n",
      "        0.8040, 0.9034, 0.9217, 0.5853, 1.1174, 1.1460, 1.2083, 0.7813, 0.1892,\n",
      "        1.0638, 0.4040, 0.8376, 1.0366, 1.1341, 0.8047, 1.1448, 0.7878, 1.1648,\n",
      "        0.9271, 1.5086, 0.4642, 1.0885, 1.1044, 1.2318, 1.2850, 1.2604, 1.5537,\n",
      "        1.0921, 0.9084, 1.0016, 1.0211])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/Nicola_unipd/MasterThesis/RelationalDeepRL/RelationalModule/train_agent.py\u001b[0m in \u001b[0;36mtrain_boxworld\u001b[0;34m(agent, game_params, n_episodes, max_steps, return_agent, mask)\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;31m#print(\"Episode %d - reward: %.0f\"%(e+1, performance[-1]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistributions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbootstrap\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m         \u001b[0mt2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;31m#print(\"Time updating the agent: %.2f s\"%(t2-t1))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Nicola_unipd/MasterThesis/RelationalDeepRL/RelationalModule/ActorCritic.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTD\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0mcritic_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactor_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_TD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0mcritic_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactor_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_MC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Nicola_unipd/MasterThesis/RelationalDeepRL/RelationalModule/ActorCritic.py\u001b[0m in \u001b[0;36mupdate_TD\u001b[0;34m(self, rewards, log_probs, distributions, states, done, bootstrap)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;31m### Update critic and then actor ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0mcritic_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_critic_TD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_step_rewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGamma_V\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m         \u001b[0mactor_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_actor_TD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_step_rewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistributions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGamma_V\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Nicola_unipd/MasterThesis/RelationalDeepRL/RelationalModule/ActorCritic.py\u001b[0m in \u001b[0;36mupdate_critic_TD\u001b[0;34m(self, n_step_rewards, new_states, old_states, done, Gamma_V)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtwin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m             \u001b[0mV1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"V1.shape: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Nicola_unipd/MasterThesis/RelationalDeepRL/RelationalModule/AC_networks.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtwin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0mv1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0mv2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Nicola_unipd/MasterThesis/RelationalDeepRL/RelationalModule/AC_networks.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboxnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0mV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Nicola_unipd/MasterThesis/RelationalDeepRL/RelationalModule/RelationalNetworks.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"x.shape (BoxWorldNet): \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Nicola_unipd/MasterThesis/RelationalDeepRL/RelationalModule/RelationalNetworks.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;34m\"\"\"Expects an input of shape (batch_size, n_pixels, n_kernels)\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"x.shape (RelationalModule): \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Nicola_unipd/MasterThesis/RelationalDeepRL/RelationalModule/RelationalNetworks.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m    127\u001b[0m         \"\"\"\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m         \u001b[0mattn_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_output_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# MHA step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0mx_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_output\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# add and norm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_norm\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# FF step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.7/site-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask)\u001b[0m\n\u001b[1;32m    817\u001b[0m                 \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m                 \u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneed_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mneed_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m                 attn_mask=attn_mask)\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v)\u001b[0m\n\u001b[1;32m   3296\u001b[0m             \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_proj_weight_non_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_proj_bias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3297\u001b[0m             \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_proj_weight_non_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_proj_bias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3298\u001b[0;31m     \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mscaling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3300\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbias_k\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias_v\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results = train.train_boxworld(agent, game_params, n_episodes=5000, \n",
    "                               max_steps=game_params['max_num_steps'], return_agent=True, mask=True)\n",
    "score, asymptotic_score, asymptotic_std, trained_agent, time_profile = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rkdAxrzyfTmG"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "n_epochs = np.arange(100, len(score))\n",
    "average_score = np.array([np.mean(score[i:i+100]) for i in range(len(score)-100)])\n",
    "plt.plot(n_epochs, average_score, alpha=0.9)\n",
    "plt.title(\"Performance\", fontsize=16)\n",
    "plt.xlabel(\"Number of epochs\", fontsize=16)\n",
    "plt.ylabel(\"Total reward\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HRZNea0rKVE_"
   },
   "outputs": [],
   "source": [
    "save = False\n",
    "keywords = ['relational', 'residual','unboxed_gem',str(len(control_score))+\"-episodes\",\"50-steps\"] # example\n",
    "\n",
    "if colab and save:\n",
    "    %cd ~\n",
    "    parent_dir = \"/content/gdrive/My Drive/Colab Notebooks/\"\n",
    "    save_dir  = \"RelationalTrained/\"\n",
    "    %cd \"{parent_dir}\"\n",
    "    !mkdir \"{save_dir}\"\n",
    "    ID = utils.save_session(save_dir, keywords, game_params, HPs, score)\n",
    "    torch.save(trained_agent, save_dir+\"agent_\"+ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dluPwuvZcAkI"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "BoxWorldTesting.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
