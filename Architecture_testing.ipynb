{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relational Deep Reinforcement Learning\n",
    "\n",
    "**Plan:**\n",
    "1. Architecture\n",
    "2. Agent\n",
    "3. Environment\n",
    "4. Training cycle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relational architecture\n",
    "\n",
    "**Input: (b,n,n,1)** = (batch length, linear size, linear size, greyscale)\n",
    "\n",
    "**Extract entities: (b,n,n,1) -> (b, m, m, 2k)** \n",
    "* embedding layer: vocab_size = MAX_PIXELS+1, embedding_dim = n_dim\n",
    "* convolutional_layer1(kernel_size = (2,2), input_filters = n_dim, output_filters = k, stride = 1, pad = (1,1))\n",
    "* convolutional_layer2(kernel_size = (2,2), input_filters = k, output_filters = 2k, stride = 1, pad = (1,1))\n",
    "\n",
    "**Relational block: (b, m, m, 2k) -> (b,d_m)**\n",
    "* Positional Encoding: (b, m, m, 2k) -> (b, m^2, d_m)\n",
    "* N Multi-Headed Attention blocks: (b, m^2, d_m) -> (b, m^2, d_m)\n",
    "\n",
    "**Feature-wise max pooling: (b, m^2, d_m) -> (b, d_m)**\n",
    "\n",
    "**Multi-Layer Perceptron: (b, d_m) -> (b, d_m)**\n",
    "* 4 fully connected layers (d_m,d_m) with ReLUs (TODO: add skip-connections)\n",
    "\n",
    "**Actor output: (b,d_m) -> (b,a)** [a = number of possible actions]\n",
    "* Single linear layer with softmax at the end\n",
    "\n",
    "**Critic output: (b,d_m) -> (b,1)** \n",
    "* Single linear layer without activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Control architecture\n",
    "\n",
    "**Input: (b,n,n,1)** = (batch length, linear size, linear size, greyscale)\n",
    "\n",
    "**Extract entities: (b,n,n,1) -> (b, m, m, 2k)** \n",
    "* embedding layer: vocab_size = MAX_PIXELS+1, embedding_dim = n_dim\n",
    "* convolutional_layer1(kernel_size = (2,2), input_filters = n_dim, output_filters = k, stride = 1, pad = (1,1))\n",
    "* convolutional_layer2(kernel_size = (2,2), input_filters = k, output_filters = 2k, stride = 1, pad = (1,1))\n",
    "\n",
    "**1D Convolutional block: (b, m, m, 2k) -> (b, m^2, d_m)**\n",
    "* Positional Encoding: (b, m, m, 2k) -> (b, m^2, d_m)\n",
    "* 2 1D convolutional blocks with ReLUs: (b, m^2, d_m) -> (b, m^2, d_m) - pixel-wise\n",
    "\n",
    "**Feature-wise max pooling: (b, m^2, d_m) -> (b, d_m)**\n",
    "\n",
    "**Multi-Layer Perceptron: (b, d_m) -> (b, d_m)**\n",
    "* 4 fully connected layers (d_m,d_m) with ReLUs - feature-wise\n",
    "* (TODO: add skip-connections)\n",
    "\n",
    "**Actor output: (b,d_m) -> (b,a)** [a = number of possible actions]\n",
    "* Single linear layer with softmax at the end\n",
    "\n",
    "**Critic output: (b,d_m) -> (b,1)** \n",
    "* Single linear layer without activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'ControlNetworks' from 'RelationalModule' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-a45fecd02d8c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mRelationalModule\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRelationalNetworks\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mrnet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mRelationalModule\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mControlNetworks\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcnet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'ControlNetworks' from 'RelationalModule' (unknown location)"
     ]
    }
   ],
   "source": [
    "from RelationalModule import RelationalNetworks as rnet\n",
    "from RelationalModule import ControlNetworks as cnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "reload(rnet)\n",
    "reload(cnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample image from the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"pycolab/pycolab/examples/research/box_world\")\n",
    "import box_world as bw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicola/anaconda3/envs/torch/lib/python3.7/site-packages/pycolab/ascii_art.py:318: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  art = np.vstack(np.fromstring(line, dtype=np.uint8) for line in art)\n"
     ]
    }
   ],
   "source": [
    "GRID_SIZE = 12\n",
    "game_params = dict(grid_size=GRID_SIZE,\n",
    "                solution_length=[2], # number of boxes to be opened to get the gem\n",
    "                num_forward = [1], # number of distractors\n",
    "                num_backward=[0], # just set to 0 for now\n",
    "                branch_length=1, # length of forward distractors\n",
    "                max_num_steps = 50\n",
    "               )\n",
    "game = bw.make_game(**game_params)\n",
    "\n",
    "observation, _, _ = game.its_showtime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############\n",
      "#            #\n",
      "#            #\n",
      "#        *P  #\n",
      "#  j         #\n",
      "#         .  #\n",
      "#            #\n",
      "#            #\n",
      "#        hJ  #\n",
      "#            #\n",
      "#      pJ    #\n",
      "#            #\n",
      "#            #\n",
      "##############\n"
     ]
    }
   ],
   "source": [
    "b = observation.board\n",
    "l = observation.layers\n",
    "def show_game_state(observation):\n",
    "    for row in observation.board: print(row.tostring().decode('ascii'))\n",
    "        \n",
    "show_game_state(observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can forget about the board and just use the layers' masks to build our custom state.\n",
    "Chars (symbols) -> object names -> object code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_color_dict(seed=100):\n",
    "    import string\n",
    "    alphabet = string.ascii_lowercase\n",
    "    np.random.seed(seed)\n",
    "    RGB_list = np.random.rand(len(alphabet),3)\n",
    "    color_dict = {}\n",
    "    for l, c in zip(alphabet,RGB_list):\n",
    "        color_dict[l] = c\n",
    "    np.random.seed(None)\n",
    "    \n",
    "    # add colors for 'agent' and  'gem'\n",
    "    color_dict['agent'] = np.array([1.,0.,0.])\n",
    "    color_dict['gem'] = np.array([1.,1.,1.])\n",
    "    \n",
    "    return color_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_dict = get_color_dict()\n",
    "object_dict = {'ground':0, 'wall':1, 'agent':2, 'gem':3, 'key':4, 'box':5}\n",
    "symbol_dict = {' ':'ground', '#':'wall', '.':'agent', '*':'gem'} # key and box can have any possible letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############\n",
      "#            #\n",
      "#            #\n",
      "#        *P  #\n",
      "#  j         #\n",
      "#         .  #\n",
      "#            #\n",
      "#            #\n",
      "#        hJ  #\n",
      "#            #\n",
      "#      pJ    #\n",
      "#            #\n",
      "#            #\n",
      "##############\n"
     ]
    }
   ],
   "source": [
    "def show_game_state(observation):\n",
    "    for row in observation.board: print(row.tostring().decode('ascii'))\n",
    "\n",
    "b = observation.board\n",
    "l = observation.layers        \n",
    "show_game_state(observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state(observation):\n",
    "    b = observation.board\n",
    "    l = observation.layers  \n",
    "    color_board = np.zeros(b.shape+(3,)).astype(float)\n",
    "    object_board = np.zeros(b.shape+(1,)).astype(int)\n",
    "\n",
    "    for symbol in l.keys():\n",
    "\n",
    "        # If alphabetic character\n",
    "        if symbol.isalpha():\n",
    "            # Paint the color board cells occupied accordingly\n",
    "            color_board[l[symbol]] = color_dict[symbol.lower()]\n",
    "\n",
    "            # Upper = box, lower = key\n",
    "            if symbol.isupper():\n",
    "                object_board[l[symbol]] = object_dict['box']\n",
    "            else:\n",
    "                object_board[l[symbol]] = object_dict['key']\n",
    "\n",
    "        else:\n",
    "            object_name = symbol_dict[symbol]\n",
    "\n",
    "            # Color assigned is [0,0,0] since it's not really a property of those objects\n",
    "            # Only agent and gem have colors mainly for plotting reasons\n",
    "            if object_name == 'agent':\n",
    "                color_board[l[symbol]] = color_dict['agent']\n",
    "            elif object_name == 'gem':\n",
    "                color_board[l[symbol]] = color_dict['gem']\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "            object_board[l[symbol]] = object_dict[object_name]\n",
    "            \n",
    "    return object_board, color_board\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_board, color_board = get_state(observation)\n",
    "state = (object_board, color_board)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ground': 0, 'wall': 1, 'agent': 2, 'gem': 3, 'key': 4, 'box': 5}\n",
      "[[1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      " [1 0 0 0 0 0 0 0 0 3 5 0 0 1]\n",
      " [1 0 0 4 0 0 0 0 0 0 0 0 0 1]\n",
      " [1 0 0 0 0 0 0 0 0 0 2 0 0 1]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      " [1 0 0 0 0 0 0 0 0 4 5 0 0 1]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      " [1 0 0 0 0 0 0 4 5 0 0 0 0 1]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print(object_dict)\n",
    "print(object_board.reshape(14,14)) # remove last channel for printing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fc22cba7810>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(color_board)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1.shape:  torch.Size([10, 14, 14, 1])\n",
      "x2.shape:  torch.Size([10, 14, 14, 3])\n"
     ]
    }
   ],
   "source": [
    "# just for simulating an episode - this is one of the outputs of play_episode\n",
    "states = [state for _ in range(10)]\n",
    "x1 = torch.LongTensor([s[0] for s in states])\n",
    "print(\"x1.shape: \", x1.shape)\n",
    "x2 = torch.tensor([s[1] for s in states]).float()\n",
    "print(\"x2.shape: \", x2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = [1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(object_dict)\n",
    "n_dim = 3\n",
    "embed = nn.Embedding(vocab_size, n_dim, padding_idx=0) # backgroud is ignored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1_embed.shape:  torch.Size([10, 14, 14, 3])\n",
      "x.shape:  torch.Size([10, 14, 14, 6])\n",
      "x.shape:  torch.Size([10, 6, 14, 14])\n"
     ]
    }
   ],
   "source": [
    "if len(x1.shape) < 4:\n",
    "    x1.unsqueeze(0)\n",
    "x1_embed = embed(x1)\n",
    "x1_embed = x1_embed.reshape(x1.shape[:3]+(-1,))\n",
    "print(\"x1_embed.shape: \", x1_embed.shape)\n",
    "x = torch.cat((x1_embed, x2), axis = 3)\n",
    "print('x.shape: ', x.shape)\n",
    "x = x.transpose(-1,1)\n",
    "x = x.transpose(-1,-2)\n",
    "print('x.shape: ', x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old part using observation.board as state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_zeros.shape:  torch.Size([10, 10, 3])\n",
      "tensor([[1.3823, 1.3823, 1.3823, 1.3823, 1.3823, 1.3823, 1.3823, 1.3823, 1.3823,\n",
      "         1.3823],\n",
      "        [1.3823, 1.3823, 1.3823, 1.3823, 1.3823, 1.3823, 1.3823, 1.3823, 1.3823,\n",
      "         1.3823],\n",
      "        [1.3823, 1.3823, 1.3823, 1.3823, 1.3823, 1.3823, 1.3823, 1.3823, 1.3823,\n",
      "         1.3823],\n",
      "        [1.3823, 1.3823, 1.3823, 1.3823, 1.3823, 1.3823, 1.3823, 1.3823, 1.3823,\n",
      "         1.3823],\n",
      "        [1.3823, 1.3823, 1.3823, 1.3823, 1.3823, 1.3823, 1.3823, 1.3823, 1.3823,\n",
      "         1.3823],\n",
      "        [1.3823, 1.3823, 1.3823, 1.3823, 1.3823, 1.3823, 1.3823, 1.3823, 1.3823,\n",
      "         1.3823],\n",
      "        [1.3823, 1.3823, 1.3823, 1.3823, 1.3823, 1.3823, 1.3823, 1.3823, 1.3823,\n",
      "         1.3823],\n",
      "        [1.3823, 1.3823, 1.3823, 1.3823, 1.3823, 1.3823, 1.3823, 1.3823, 1.3823,\n",
      "         1.3823],\n",
      "        [1.3823, 1.3823, 1.3823, 1.3823, 1.3823, 1.3823, 1.3823, 1.3823, 1.3823,\n",
      "         1.3823],\n",
      "        [1.3823, 1.3823, 1.3823, 1.3823, 1.3823, 1.3823, 1.3823, 1.3823, 1.3823,\n",
      "         1.3823]], grad_fn=<SelectBackward>)\n",
      "tensor([[1.0949, 1.0949, 1.0949, 1.0949, 1.0949, 1.0949, 1.0949, 1.0949, 1.0949,\n",
      "         1.0949],\n",
      "        [1.0949, 1.0949, 1.0949, 1.0949, 1.0949, 1.0949, 1.0949, 1.0949, 1.0949,\n",
      "         1.0949],\n",
      "        [1.0949, 1.0949, 1.0949, 1.0949, 1.0949, 1.0949, 1.0949, 1.0949, 1.0949,\n",
      "         1.0949],\n",
      "        [1.0949, 1.0949, 1.0949, 1.0949, 1.0949, 1.0949, 1.0949, 1.0949, 1.0949,\n",
      "         1.0949],\n",
      "        [1.0949, 1.0949, 1.0949, 1.0949, 1.0949, 1.0949, 1.0949, 1.0949, 1.0949,\n",
      "         1.0949],\n",
      "        [1.0949, 1.0949, 1.0949, 1.0949, 1.0949, 1.0949, 1.0949, 1.0949, 1.0949,\n",
      "         1.0949],\n",
      "        [1.0949, 1.0949, 1.0949, 1.0949, 1.0949, 1.0949, 1.0949, 1.0949, 1.0949,\n",
      "         1.0949],\n",
      "        [1.0949, 1.0949, 1.0949, 1.0949, 1.0949, 1.0949, 1.0949, 1.0949, 1.0949,\n",
      "         1.0949],\n",
      "        [1.0949, 1.0949, 1.0949, 1.0949, 1.0949, 1.0949, 1.0949, 1.0949, 1.0949,\n",
      "         1.0949],\n",
      "        [1.0949, 1.0949, 1.0949, 1.0949, 1.0949, 1.0949, 1.0949, 1.0949, 1.0949,\n",
      "         1.0949]], grad_fn=<SelectBackward>)\n",
      "tensor([[-1.3931, -1.3931, -1.3931, -1.3931, -1.3931, -1.3931, -1.3931, -1.3931,\n",
      "         -1.3931, -1.3931],\n",
      "        [-1.3931, -1.3931, -1.3931, -1.3931, -1.3931, -1.3931, -1.3931, -1.3931,\n",
      "         -1.3931, -1.3931],\n",
      "        [-1.3931, -1.3931, -1.3931, -1.3931, -1.3931, -1.3931, -1.3931, -1.3931,\n",
      "         -1.3931, -1.3931],\n",
      "        [-1.3931, -1.3931, -1.3931, -1.3931, -1.3931, -1.3931, -1.3931, -1.3931,\n",
      "         -1.3931, -1.3931],\n",
      "        [-1.3931, -1.3931, -1.3931, -1.3931, -1.3931, -1.3931, -1.3931, -1.3931,\n",
      "         -1.3931, -1.3931],\n",
      "        [-1.3931, -1.3931, -1.3931, -1.3931, -1.3931, -1.3931, -1.3931, -1.3931,\n",
      "         -1.3931, -1.3931],\n",
      "        [-1.3931, -1.3931, -1.3931, -1.3931, -1.3931, -1.3931, -1.3931, -1.3931,\n",
      "         -1.3931, -1.3931],\n",
      "        [-1.3931, -1.3931, -1.3931, -1.3931, -1.3931, -1.3931, -1.3931, -1.3931,\n",
      "         -1.3931, -1.3931],\n",
      "        [-1.3931, -1.3931, -1.3931, -1.3931, -1.3931, -1.3931, -1.3931, -1.3931,\n",
      "         -1.3931, -1.3931],\n",
      "        [-1.3931, -1.3931, -1.3931, -1.3931, -1.3931, -1.3931, -1.3931, -1.3931,\n",
      "         -1.3931, -1.3931]], grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "zeros = torch.ones((10,10), dtype=int)\n",
    "y_zeros = embed(zeros)\n",
    "print(\"y_zeros.shape: \", y_zeros.shape)\n",
    "print(y_zeros[:,:,0])\n",
    "print(y_zeros[:,:,1])\n",
    "print(y_zeros[:,:,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same integer values get mapped to same vectors, as it should be. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fc22c691610>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "eye = torch.eye(10, dtype=int)\n",
    "y_eye = embed(eye).detach()\n",
    "plt.imshow(y_eye.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again the embedding works, since each integer value is associated to a particular vector (in this case 3D vector, that can be represented as RGB color once clipped)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-90846a95051a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m#print(\"x.shape (before embed): \", x.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "for state in states:\n",
    "    x = state\n",
    "    if len(x.shape) <= 3:\n",
    "        x = x.unsqueeze(0)\n",
    "    #print(\"x.shape (before embed): \", x.shape)\n",
    "    x = embed(x)\n",
    "    #print(\"x.shape (after embed): \", x.shape)\n",
    "    #print(\"x.sum in slices: \", sum_slices(x,(0,1)))\n",
    "    x = x.transpose(-1,-3)\n",
    "    #print(\"x.shape: \", x.shape)\n",
    "    x = x.transpose(-1,-2).reshape(x.shape[0],-1,x.shape[-2],x.shape[-1]).squeeze()\n",
    "    #print(\"x.shape: \", x.shape)\n",
    "    ### This part is just for correct visualization after embedding ###\n",
    "    x = x.transpose(-1,0)\n",
    "    x = x.transpose(1,0)\n",
    "    #print(\"x.shape: \", x.shape)\n",
    "    x = x.detach().numpy()\n",
    "    M = x.max(axis=(0,1))\n",
    "    m= x.min(axis=(0,1))\n",
    "    #print(\"M: \", M.shape )\n",
    "    #print(\"m: \", m.shape)\n",
    "    x = (x - m)/(M-m)\n",
    "    plt.imshow(x)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given this simple rendering (after embedding) from naked eye we can only see that:\n",
    "1. keys have different colors from the boxes they should open\n",
    "2. distractors are identical to the right boxes\n",
    "\n",
    "Then we also have to notice that in this representation colors are biased because I had to normalize in [0,1] the \"RGB\" channels given by the embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_state = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_in = 1\n",
    "k_out = 24\n",
    "kernel_size = 2\n",
    "stride = 1\n",
    "padding = 0\n",
    "\n",
    "layers = []\n",
    "layers.append(nn.Conv2d(n_dim*k_in, k_out//2, kernel_size, stride, padding))\n",
    "layers.append(nn.ReLU())\n",
    "layers.append(nn.Conv2d(k_out//2, k_out, kernel_size, stride, padding))\n",
    "#layers.append(nn.ReLU())\n",
    "net = nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_slices(x, axes):\n",
    "    return x.squeeze().sum(axis=axes).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape (before embed):  torch.Size([1, 1, 14, 14])\n",
      "x.shape (after embed):  torch.Size([1, 1, 14, 14, 3])\n",
      "x.sum in slices:  tensor([0., 0., 0.])\n",
      "x.shape:  torch.Size([1, 1, 3, 14, 14])\n",
      "x.shape:  torch.Size([1, 3, 14, 14])\n",
      "y.shape:  torch.Size([1, 24, 12, 12])\n"
     ]
    }
   ],
   "source": [
    "if use_state:\n",
    "    x = states[0]\n",
    "else:\n",
    "    x = torch.zeros((1,14,14), dtype=int) # this is the structure of the state retrieved by the game\n",
    "if len(x.shape) <= 3:\n",
    "    x = x.unsqueeze(0)\n",
    "print(\"x.shape (before embed): \", x.shape)\n",
    "x = embed(x)\n",
    "print(\"x.shape (after embed): \", x.shape)\n",
    "print(\"x.sum in slices: \", sum_slices(x,(0,1)))\n",
    "x = x.transpose(-1,-3)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = x.transpose(-1,-2).reshape(x.shape[0],-1,x.shape[-2],x.shape[-1])\n",
    "print(\"x.shape: \", x.shape)\n",
    "y = net(x)\n",
    "print(\"y.shape: \", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ -0.6587, -14.0156,  17.7553,  -2.3413,  -2.9424,  29.2448,   1.5390,\n",
      "          4.7220,  30.8630,  25.3627,   7.3142,  15.8346,   2.0629,   2.1619,\n",
      "         -7.3758,  22.1776, -29.9065,  19.2171,  -4.8539,   6.5445,  -5.5534,\n",
      "          3.2113, -11.2617,  -1.8192])\n",
      "y[0,:,:]:  tensor([[-0.0046, -0.0046, -0.0046, -0.0046, -0.0046, -0.0046, -0.0046, -0.0046,\n",
      "         -0.0046, -0.0046, -0.0046, -0.0046],\n",
      "        [-0.0046, -0.0046, -0.0046, -0.0046, -0.0046, -0.0046, -0.0046, -0.0046,\n",
      "         -0.0046, -0.0046, -0.0046, -0.0046],\n",
      "        [-0.0046, -0.0046, -0.0046, -0.0046, -0.0046, -0.0046, -0.0046, -0.0046,\n",
      "         -0.0046, -0.0046, -0.0046, -0.0046],\n",
      "        [-0.0046, -0.0046, -0.0046, -0.0046, -0.0046, -0.0046, -0.0046, -0.0046,\n",
      "         -0.0046, -0.0046, -0.0046, -0.0046],\n",
      "        [-0.0046, -0.0046, -0.0046, -0.0046, -0.0046, -0.0046, -0.0046, -0.0046,\n",
      "         -0.0046, -0.0046, -0.0046, -0.0046],\n",
      "        [-0.0046, -0.0046, -0.0046, -0.0046, -0.0046, -0.0046, -0.0046, -0.0046,\n",
      "         -0.0046, -0.0046, -0.0046, -0.0046],\n",
      "        [-0.0046, -0.0046, -0.0046, -0.0046, -0.0046, -0.0046, -0.0046, -0.0046,\n",
      "         -0.0046, -0.0046, -0.0046, -0.0046],\n",
      "        [-0.0046, -0.0046, -0.0046, -0.0046, -0.0046, -0.0046, -0.0046, -0.0046,\n",
      "         -0.0046, -0.0046, -0.0046, -0.0046],\n",
      "        [-0.0046, -0.0046, -0.0046, -0.0046, -0.0046, -0.0046, -0.0046, -0.0046,\n",
      "         -0.0046, -0.0046, -0.0046, -0.0046],\n",
      "        [-0.0046, -0.0046, -0.0046, -0.0046, -0.0046, -0.0046, -0.0046, -0.0046,\n",
      "         -0.0046, -0.0046, -0.0046, -0.0046],\n",
      "        [-0.0046, -0.0046, -0.0046, -0.0046, -0.0046, -0.0046, -0.0046, -0.0046,\n",
      "         -0.0046, -0.0046, -0.0046, -0.0046],\n",
      "        [-0.0046, -0.0046, -0.0046, -0.0046, -0.0046, -0.0046, -0.0046, -0.0046,\n",
      "         -0.0046, -0.0046, -0.0046, -0.0046]])\n"
     ]
    }
   ],
   "source": [
    "y = y.squeeze().detach()\n",
    "print(sum_slices(y,(1,2)))\n",
    "print(\"y[0,:,:]: \", y[0,:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So basically each layer is uniform thanks to the input and how convolution works (each slice is the result of the convolution from the same kernel of the same input). All the zeros that can be seen are due to the ReLU activations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_encoding2D(x):\n",
    "    x_ax = x.shape[-2]\n",
    "    y_ax = x.shape[-1]\n",
    "\n",
    "    x_lin = torch.linspace(-1,1,x_ax)\n",
    "    xx = x_lin.repeat(x.shape[0],y_ax,1).view(-1, 1, y_ax, x_ax).transpose(3,2)\n",
    "\n",
    "    y_lin = torch.linspace(-1,1,y_ax).view(-1,1)\n",
    "    yy = y_lin.repeat(x.shape[0],1,x_ax).view(-1, 1, y_ax, x_ax).transpose(3,2)\n",
    "\n",
    "    x = torch.cat((x,xx,yy), axis=1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape (before embed):  torch.Size([1, 1, 14, 14])\n",
      "x.shape (after embed):  torch.Size([1, 1, 14, 14, 3])\n",
      "x.sum in slices:  tensor([0., 0., 0.])\n",
      "x.shape:  torch.Size([1, 1, 3, 14, 14])\n",
      "x.shape:  torch.Size([1, 3, 14, 14])\n",
      "y.shape:  torch.Size([1, 24, 12, 12])\n",
      "y_enc.shape:  torch.Size([1, 26, 12, 12])\n"
     ]
    }
   ],
   "source": [
    "if use_state:\n",
    "    x = states[0]\n",
    "else:\n",
    "    x = torch.zeros((1,14,14), dtype=int) # this is the structure of the state retrieved by the game\n",
    "if len(x.shape) <= 3:\n",
    "    x = x.unsqueeze(0)\n",
    "print(\"x.shape (before embed): \", x.shape)\n",
    "x = embed(x)\n",
    "print(\"x.shape (after embed): \", x.shape)\n",
    "print(\"x.sum in slices: \", sum_slices(x,(0,1)))\n",
    "x = x.transpose(-1,-3)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = x.transpose(-1,-2).reshape(x.shape[0],-1,x.shape[-2],x.shape[-1])\n",
    "print(\"x.shape: \", x.shape)\n",
    "y = net(x)\n",
    "print(\"y.shape: \", y.shape)\n",
    "y_enc = add_encoding2D(y)\n",
    "print(\"y_enc.shape: \", y_enc.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify that the last 2 layers have a positional encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_tmp.shape:  torch.Size([26, 12, 12])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_tmp = y_enc.squeeze().detach()\n",
    "print(\"y_tmp.shape: \", y_tmp.shape)\n",
    "plt.imshow(y_tmp[-2])\n",
    "plt.show()\n",
    "plt.imshow(y_tmp[-1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A different approach would be to sum these two layers pixel-wise to all other features. Probably it would amplify the importance of the position, at the risk that if the magnitude is too high we would lose data.\n",
    "\n",
    "Also more complicated encodings are possible; this one is the one I think they used in the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projection from 26 to n_features (default 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 256\n",
    "projection = nn.Linear(k_out + 2, n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape (before embed):  torch.Size([1, 1, 14, 14])\n",
      "x.shape (after embed):  torch.Size([1, 1, 14, 14, 3])\n",
      "x.sum in slices:  tensor([0., 0., 0.])\n",
      "x.shape:  torch.Size([1, 1, 3, 14, 14])\n",
      "x.shape:  torch.Size([1, 3, 14, 14])\n",
      "x.shape:  torch.Size([1, 24, 12, 12])\n",
      "x.shape:  torch.Size([1, 26, 12, 12])\n",
      "x.shape:  torch.Size([1, 26, 144])\n",
      "x.shape:  torch.Size([1, 144, 26])\n",
      "x.shape:  torch.Size([1, 144, 256])\n",
      "x.shape:  torch.Size([144, 1, 256])\n"
     ]
    }
   ],
   "source": [
    "if use_state:\n",
    "    x = states[0]\n",
    "else:\n",
    "    x = torch.zeros((1,14,14), dtype=int) # this is the structure of the state retrieved by the game\n",
    "if len(x.shape) <= 3:\n",
    "    x = x.unsqueeze(0)\n",
    "print(\"x.shape (before embed): \", x.shape)\n",
    "x = embed(x)\n",
    "print(\"x.shape (after embed): \", x.shape)\n",
    "print(\"x.sum in slices: \", sum_slices(x,(0,1)))\n",
    "x = x.transpose(-1,-3)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = x.transpose(-1,-2).reshape(x.shape[0],-1,x.shape[-2],x.shape[-1])\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = net(x)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x= add_encoding2D(x)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = x.view(x.shape[0], x.shape[1],-1)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = x.transpose(2,1)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = projection(x)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = x.transpose(1,0)\n",
    "print(\"x.shape: \", x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here everything seems clean. Let's see if there is some trace of the positional encoding left. Ideally thanks to the projection now each feature potentially has a positional encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_tmp = x.squeeze().detach().view(12,12,256)\n",
    "plt.imshow(x_tmp[:,:,0])\n",
    "plt.show()\n",
    "plt.imshow(x_tmp[:,:,128])\n",
    "plt.show()\n",
    "plt.imshow(x_tmp[:,:,-2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again everything seems fine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relational Block \n",
    "Implements the relational block, composed by a Multi-Headed Dot-Product Attention layer followed by a Position-wise Feed-Forward layer. I implement here the former one, whereas I just import the latter from the module, since it's very basic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 64\n",
    "dropout = 0\n",
    "n_heads = 4\n",
    "\n",
    "norm = nn.LayerNorm(n_features)\n",
    "drop = nn.Dropout(dropout) # disabled\n",
    "attn = nn.MultiheadAttention(n_features, n_heads, dropout)\n",
    "ff = rnet.PositionwiseFeedForward(n_features, hidden_dim, dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if use_state:\n",
    "    x = states[0]\n",
    "else:\n",
    "    x = torch.zeros((1,14,14), dtype=int) # this is the structure of the state retrieved by the game\n",
    "if len(x.shape) <= 3:\n",
    "    x = x.unsqueeze(0)\n",
    "print(\"x.shape (before embed): \", x.shape)\n",
    "x = embed(x)\n",
    "print(\"x.shape (after embed): \", x.shape)\n",
    "print(\"x.sum in slices: \", sum_slices(x,(0,1)))\n",
    "x = x.transpose(-1,-3)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = x.transpose(-1,-2).reshape(x.shape[0],-1,x.shape[-2],x.shape[-1])\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = net(x)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x= add_encoding2D(x)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = x.view(x.shape[0], x.shape[1],-1)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = x.transpose(2,1)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = projection(x)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = x.transpose(1,0)\n",
    "print(\"x.shape: \", x.shape)\n",
    "\n",
    "x_tmp = x # save it for plotting\n",
    "\n",
    "# From here it has always the same shape\n",
    "attn_output, attn_output_weights =  attn(x,x,x, key_padding_mask=None) # MHA step\n",
    "print(\"attn_output.shape: \", attn_output.shape)\n",
    "x_add = attn_output + x\n",
    "print(\"x_add.shape: \", x_add.shape)\n",
    "x_norm = drop(norm(x_add))\n",
    "print(\"x_norm.shape: \", x_norm.shape)\n",
    "x_ff = ff(x_norm)\n",
    "print(\"x_ff.shape: \", x_ff.shape)\n",
    "out = drop(norm(x_ff))\n",
    "print(\"out.shape: \", out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_layer(x, layer=0):\n",
    "    x = x.squeeze().detach()[:,layer]\n",
    "    plt.imshow(x.view(12,12))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Input\")\n",
    "plot_layer(x_tmp)\n",
    "print(\"Attention output\")\n",
    "plot_layer(attn_output)\n",
    "print(\"Input + attention\")\n",
    "plot_layer(x_add)\n",
    "print(\"After LayerNorm\")\n",
    "plot_layer(x_norm)\n",
    "print(\"After position-wise FF\")\n",
    "plot_layer(x_ff)\n",
    "print(\"After LayerNorm\")\n",
    "plot_layer(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we didn't see anything suspicious, with the attention layer not doing much and the only real change happening during the positionwise feed forward, in which we make a convolution of the 256 features to obtain new ones, so of course after that we are looking at a different feature plane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see if the attention layer is working correctly, we can play with the pixel and batch asix and see if the result is affected by those changes. Since the attention mechanism works taking into account relations between pixels, masking some of them should change the output for the others. Instead the batch dimension shouldn't intefrere with the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if batch size changes the output\n",
    "if use_state:\n",
    "    x = states[0]\n",
    "else:\n",
    "    x = torch.zeros((1,14,14), dtype=int) # this is the structure of the state retrieved by the game\n",
    "if len(x.shape) <= 3:\n",
    "    x = x.unsqueeze(0)\n",
    "print(\"x.shape (before embed): \", x.shape)\n",
    "x = embed(x)\n",
    "print(\"x.shape (after embed): \", x.shape)\n",
    "print(\"x.sum in slices: \", sum_slices(x,(0,1)))\n",
    "x = x.transpose(-1,-3)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = x.transpose(-1,-2).reshape(x.shape[0],-1,x.shape[-2],x.shape[-1])\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = net(x)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x= add_encoding2D(x)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = x.view(x.shape[0], x.shape[1],-1)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = x.transpose(2,1)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = projection(x)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = x.transpose(1,0)\n",
    "print(\"x.shape: \", x.shape)\n",
    "\n",
    "x_tmp2 = torch.cat((x,x), axis=1)\n",
    "attn_output, attn_output_weights =  attn(x_tmp2,x_tmp2,x_tmp2, key_padding_mask=None) # MHA step\n",
    "print(\"attn_output.shape: \", attn_output.shape)\n",
    "x_add = attn_output + x\n",
    "print(\"x_add.shape: \", x_add.shape)\n",
    "x_norm = drop(norm(x_add))\n",
    "print(\"x_norm.shape: \", x_norm.shape)\n",
    "x_ff = ff(x_norm)\n",
    "print(\"x_ff.shape: \", x_ff.shape)\n",
    "out2 = drop(norm(x_ff))\n",
    "out3 = out2[:,1,:].unsqueeze(1)\n",
    "out2 = out2[:,0,:].unsqueeze(1)\n",
    "\n",
    "print(\"out2.shape: \", out2.shape)\n",
    "print(\"out2: \", out2)\n",
    "print(\"out: \", out)\n",
    "print(\"Element sum of the difference 2-3: \", torch.sum(out2- out3).item())\n",
    "print(\"Element sum of the difference 2-0: \", torch.sum(out2- out).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "out2 = out3 means that the batch dimension correctly hasn't changed the result, because the same input has been concatenated along that axes and the same two outputs have been obtained on the other end. out2 = out shows that different samples are handled independently, as it should happen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if pixel sequence changes the output\n",
    "if use_state:\n",
    "    x = states[0]\n",
    "else:\n",
    "    x = torch.zeros((1,14,14), dtype=int) # this is the structure of the state retrieved by the game\n",
    "if len(x.shape) <= 3:\n",
    "    x = x.unsqueeze(0)\n",
    "print(\"x.shape (before embed): \", x.shape)\n",
    "x = embed(x)\n",
    "print(\"x.shape (after embed): \", x.shape)\n",
    "print(\"x.sum in slices: \", sum_slices(x,(0,1)))\n",
    "x = x.transpose(-1,-3)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = x.transpose(-1,-2).reshape(x.shape[0],-1,x.shape[-2],x.shape[-1])\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = net(x)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x= add_encoding2D(x)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = x.view(x.shape[0], x.shape[1],-1)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = x.transpose(2,1)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = projection(x)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = x.transpose(1,0)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x_tmp1 = x\n",
    "x_tmp1[100:] = 0. # mask last 44 positions\n",
    "attn_output, attn_output_weights =  attn(x_tmp1,x_tmp1,x_tmp1, key_padding_mask=None) # MHA step\n",
    "print(\"attn_output.shape: \", attn_output.shape)\n",
    "x_add = attn_output + x\n",
    "print(\"x_add.shape: \", x_add.shape)\n",
    "x_norm = drop(norm(x_add))\n",
    "print(\"x_norm.shape: \", x_norm.shape)\n",
    "x_ff = ff(x_norm)\n",
    "print(\"x_ff.shape: \", x_ff.shape)\n",
    "out1 = drop(norm(x_ff))\n",
    "\n",
    "print(\"out1: \", out1)\n",
    "print(\"out: \", out)\n",
    "print(\"Element sum of the difference: \", torch.sum(out1 - out).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, the new output has changed, as it should."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LayerNorm formula**\n",
    "$$y = \\frac{x - \\mathrm{E}[x]}{ \\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma + \\beta$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# What LayerNorm does\n",
    "\n",
    "E = x_add[0,0,:].mean()\n",
    "print(\"E: \", E)\n",
    "V = x_add[0,0,:].var()\n",
    "print(\"V: \", V)\n",
    "y = (x_add[0,0,:]-E)/torch.sqrt(V+1e-5)\n",
    "print(\"LayerNorm by hand: \\n\", y)\n",
    "print(\"LayerNorm: \\n\", x_norm[0,0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Featurewise MaxPooling\n",
    "\n",
    "For each feature, take the maximum value among the pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.zeros((1,14,14), dtype=int) # this is the structure of the state retrieved by the game\n",
    "if len(x.shape) <= 3:\n",
    "    x = x.unsqueeze(0)\n",
    "print(\"x.shape (before embed): \", x.shape)\n",
    "x = embed(x)\n",
    "print(\"x.shape (after embed): \", x.shape)\n",
    "print(\"x.sum in slices: \", sum_slices(x,(0,1)))\n",
    "x = x.transpose(-1,-3)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = x.transpose(-1,-2).reshape(x.shape[0],-1,x.shape[-2],x.shape[-1])\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = net(x)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x= add_encoding2D(x)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = x.view(x.shape[0], x.shape[1],-1)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = x.transpose(2,1)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = projection(x)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = x.transpose(1,0)\n",
    "print(\"x.shape: \", x.shape)\n",
    "\n",
    "x_tmp = x # save it for plotting\n",
    "\n",
    "# From here it has always the same shape\n",
    "attn_output, attn_output_weights =  attn(x,x,x, key_padding_mask=None) # MHA step\n",
    "print(\"attn_output.shape: \", attn_output.shape)\n",
    "x_add = attn_output + x\n",
    "print(\"x_add.shape: \", x_add.shape)\n",
    "x_norm = drop(norm(x_add))\n",
    "print(\"x_norm.shape: \", x_norm.shape)\n",
    "x_ff = ff(x_norm)\n",
    "print(\"x_ff.shape: \", x_ff.shape)\n",
    "x = drop(norm(x_ff))\n",
    "print(\"x.shape: \", x.shape)\n",
    "# Max pooling feature-wise\n",
    "x, _ = torch.max(x, axis=0)\n",
    "print(\"x.shape: \", x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing much to control here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternative to max pooling - Linear projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(rnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_proj = nn.Linear(144,1) # needs to know how many pixels there are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.zeros((1,1,14,14), dtype=int) # this is the structure of the state retrieved by the game\n",
    "if len(x.shape) <= 3:\n",
    "    x = x.unsqueeze(0)\n",
    "print(\"x.shape (before embed): \", x.shape)\n",
    "x = embed(x)\n",
    "print(\"x.shape (after embed): \", x.shape)\n",
    "print(\"x.sum in slices: \", sum_slices(x,(0,1)))\n",
    "x = x.transpose(-1,-3)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = x.transpose(-1,-2).reshape(x.shape[0],-1,x.shape[-2],x.shape[-1])\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = net(x)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x= add_encoding2D(x)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = x.view(x.shape[0], x.shape[1],-1)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = x.transpose(2,1)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = projection(x)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = x.transpose(1,0)\n",
    "print(\"x.shape: \", x.shape)\n",
    "\n",
    "x_tmp = x # save it for plotting\n",
    "\n",
    "# From here it has always the same shape\n",
    "attn_output, attn_output_weights =  attn(x,x,x, key_padding_mask=None) # MHA step\n",
    "print(\"attn_output.shape: \", attn_output.shape)\n",
    "x_add = attn_output + x\n",
    "print(\"x_add.shape: \", x_add.shape)\n",
    "x_norm = drop(norm(x_add))\n",
    "print(\"x_norm.shape: \", x_norm.shape)\n",
    "x_ff = ff(x_norm)\n",
    "print(\"x_ff.shape: \", x_ff.shape)\n",
    "x = drop(norm(x_ff))\n",
    "print(\"x.shape: \", x.shape)\n",
    "\n",
    "# Feature-wise projection\n",
    "x = x.transpose(-1,0)\n",
    "print(\"x.shape (before linear): \", x.shape)\n",
    "shape = x.shape\n",
    "x = linear_proj(x).reshape(shape[0],shape[1])\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = x.transpose(-1,0)\n",
    "print(\"x.shape: \", x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual Layer\n",
    "\n",
    "Here the original paper uses just a Multi-Layer Perceptron, but I thought it would be nice to have sone skip connections in order to make the architecture more flexible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(rnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hiddens = 256 \n",
    "residual_layer = rnet.ResidualLayer(n_features, n_hiddens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "residual_layer(x) - x # residual after ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'RelationalModule.RelationalNetworks' from '/home/nicola/Nicola_unipd/MasterThesis/RelationalDeepRL/RelationalModule/RelationalNetworks.py'>"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(rnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): RelationalModule(\n",
      "    (net): Sequential(\n",
      "      (0): PositionalEncoding(\n",
      "        (projection): Linear(in_features=26, out_features=256, bias=True)\n",
      "      )\n",
      "      (1): AttentionBlock(\n",
      "        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (ff): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=64, bias=True)\n",
      "          (w_2): Linear(in_features=64, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (2): AttentionBlock(\n",
      "        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (ff): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=64, bias=True)\n",
      "          (w_2): Linear(in_features=64, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (1): FeaturewiseMaxPool()\n",
      "  (2): ResidualLayer(\n",
      "    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (w1): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (w2): Linear(in_features=256, out_features=256, bias=True)\n",
      "  )\n",
      "  (3): ResidualLayer(\n",
      "    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (w1): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (w2): Linear(in_features=256, out_features=256, bias=True)\n",
      "  )\n",
      "  (4): ResidualLayer(\n",
      "    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (w1): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (w2): Linear(in_features=256, out_features=256, bias=True)\n",
      "  )\n",
      "  (5): ResidualLayer(\n",
      "    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (w1): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (w2): Linear(in_features=256, out_features=256, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "box_net = rnet.BoxWorldNet(in_channels=3, n_kernels=24, vocab_size=6, n_dim=3,\n",
    "                              n_features=256, n_attn_modules=2, n_linears=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): RelationalModule(\n",
      "    (net): Sequential(\n",
      "      (0): PositionalEncoding(\n",
      "        (projection): Linear(in_features=26, out_features=256, bias=True)\n",
      "      )\n",
      "      (1): AttentionBlock(\n",
      "        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (ff): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=64, bias=True)\n",
      "          (w_2): Linear(in_features=64, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (2): AttentionBlock(\n",
      "        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (ff): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=64, bias=True)\n",
      "          (w_2): Linear(in_features=64, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (1): FeaturewiseProjection(\n",
      "    (norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "    (projection): Linear(in_features=144, out_features=1, bias=True)\n",
      "  )\n",
      "  (2): ResidualLayer(\n",
      "    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (w1): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (w2): Linear(in_features=256, out_features=256, bias=True)\n",
      "  )\n",
      "  (3): ResidualLayer(\n",
      "    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (w1): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (w2): Linear(in_features=256, out_features=256, bias=True)\n",
      "  )\n",
      "  (4): ResidualLayer(\n",
      "    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (w1): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (w2): Linear(in_features=256, out_features=256, bias=True)\n",
      "  )\n",
      "  (5): ResidualLayer(\n",
      "    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (w1): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (w2): Linear(in_features=256, out_features=256, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "box_net_v1 = rnet.BoxWorldNet(in_channels=3, n_kernels=24, vocab_size=6, n_dim=3,\n",
    "                              n_features=256, n_attn_modules=2, n_linears=4, max_pool=False,\n",
    "                              linear_size=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([14, 14, 1])\n",
      "torch.Size([14, 14, 3])\n"
     ]
    }
   ],
   "source": [
    "x1 = torch.LongTensor(state[0])\n",
    "print(x1.shape)\n",
    "x2 = torch.tensor(state[1]).float()\n",
    "print(x2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1.shape (before ExtractEntities):  torch.Size([14, 14, 1])\n",
      "x2.shape (before ExtractEntities):  torch.Size([14, 14, 3])\n",
      "x.shape (after Embedding and reshape):  torch.Size([1, 6, 14, 14])\n",
      "x.shape (ExtractEntities):  torch.Size([1, 24, 12, 12])\n",
      "x.shape (After encoding):  torch.Size([1, 26, 12, 12])\n",
      "x.shape (Before transposing and projection):  torch.Size([1, 26, 144])\n",
      "x.shape (PositionalEncoding):  torch.Size([144, 1, 256])\n",
      "x.shape (RelationalModule):  torch.Size([144, 1, 256])\n",
      "x.shape (FeaturewiseProjection):  torch.Size([144, 1, 256])\n",
      "x.shape (FeaturewiseProjection):  torch.Size([256, 1, 144])\n",
      "x.shape (FeaturewiseProjection):  torch.Size([256, 1])\n",
      "x.shape (FeaturewiseProjection):  torch.Size([1, 256])\n",
      "x.shape (BoxWorldNet):  torch.Size([1, 256])\n",
      "y.shape:  torch.Size([1, 256])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0594,  1.1729,  0.9001,  0.4614, -1.3175,  0.0731,  0.9347, -0.9794,\n",
       "         -1.5173, -0.7390, -0.9998,  2.4710, -1.1093, -0.6915, -3.4846,  2.2579,\n",
       "         -1.9974, -0.5731, -0.4970, -0.1387,  1.4172,  0.6031,  0.1381,  1.5487,\n",
       "          0.4061, -0.1736, -0.5689, -0.6867, -1.1547,  0.6612, -0.0434,  0.1895,\n",
       "         -0.1609,  0.4308,  2.0075,  0.1477,  1.8875,  1.4404, -0.5629, -0.2460,\n",
       "          1.6132, -1.5495,  1.3293, -1.8583, -0.4135,  0.2229,  0.1548,  2.1788,\n",
       "         -0.4053,  1.1293,  0.6658,  0.8162,  3.4881,  0.3799,  2.8879,  0.2408,\n",
       "         -0.2668, -0.1129,  0.8771,  0.1817,  1.6333,  0.5355,  0.5702, -0.9400,\n",
       "         -0.8138, -0.2316,  0.2152, -2.4184, -1.0042, -0.3276, -0.2822,  1.0378,\n",
       "          1.4559, -0.2632,  2.6049,  1.5024,  1.4799, -1.2181,  0.9351, -1.7353,\n",
       "         -1.2603, -0.4981,  0.5729, -0.5147, -1.8516,  0.3851,  0.4077, -1.3375,\n",
       "          1.4091,  1.5234, -0.9242,  0.9547, -0.4658,  0.6823, -0.6142, -1.1469,\n",
       "         -1.4122,  0.5281,  1.3724, -1.7548, -0.2391, -0.8516,  1.8060, -0.1137,\n",
       "         -1.3639, -0.6103, -1.1044, -0.4112,  0.6626,  2.4084,  0.3815, -0.3091,\n",
       "          1.5008,  0.8576, -0.7875, -1.0062, -0.5292,  0.9780,  1.4716,  0.4395,\n",
       "         -0.4712,  1.3198, -0.3525, -0.9911,  0.9307, -1.2999,  0.8449, -1.2089,\n",
       "         -0.8410, -0.5941, -0.0397,  2.0397,  1.0871, -1.7894,  0.8869,  0.4383,\n",
       "         -0.1737,  1.1713,  0.8410,  0.9965,  0.3011, -2.1499,  0.2293,  0.6582,\n",
       "          0.8354,  0.6355,  0.5543,  0.0048, -0.3656,  0.2970,  0.7668, -1.9987,\n",
       "          0.0722, -0.4991,  1.5351,  0.4473,  0.0422,  0.0940, -1.8532, -1.4366,\n",
       "         -0.4296, -1.6004, -1.3151,  0.4036,  1.7561, -1.2755,  0.6525,  0.2279,\n",
       "         -0.1960, -0.3997,  0.5094,  0.7819,  0.4383,  0.5715,  0.5258, -0.3637,\n",
       "         -0.1222, -1.5624, -0.3792, -1.4540,  1.4918, -0.5330,  0.5449,  1.3594,\n",
       "          1.0556, -1.0754, -0.5877,  1.1049, -0.4777,  1.2080, -0.1692,  0.1146,\n",
       "         -2.6222, -0.4799, -1.5950, -0.2370, -0.8800,  1.4404,  1.2603,  0.8302,\n",
       "          0.1233,  0.1661,  0.0079, -0.3477,  0.7621,  0.3748,  2.6161, -0.2069,\n",
       "          0.4668, -1.4139, -1.2182,  1.1051,  0.5597, -2.0778,  1.4008, -0.2809,\n",
       "         -0.0312, -1.7374, -0.6511,  0.7491,  0.8382,  2.6533,  0.8734, -1.0396,\n",
       "         -1.0296, -0.7004, -2.3923,  0.7202,  0.0964,  1.2664, -0.0616,  0.8752,\n",
       "         -0.1115, -1.0939, -1.9377,  1.2493,  0.8829, -1.4044, -0.5779, -1.6190,\n",
       "         -0.1982, -0.6651, -0.4949,  2.7224, -0.4127,  0.8885, -0.6328, -1.8119,\n",
       "          0.7735, -1.6129,  1.9822, -0.5384,  1.0313,  0.0839,  0.0766,  1.3489]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = box_net_v1((x1, x2))\n",
    "print(\"y.shape: \", y.shape)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
