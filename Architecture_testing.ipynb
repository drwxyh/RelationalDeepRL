{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relational Deep Reinforcement Learning\n",
    "\n",
    "**Plan:**\n",
    "1. Architecture\n",
    "2. Agent\n",
    "3. Environment\n",
    "4. Training cycle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relational architecture\n",
    "\n",
    "**Input: (b,n,n,1)** = (batch length, linear size, linear size, greyscale)\n",
    "\n",
    "**Extract entities: (b,n,n,1) -> (b, m, m, 2k)** \n",
    "* embedding layer: vocab_size = MAX_PIXELS+1, embedding_dim = n_dim\n",
    "* convolutional_layer1(kernel_size = (2,2), input_filters = n_dim, output_filters = k, stride = 1, pad = (1,1))\n",
    "* convolutional_layer2(kernel_size = (2,2), input_filters = k, output_filters = 2k, stride = 1, pad = (1,1))\n",
    "\n",
    "**Relational block: (b, m, m, 2k) -> (b,d_m)**\n",
    "* Positional Encoding: (b, m, m, 2k) -> (b, m^2, d_m)\n",
    "* N Multi-Headed Attention blocks: (b, m^2, d_m) -> (b, m^2, d_m)\n",
    "\n",
    "**Feature-wise max pooling: (b, m^2, d_m) -> (b, d_m)**\n",
    "\n",
    "**Multi-Layer Perceptron: (b, d_m) -> (b, d_m)**\n",
    "* 4 fully connected layers (d_m,d_m) with ReLUs (TODO: add skip-connections)\n",
    "\n",
    "**Actor output: (b,d_m) -> (b,a)** [a = number of possible actions]\n",
    "* Single linear layer with softmax at the end\n",
    "\n",
    "**Critic output: (b,d_m) -> (b,1)** \n",
    "* Single linear layer without activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Control architecture\n",
    "\n",
    "**Input: (b,n,n,1)** = (batch length, linear size, linear size, greyscale)\n",
    "\n",
    "**Extract entities: (b,n,n,1) -> (b, m, m, 2k)** \n",
    "* embedding layer: vocab_size = MAX_PIXELS+1, embedding_dim = n_dim\n",
    "* convolutional_layer1(kernel_size = (2,2), input_filters = n_dim, output_filters = k, stride = 1, pad = (1,1))\n",
    "* convolutional_layer2(kernel_size = (2,2), input_filters = k, output_filters = 2k, stride = 1, pad = (1,1))\n",
    "\n",
    "**1D Convolutional block: (b, m, m, 2k) -> (b, m^2, d_m)**\n",
    "* Positional Encoding: (b, m, m, 2k) -> (b, m^2, d_m)\n",
    "* 2 1D convolutional blocks with ReLUs: (b, m^2, d_m) -> (b, m^2, d_m) - pixel-wise\n",
    "\n",
    "**Feature-wise max pooling: (b, m^2, d_m) -> (b, d_m)**\n",
    "\n",
    "**Multi-Layer Perceptron: (b, d_m) -> (b, d_m)**\n",
    "* 4 fully connected layers (d_m,d_m) with ReLUs - feature-wise\n",
    "* (TODO: add skip-connections)\n",
    "\n",
    "**Actor output: (b,d_m) -> (b,a)** [a = number of possible actions]\n",
    "* Single linear layer with softmax at the end\n",
    "\n",
    "**Critic output: (b,d_m) -> (b,1)** \n",
    "* Single linear layer without activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from RelationalModule import RelationalNetworks as rnet\n",
    "from RelationalModule import ControlNetworks as cnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "reload(rnet)\n",
    "reload(cnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample image from the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"pycolab/pycolab/examples/research/box_world\")\n",
    "import box_world as bw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state(observation, mask=True):\n",
    "    #print(\"Keys: \", observation.layers.keys())\n",
    "    board = observation.board#.astype('float')\n",
    "    \n",
    "    if mask:\n",
    "        walls = observation.layers['#'].astype(int)\n",
    "        #print(\"walls: \", walls)\n",
    "        background = observation.layers[' '].astype(int)\n",
    "        #print(\"background: \", background)\n",
    "        ambient = walls + background\n",
    "        #print(\"ambient: \", ambient)\n",
    "        board[ambient.astype(bool)] = 0\n",
    "        #print(\"board (masked): \", board)\n",
    "    grid_size = board.shape[0]\n",
    "    board = board.reshape(1, grid_size, grid_size)\n",
    "    return board #/MAX_PIXEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_key_values(observation):\n",
    "    layers = observation.layers\n",
    "    board = observation.board\n",
    "    print(\"\\nKeys: \", layers.keys())\n",
    "    for k in layers.keys():\n",
    "        values = board[layers[k]]\n",
    "        if k == ' ':\n",
    "            print(\"background \", np.max(values))\n",
    "        elif k == '#':\n",
    "            print(\"wall \", np.max(values))\n",
    "        elif k == '.':\n",
    "            print(\"agent \",  np.max(values))\n",
    "        else:\n",
    "            print(k, np.max(values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_init_state():\n",
    "    GRID_SIZE = 12\n",
    "    game_params = dict(grid_size=GRID_SIZE,\n",
    "                    solution_length=[2], # number of boxes to be opened to get the gem\n",
    "                    num_forward = [1], # number of distractors\n",
    "                    num_backward=[0], # just set to 0 for now\n",
    "                    branch_length=1, # length of forward distractors\n",
    "                    max_num_steps = 50\n",
    "                   )\n",
    "    game = bw.make_game(**game_params)\n",
    "\n",
    "\n",
    "\n",
    "    observation, _, _ = game.its_showtime()\n",
    "    print_key_values(observation)\n",
    "    state = get_state(observation)\n",
    "    state = torch.from_numpy(state.astype(int))\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "states = [get_init_state() for _ in range(5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see that actually the agent, the wall, the background and the gem have always the same value along episodes. Also same key-box couples have the same pair color, so once experienced a certain opening, that opening can be memorized and used in the future (otherwise the agent should rely on pure chance and trying to open every box every time).\n",
    "\n",
    "A simpler setup would be that of masking to 0 all walls and background, at least to see if there is anything about them that is impeding the learning process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 256\n",
    "n_dim = 3\n",
    "embed = nn.Embedding(vocab_size, n_dim, padding_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "zeros = torch.ones((10,10), dtype=int)\n",
    "y_zeros = embed(zeros)\n",
    "print(\"y_zeros.shape: \", y_zeros.shape)\n",
    "print(y_zeros[:,:,0])\n",
    "print(y_zeros[:,:,1])\n",
    "print(y_zeros[:,:,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same integer values get mapped to same vectors, as it should be. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eye = torch.eye(10, dtype=int)\n",
    "y_eye = embed(eye).detach()\n",
    "plt.imshow(y_eye.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again the embedding works, since each integer value is associated to a particular vector (in this case 3D vector, that can be represented as RGB color once clipped)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for state in states:\n",
    "    x = state\n",
    "    if len(x.shape) <= 3:\n",
    "        x = x.unsqueeze(0)\n",
    "    #print(\"x.shape (before embed): \", x.shape)\n",
    "    x = embed(x)\n",
    "    #print(\"x.shape (after embed): \", x.shape)\n",
    "    #print(\"x.sum in slices: \", sum_slices(x,(0,1)))\n",
    "    x = x.transpose(-1,-3)\n",
    "    #print(\"x.shape: \", x.shape)\n",
    "    x = x.transpose(-1,-2).reshape(x.shape[0],-1,x.shape[-2],x.shape[-1]).squeeze()\n",
    "    #print(\"x.shape: \", x.shape)\n",
    "    ### This part is just for correct visualization after embedding ###\n",
    "    x = x.transpose(-1,0)\n",
    "    x = x.transpose(1,0)\n",
    "    #print(\"x.shape: \", x.shape)\n",
    "    x = x.detach().numpy()\n",
    "    M = x.max(axis=(0,1))\n",
    "    m= x.min(axis=(0,1))\n",
    "    #print(\"M: \", M.shape )\n",
    "    #print(\"m: \", m.shape)\n",
    "    x = (x - m)/(M-m)\n",
    "    plt.imshow(x)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given this simple rendering (after embedding) from naked eye we can only see that:\n",
    "1. keys have different colors from the boxes they should open\n",
    "2. distractors are identical to the right boxes\n",
    "\n",
    "Then we also have to notice that in this representation colors are biased because I had to normalize in [0,1] the \"RGB\" channels given by the embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_state = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_in = 1\n",
    "k_out = 24\n",
    "kernel_size = 2\n",
    "stride = 1\n",
    "padding = 0\n",
    "\n",
    "layers = []\n",
    "layers.append(nn.Conv2d(n_dim*k_in, k_out//2, kernel_size, stride, padding))\n",
    "layers.append(nn.ReLU())\n",
    "layers.append(nn.Conv2d(k_out//2, k_out, kernel_size, stride, padding))\n",
    "#layers.append(nn.ReLU())\n",
    "net = nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_slices(x, axes):\n",
    "    return x.squeeze().sum(axis=axes).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if use_state:\n",
    "    x = states[0]\n",
    "else:\n",
    "    x = torch.zeros((1,14,14), dtype=int) # this is the structure of the state retrieved by the game\n",
    "if len(x.shape) <= 3:\n",
    "    x = x.unsqueeze(0)\n",
    "print(\"x.shape (before embed): \", x.shape)\n",
    "x = embed(x)\n",
    "print(\"x.shape (after embed): \", x.shape)\n",
    "print(\"x.sum in slices: \", sum_slices(x,(0,1)))\n",
    "x = x.transpose(-1,-3)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = x.transpose(-1,-2).reshape(x.shape[0],-1,x.shape[-2],x.shape[-1])\n",
    "print(\"x.shape: \", x.shape)\n",
    "y = net(x)\n",
    "print(\"y.shape: \", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y = y.squeeze().detach()\n",
    "print(sum_slices(y,(1,2)))\n",
    "print(\"y[0,:,:]: \", y[0,:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So basically each layer is uniform thanks to the input and how convolution works (each slice is the result of the convolution from the same kernel of the same input). All the zeros that can be seen are due to the ReLU activations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_encoding2D(x):\n",
    "    x_ax = x.shape[-2]\n",
    "    y_ax = x.shape[-1]\n",
    "\n",
    "    x_lin = torch.linspace(-1,1,x_ax)\n",
    "    xx = x_lin.repeat(x.shape[0],y_ax,1).view(-1, 1, y_ax, x_ax).transpose(3,2)\n",
    "\n",
    "    y_lin = torch.linspace(-1,1,y_ax).view(-1,1)\n",
    "    yy = y_lin.repeat(x.shape[0],1,x_ax).view(-1, 1, y_ax, x_ax).transpose(3,2)\n",
    "\n",
    "    x = torch.cat((x,xx,yy), axis=1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_state:\n",
    "    x = states[0]\n",
    "else:\n",
    "    x = torch.zeros((1,14,14), dtype=int) # this is the structure of the state retrieved by the game\n",
    "if len(x.shape) <= 3:\n",
    "    x = x.unsqueeze(0)\n",
    "print(\"x.shape (before embed): \", x.shape)\n",
    "x = embed(x)\n",
    "print(\"x.shape (after embed): \", x.shape)\n",
    "print(\"x.sum in slices: \", sum_slices(x,(0,1)))\n",
    "x = x.transpose(-1,-3)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = x.transpose(-1,-2).reshape(x.shape[0],-1,x.shape[-2],x.shape[-1])\n",
    "print(\"x.shape: \", x.shape)\n",
    "y = net(x)\n",
    "print(\"y.shape: \", y.shape)\n",
    "y_enc = add_encoding2D(y)\n",
    "print(\"y_enc.shape: \", y_enc.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify that the last 2 layers have a positional encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tmp = y_enc.squeeze().detach()\n",
    "print(\"y_tmp.shape: \", y_tmp.shape)\n",
    "plt.imshow(y_tmp[-2])\n",
    "plt.show()\n",
    "plt.imshow(y_tmp[-1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A different approach would be to sum these two layers pixel-wise to all other features. Probably it would amplify the importance of the position, at the risk that if the magnitude is too high we would lose data.\n",
    "\n",
    "Also more complicated encodings are possible; this one is the one I think they used in the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projection from 26 to n_features (default 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 256\n",
    "projection = nn.Linear(k_out + 2, n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_state:\n",
    "    x = states[0]\n",
    "else:\n",
    "    x = torch.zeros((1,14,14), dtype=int) # this is the structure of the state retrieved by the game\n",
    "if len(x.shape) <= 3:\n",
    "    x = x.unsqueeze(0)\n",
    "print(\"x.shape (before embed): \", x.shape)\n",
    "x = embed(x)\n",
    "print(\"x.shape (after embed): \", x.shape)\n",
    "print(\"x.sum in slices: \", sum_slices(x,(0,1)))\n",
    "x = x.transpose(-1,-3)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = x.transpose(-1,-2).reshape(x.shape[0],-1,x.shape[-2],x.shape[-1])\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = net(x)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x= add_encoding2D(x)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = x.view(x.shape[0], x.shape[1],-1)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = x.transpose(2,1)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = projection(x)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = x.transpose(1,0)\n",
    "print(\"x.shape: \", x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here everything seems clean. Let's see if there is some trace of the positional encoding left. Ideally thanks to the projection now each feature potentially has a positional encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tmp = x.squeeze().detach().view(12,12,256)\n",
    "plt.imshow(x_tmp[:,:,0])\n",
    "plt.show()\n",
    "plt.imshow(x_tmp[:,:,128])\n",
    "plt.show()\n",
    "plt.imshow(x_tmp[:,:,-2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again everything seems fine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relational Block \n",
    "Implements the relational block, composed by a Multi-Headed Dot-Product Attention layer followed by a Position-wise Feed-Forward layer. I implement here the former one, whereas I just import the latter from the module, since it's very basic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 64\n",
    "dropout = 0\n",
    "n_heads = 4\n",
    "\n",
    "norm = nn.LayerNorm(n_features)\n",
    "drop = nn.Dropout(dropout) # disabled\n",
    "attn = nn.MultiheadAttention(n_features, n_heads, dropout)\n",
    "ff = rnet.PositionwiseFeedForward(n_features, hidden_dim, dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if use_state:\n",
    "    x = states[0]\n",
    "else:\n",
    "    x = torch.zeros((1,14,14), dtype=int) # this is the structure of the state retrieved by the game\n",
    "if len(x.shape) <= 3:\n",
    "    x = x.unsqueeze(0)\n",
    "print(\"x.shape (before embed): \", x.shape)\n",
    "x = embed(x)\n",
    "print(\"x.shape (after embed): \", x.shape)\n",
    "print(\"x.sum in slices: \", sum_slices(x,(0,1)))\n",
    "x = x.transpose(-1,-3)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = x.transpose(-1,-2).reshape(x.shape[0],-1,x.shape[-2],x.shape[-1])\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = net(x)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x= add_encoding2D(x)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = x.view(x.shape[0], x.shape[1],-1)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = x.transpose(2,1)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = projection(x)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = x.transpose(1,0)\n",
    "print(\"x.shape: \", x.shape)\n",
    "\n",
    "x_tmp = x # save it for plotting\n",
    "\n",
    "# From here it has always the same shape\n",
    "attn_output, attn_output_weights =  attn(x,x,x, key_padding_mask=None) # MHA step\n",
    "print(\"attn_output.shape: \", attn_output.shape)\n",
    "x_add = attn_output + x\n",
    "print(\"x_add.shape: \", x_add.shape)\n",
    "x_norm = drop(norm(x_add))\n",
    "print(\"x_norm.shape: \", x_norm.shape)\n",
    "x_ff = ff(x_norm)\n",
    "print(\"x_ff.shape: \", x_ff.shape)\n",
    "out = drop(norm(x_ff))\n",
    "print(\"out.shape: \", out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_layer(x, layer=0):\n",
    "    x = x.squeeze().detach()[:,layer]\n",
    "    plt.imshow(x.view(12,12))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Input\")\n",
    "plot_layer(x_tmp)\n",
    "print(\"Attention output\")\n",
    "plot_layer(attn_output)\n",
    "print(\"Input + attention\")\n",
    "plot_layer(x_add)\n",
    "print(\"After LayerNorm\")\n",
    "plot_layer(x_norm)\n",
    "print(\"After position-wise FF\")\n",
    "plot_layer(x_ff)\n",
    "print(\"After LayerNorm\")\n",
    "plot_layer(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we didn't see anything suspicious, with the attention layer not doing much and the only real change happening during the positionwise feed forward, in which we make a convolution of the 256 features to obtain new ones, so of course after that we are looking at a different feature plane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see if the attention layer is working correctly, we can play with the pixel and batch asix and see if the result is affected by those changes. Since the attention mechanism works taking into account relations between pixels, masking some of them should change the output for the others. Instead the batch dimension shouldn't intefrere with the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if batch size changes the output\n",
    "if use_state:\n",
    "    x = states[0]\n",
    "else:\n",
    "    x = torch.zeros((1,14,14), dtype=int) # this is the structure of the state retrieved by the game\n",
    "if len(x.shape) <= 3:\n",
    "    x = x.unsqueeze(0)\n",
    "print(\"x.shape (before embed): \", x.shape)\n",
    "x = embed(x)\n",
    "print(\"x.shape (after embed): \", x.shape)\n",
    "print(\"x.sum in slices: \", sum_slices(x,(0,1)))\n",
    "x = x.transpose(-1,-3)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = x.transpose(-1,-2).reshape(x.shape[0],-1,x.shape[-2],x.shape[-1])\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = net(x)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x= add_encoding2D(x)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = x.view(x.shape[0], x.shape[1],-1)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = x.transpose(2,1)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = projection(x)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = x.transpose(1,0)\n",
    "print(\"x.shape: \", x.shape)\n",
    "\n",
    "x_tmp2 = torch.cat((x,x), axis=1)\n",
    "attn_output, attn_output_weights =  attn(x_tmp2,x_tmp2,x_tmp2, key_padding_mask=None) # MHA step\n",
    "print(\"attn_output.shape: \", attn_output.shape)\n",
    "x_add = attn_output + x\n",
    "print(\"x_add.shape: \", x_add.shape)\n",
    "x_norm = drop(norm(x_add))\n",
    "print(\"x_norm.shape: \", x_norm.shape)\n",
    "x_ff = ff(x_norm)\n",
    "print(\"x_ff.shape: \", x_ff.shape)\n",
    "out2 = drop(norm(x_ff))\n",
    "out3 = out2[:,1,:].unsqueeze(1)\n",
    "out2 = out2[:,0,:].unsqueeze(1)\n",
    "\n",
    "print(\"out2.shape: \", out2.shape)\n",
    "print(\"out2: \", out2)\n",
    "print(\"out: \", out)\n",
    "print(\"Element sum of the difference 2-3: \", torch.sum(out2- out3).item())\n",
    "print(\"Element sum of the difference 2-0: \", torch.sum(out2- out).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "out2 = out3 means that the batch dimension correctly hasn't changed the result, because the same input has been concatenated along that axes and the same two outputs have been obtained on the other end. out2 = out shows that different samples are handled independently, as it should happen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if pixel sequence changes the output\n",
    "if use_state:\n",
    "    x = states[0]\n",
    "else:\n",
    "    x = torch.zeros((1,14,14), dtype=int) # this is the structure of the state retrieved by the game\n",
    "if len(x.shape) <= 3:\n",
    "    x = x.unsqueeze(0)\n",
    "print(\"x.shape (before embed): \", x.shape)\n",
    "x = embed(x)\n",
    "print(\"x.shape (after embed): \", x.shape)\n",
    "print(\"x.sum in slices: \", sum_slices(x,(0,1)))\n",
    "x = x.transpose(-1,-3)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = x.transpose(-1,-2).reshape(x.shape[0],-1,x.shape[-2],x.shape[-1])\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = net(x)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x= add_encoding2D(x)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = x.view(x.shape[0], x.shape[1],-1)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = x.transpose(2,1)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = projection(x)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = x.transpose(1,0)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x_tmp1 = x\n",
    "x_tmp1[100:] = 0. # mask last 44 positions\n",
    "attn_output, attn_output_weights =  attn(x_tmp1,x_tmp1,x_tmp1, key_padding_mask=None) # MHA step\n",
    "print(\"attn_output.shape: \", attn_output.shape)\n",
    "x_add = attn_output + x\n",
    "print(\"x_add.shape: \", x_add.shape)\n",
    "x_norm = drop(norm(x_add))\n",
    "print(\"x_norm.shape: \", x_norm.shape)\n",
    "x_ff = ff(x_norm)\n",
    "print(\"x_ff.shape: \", x_ff.shape)\n",
    "out1 = drop(norm(x_ff))\n",
    "\n",
    "print(\"out1: \", out1)\n",
    "print(\"out: \", out)\n",
    "print(\"Element sum of the difference: \", torch.sum(out1 - out).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, the new output has changed, as it should."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LayerNorm formula**\n",
    "$$y = \\frac{x - \\mathrm{E}[x]}{ \\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma + \\beta$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# What LayerNorm does\n",
    "\n",
    "E = x_add[0,0,:].mean()\n",
    "print(\"E: \", E)\n",
    "V = x_add[0,0,:].var()\n",
    "print(\"V: \", V)\n",
    "y = (x_add[0,0,:]-E)/torch.sqrt(V+1e-5)\n",
    "print(\"LayerNorm by hand: \\n\", y)\n",
    "print(\"LayerNorm: \\n\", x_norm[0,0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Featurewise MaxPooling\n",
    "\n",
    "For each feature, take the maximum value among the pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.zeros((1,14,14), dtype=int) # this is the structure of the state retrieved by the game\n",
    "if len(x.shape) <= 3:\n",
    "    x = x.unsqueeze(0)\n",
    "print(\"x.shape (before embed): \", x.shape)\n",
    "x = embed(x)\n",
    "print(\"x.shape (after embed): \", x.shape)\n",
    "print(\"x.sum in slices: \", sum_slices(x,(0,1)))\n",
    "x = x.transpose(-1,-3)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = x.transpose(-1,-2).reshape(x.shape[0],-1,x.shape[-2],x.shape[-1])\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = net(x)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x= add_encoding2D(x)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = x.view(x.shape[0], x.shape[1],-1)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = x.transpose(2,1)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = projection(x)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = x.transpose(1,0)\n",
    "print(\"x.shape: \", x.shape)\n",
    "\n",
    "x_tmp = x # save it for plotting\n",
    "\n",
    "# From here it has always the same shape\n",
    "attn_output, attn_output_weights =  attn(x,x,x, key_padding_mask=None) # MHA step\n",
    "print(\"attn_output.shape: \", attn_output.shape)\n",
    "x_add = attn_output + x\n",
    "print(\"x_add.shape: \", x_add.shape)\n",
    "x_norm = drop(norm(x_add))\n",
    "print(\"x_norm.shape: \", x_norm.shape)\n",
    "x_ff = ff(x_norm)\n",
    "print(\"x_ff.shape: \", x_ff.shape)\n",
    "x = drop(norm(x_ff))\n",
    "print(\"x.shape: \", x.shape)\n",
    "# Max pooling feature-wise\n",
    "x, _ = torch.max(x, axis=0)\n",
    "print(\"x.shape: \", x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing much to control here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternative to max pooling - Linear projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(rnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_proj = nn.Linear(144,1) # needs to know how many pixels there are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.zeros((1,1,14,14), dtype=int) # this is the structure of the state retrieved by the game\n",
    "if len(x.shape) <= 3:\n",
    "    x = x.unsqueeze(0)\n",
    "print(\"x.shape (before embed): \", x.shape)\n",
    "x = embed(x)\n",
    "print(\"x.shape (after embed): \", x.shape)\n",
    "print(\"x.sum in slices: \", sum_slices(x,(0,1)))\n",
    "x = x.transpose(-1,-3)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = x.transpose(-1,-2).reshape(x.shape[0],-1,x.shape[-2],x.shape[-1])\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = net(x)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x= add_encoding2D(x)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = x.view(x.shape[0], x.shape[1],-1)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = x.transpose(2,1)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = projection(x)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = x.transpose(1,0)\n",
    "print(\"x.shape: \", x.shape)\n",
    "\n",
    "x_tmp = x # save it for plotting\n",
    "\n",
    "# From here it has always the same shape\n",
    "attn_output, attn_output_weights =  attn(x,x,x, key_padding_mask=None) # MHA step\n",
    "print(\"attn_output.shape: \", attn_output.shape)\n",
    "x_add = attn_output + x\n",
    "print(\"x_add.shape: \", x_add.shape)\n",
    "x_norm = drop(norm(x_add))\n",
    "print(\"x_norm.shape: \", x_norm.shape)\n",
    "x_ff = ff(x_norm)\n",
    "print(\"x_ff.shape: \", x_ff.shape)\n",
    "x = drop(norm(x_ff))\n",
    "print(\"x.shape: \", x.shape)\n",
    "\n",
    "# Feature-wise projection\n",
    "x = x.transpose(-1,0)\n",
    "print(\"x.shape (before linear): \", x.shape)\n",
    "shape = x.shape\n",
    "x = linear_proj(x).reshape(shape[0],shape[1])\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = x.transpose(-1,0)\n",
    "print(\"x.shape: \", x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual Layer\n",
    "\n",
    "Here the original paper uses just a Multi-Layer Perceptron, but I thought it would be nice to have sone skip connections in order to make the architecture more flexible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(rnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hiddens = 256 \n",
    "residual_layer = rnet.ResidualLayer(n_features, n_hiddens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "residual_layer(x) - x # residual after ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(rnet)\n",
    "reload(cnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box_net = rnet.BoxWorldNet(in_channels=1, n_kernels=24, vocab_size=117, n_dim=3,\n",
    "                              n_features=256, n_attn_modules=2, n_linears=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box_net_v1 = rnet.BoxWorldNet(in_channels=1, n_kernels=24, vocab_size=117, n_dim=3,\n",
    "                              n_features=256, n_attn_modules=2, n_linears=4, max_pool=False,\n",
    "                              linear_size=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_net_v0 = cnet.ControlNet_v0(in_channels=1, n_kernels=24, vocab_size=117, n_dim=3,\n",
    "                              n_features=256, hidden_dim=64, n_control_modules=2, n_linears=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_net = cnet.ControlNet(vocab_size=117, n_dim=3, linear_size=14, n_features=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randint(high=116, size = (1,14,14))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y = box_net_v1(x)\n",
    "print(\"y.shape: \", y.shape)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = control_net(x)\n",
    "print(\"y.shape: \", y.shape)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-channel embedding layer test\n",
    "\n",
    "Here I just wanted to see how to embed even images with more than one channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(rnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_entities = rnet.ExtractEntities(k_out = 24, k_in=3, n_dim=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = nn.Embedding(255,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randint(high=116, size = (3,14,14))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_entities(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.transpose(-1,-2).reshape(x.shape[0],-1,x.shape[-2],x.shape[-1])\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bw_net = rnet.BoxWorldNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bw_net(x).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
