{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relational Deep Reinforcement Learning\n",
    "\n",
    "**Plan:**\n",
    "1. Architecture\n",
    "2. Agent\n",
    "3. Environment\n",
    "4. Training cycle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relational architecture\n",
    "\n",
    "**Input: (b,n,n,1)** = (batch length, linear size, linear size, greyscale)\n",
    "\n",
    "**Extract entities: (b,n,n,1) -> (b, m, m, 2k)** \n",
    "* embedding layer: vocab_size = MAX_PIXELS+1, embedding_dim = n_dim\n",
    "* convolutional_layer1(kernel_size = (2,2), input_filters = n_dim, output_filters = k, stride = 1, pad = (1,1))\n",
    "* convolutional_layer2(kernel_size = (2,2), input_filters = k, output_filters = 2k, stride = 1, pad = (1,1))\n",
    "\n",
    "**Relational block: (b, m, m, 2k) -> (b,d_m)**\n",
    "* Positional Encoding: (b, m, m, 2k) -> (b, m^2, d_m)\n",
    "* N Multi-Headed Attention blocks: (b, m^2, d_m) -> (b, m^2, d_m)\n",
    "\n",
    "**Feature-wise max pooling: (b, m^2, d_m) -> (b, d_m)**\n",
    "\n",
    "**Multi-Layer Perceptron: (b, d_m) -> (b, d_m)**\n",
    "* 4 fully connected layers (d_m,d_m) with ReLUs (TODO: add skip-connections)\n",
    "\n",
    "**Actor output: (b,d_m) -> (b,a)** [a = number of possible actions]\n",
    "* Single linear layer with softmax at the end\n",
    "\n",
    "**Critic output: (b,d_m) -> (b,1)** \n",
    "* Single linear layer without activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Control architecture\n",
    "\n",
    "**Input: (b,n,n,1)** = (batch length, linear size, linear size, greyscale)\n",
    "\n",
    "**Extract entities: (b,n,n,1) -> (b, m, m, 2k)** \n",
    "* embedding layer: vocab_size = MAX_PIXELS+1, embedding_dim = n_dim\n",
    "* convolutional_layer1(kernel_size = (2,2), input_filters = n_dim, output_filters = k, stride = 1, pad = (1,1))\n",
    "* convolutional_layer2(kernel_size = (2,2), input_filters = k, output_filters = 2k, stride = 1, pad = (1,1))\n",
    "\n",
    "**1D Convolutional block: (b, m, m, 2k) -> (b, m^2, d_m)**\n",
    "* Positional Encoding: (b, m, m, 2k) -> (b, m^2, d_m)\n",
    "* 2 1D convolutional blocks with ReLUs: (b, m^2, d_m) -> (b, m^2, d_m) - pixel-wise\n",
    "\n",
    "**Feature-wise max pooling: (b, m^2, d_m) -> (b, d_m)**\n",
    "\n",
    "**Multi-Layer Perceptron: (b, d_m) -> (b, d_m)**\n",
    "* 4 fully connected layers (d_m,d_m) with ReLUs - feature-wise\n",
    "* (TODO: add skip-connections)\n",
    "\n",
    "**Actor output: (b,d_m) -> (b,a)** [a = number of possible actions]\n",
    "* Single linear layer with softmax at the end\n",
    "\n",
    "**Critic output: (b,d_m) -> (b,1)** \n",
    "* Single linear layer without activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from RelationalModule import RelationalNetworks as rnet\n",
    "from RelationalModule import ControlNetworks as cnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'RelationalModule.ControlNetworks' from '/home/nicola/Nicola_unipd/MasterThesis/RelationalDeepRL/RelationalModule/ControlNetworks.py'>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from importlib import reload\n",
    "reload(rnet)\n",
    "reload(cnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample image from the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"pycolab/pycolab/examples/research/box_world\")\n",
    "import box_world as bw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicola/anaconda3/envs/torch/lib/python3.7/site-packages/pycolab/ascii_art.py:318: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  art = np.vstack(np.fromstring(line, dtype=np.uint8) for line in art)\n"
     ]
    }
   ],
   "source": [
    "GRID_SIZE = 12\n",
    "game_params = dict(grid_size=GRID_SIZE,\n",
    "                solution_length=[2], # number of boxes to be opened to get the gem\n",
    "                num_forward = [1], # number of distractors\n",
    "                num_backward=[0], # just set to 0 for now\n",
    "                branch_length=1, # length of forward distractors\n",
    "                max_num_steps = 50\n",
    "               )\n",
    "game = bw.make_game(**game_params)\n",
    "\n",
    "observation, _, _ = game.its_showtime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############\n",
      "#            #\n",
      "#  f         #\n",
      "#            #\n",
      "#            #\n",
      "#            #\n",
      "# *P pF   .  #\n",
      "#            #\n",
      "#            #\n",
      "#  iF        #\n",
      "#            #\n",
      "#            #\n",
      "#            #\n",
      "##############\n"
     ]
    }
   ],
   "source": [
    "b = observation.board\n",
    "l = observation.layers\n",
    "def show_game_state(observation):\n",
    "    for row in observation.board: print(row.tostring().decode('ascii'))\n",
    "        \n",
    "show_game_state(observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can forget about the board and just use the layers' masks to build our custom state.\n",
    "Chars (symbols) -> object names -> object code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_color_dict(seed=100):\n",
    "    import string\n",
    "    alphabet = string.ascii_lowercase\n",
    "    np.random.seed(seed)\n",
    "    RGB_list = np.random.rand(len(alphabet),3)\n",
    "    color_dict = {}\n",
    "    for l, c in zip(alphabet,RGB_list):\n",
    "        color_dict[l] = c\n",
    "    np.random.seed(None)\n",
    "    \n",
    "    # add colors for 'agent' and  'gem'\n",
    "    color_dict['agent'] = np.array([1.,0.,0.])\n",
    "    color_dict['gem'] = np.array([1.,1.,1.])\n",
    "    \n",
    "    return color_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_dict = get_color_dict()\n",
    "object_dict = {'ground':0, 'wall':1, 'agent':2, 'gem':3, 'key':4, 'box':5}\n",
    "symbol_dict = {' ':'ground', '#':'wall', '.':'agent', '*':'gem'} # key and box can have any possible letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############\n",
      "#            #\n",
      "#  f         #\n",
      "#            #\n",
      "#            #\n",
      "#            #\n",
      "# *P pF   .  #\n",
      "#            #\n",
      "#            #\n",
      "#  iF        #\n",
      "#            #\n",
      "#            #\n",
      "#            #\n",
      "##############\n"
     ]
    }
   ],
   "source": [
    "def show_game_state(observation):\n",
    "    for row in observation.board: print(row.tostring().decode('ascii'))\n",
    "\n",
    "b = observation.board\n",
    "l = observation.layers        \n",
    "show_game_state(observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state(observation):\n",
    "    b = observation.board\n",
    "    l = observation.layers  \n",
    "    color_board = np.zeros(b.shape+(3,)).astype(float)\n",
    "    object_board = np.zeros(b.shape+(1,)).astype(int)\n",
    "\n",
    "    for symbol in l.keys():\n",
    "\n",
    "        # If alphabetic character\n",
    "        if symbol.isalpha():\n",
    "            # Paint the color board cells occupied accordingly\n",
    "            color_board[l[symbol]] = color_dict[symbol.lower()]\n",
    "\n",
    "            # Upper = box, lower = key\n",
    "            if symbol.isupper():\n",
    "                object_board[l[symbol]] = object_dict['box']\n",
    "            else:\n",
    "                object_board[l[symbol]] = object_dict['key']\n",
    "\n",
    "        else:\n",
    "            object_name = symbol_dict[symbol]\n",
    "\n",
    "            # Color assigned is [0,0,0] since it's not really a property of those objects\n",
    "            # Only agent and gem have colors mainly for plotting reasons\n",
    "            if object_name == 'agent':\n",
    "                color_board[l[symbol]] = color_dict['agent']\n",
    "            elif object_name == 'gem':\n",
    "                color_board[l[symbol]] = color_dict['gem']\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "            object_board[l[symbol]] = object_dict[object_name]\n",
    "            \n",
    "    return object_board, color_board\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_board, color_board = get_state(observation)\n",
    "state = (object_board, color_board)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ground': 0, 'wall': 1, 'agent': 2, 'gem': 3, 'key': 4, 'box': 5}\n",
      "[[1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      " [1 0 0 4 0 0 0 0 0 0 0 0 0 1]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      " [1 0 3 5 0 4 5 0 0 0 2 0 0 1]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      " [1 0 0 4 5 0 0 0 0 0 0 0 0 1]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print(object_dict)\n",
    "print(object_board.reshape(14,14)) # remove last channel for printing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f5fc7f80210>"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(color_board)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1.shape:  torch.Size([10, 14, 14, 1])\n",
      "x2.shape:  torch.Size([10, 14, 14, 3])\n"
     ]
    }
   ],
   "source": [
    "# just for simulating an episode - this is one of the outputs of play_episode\n",
    "states = [state for _ in range(10)]\n",
    "x1 = torch.LongTensor([s[0] for s in states])\n",
    "print(\"x1.shape: \", x1.shape)\n",
    "x2 = torch.tensor([s[1] for s in states]).float()\n",
    "print(\"x2.shape: \", x2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 14, 14, 1])"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = [1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(object_dict)\n",
    "n_dim = 3\n",
    "embed = nn.Embedding(vocab_size, n_dim, padding_idx=0) # backgroud is ignored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1_embed.shape:  torch.Size([10, 14, 14, 3])\n",
      "x.shape:  torch.Size([10, 14, 14, 6])\n",
      "x.shape:  torch.Size([10, 6, 14, 14])\n"
     ]
    }
   ],
   "source": [
    "if len(x1.shape) < 4:\n",
    "    x1.unsqueeze(0)\n",
    "x1_embed = embed(x1)\n",
    "x1_embed = x1_embed.reshape(x1.shape[:3]+(-1,))\n",
    "print(\"x1_embed.shape: \", x1_embed.shape)\n",
    "x = torch.cat((x1_embed, x2), axis = 3)\n",
    "print('x.shape: ', x.shape)\n",
    "x = x.transpose(-1,1)\n",
    "x = x.transpose(-1,-2)\n",
    "print('x.shape: ', x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old part using observation.board as state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_zeros.shape:  torch.Size([10, 10, 3])\n",
      "tensor([[1.1511, 1.1511, 1.1511, 1.1511, 1.1511, 1.1511, 1.1511, 1.1511, 1.1511,\n",
      "         1.1511],\n",
      "        [1.1511, 1.1511, 1.1511, 1.1511, 1.1511, 1.1511, 1.1511, 1.1511, 1.1511,\n",
      "         1.1511],\n",
      "        [1.1511, 1.1511, 1.1511, 1.1511, 1.1511, 1.1511, 1.1511, 1.1511, 1.1511,\n",
      "         1.1511],\n",
      "        [1.1511, 1.1511, 1.1511, 1.1511, 1.1511, 1.1511, 1.1511, 1.1511, 1.1511,\n",
      "         1.1511],\n",
      "        [1.1511, 1.1511, 1.1511, 1.1511, 1.1511, 1.1511, 1.1511, 1.1511, 1.1511,\n",
      "         1.1511],\n",
      "        [1.1511, 1.1511, 1.1511, 1.1511, 1.1511, 1.1511, 1.1511, 1.1511, 1.1511,\n",
      "         1.1511],\n",
      "        [1.1511, 1.1511, 1.1511, 1.1511, 1.1511, 1.1511, 1.1511, 1.1511, 1.1511,\n",
      "         1.1511],\n",
      "        [1.1511, 1.1511, 1.1511, 1.1511, 1.1511, 1.1511, 1.1511, 1.1511, 1.1511,\n",
      "         1.1511],\n",
      "        [1.1511, 1.1511, 1.1511, 1.1511, 1.1511, 1.1511, 1.1511, 1.1511, 1.1511,\n",
      "         1.1511],\n",
      "        [1.1511, 1.1511, 1.1511, 1.1511, 1.1511, 1.1511, 1.1511, 1.1511, 1.1511,\n",
      "         1.1511]], grad_fn=<SelectBackward>)\n",
      "tensor([[-1.1139, -1.1139, -1.1139, -1.1139, -1.1139, -1.1139, -1.1139, -1.1139,\n",
      "         -1.1139, -1.1139],\n",
      "        [-1.1139, -1.1139, -1.1139, -1.1139, -1.1139, -1.1139, -1.1139, -1.1139,\n",
      "         -1.1139, -1.1139],\n",
      "        [-1.1139, -1.1139, -1.1139, -1.1139, -1.1139, -1.1139, -1.1139, -1.1139,\n",
      "         -1.1139, -1.1139],\n",
      "        [-1.1139, -1.1139, -1.1139, -1.1139, -1.1139, -1.1139, -1.1139, -1.1139,\n",
      "         -1.1139, -1.1139],\n",
      "        [-1.1139, -1.1139, -1.1139, -1.1139, -1.1139, -1.1139, -1.1139, -1.1139,\n",
      "         -1.1139, -1.1139],\n",
      "        [-1.1139, -1.1139, -1.1139, -1.1139, -1.1139, -1.1139, -1.1139, -1.1139,\n",
      "         -1.1139, -1.1139],\n",
      "        [-1.1139, -1.1139, -1.1139, -1.1139, -1.1139, -1.1139, -1.1139, -1.1139,\n",
      "         -1.1139, -1.1139],\n",
      "        [-1.1139, -1.1139, -1.1139, -1.1139, -1.1139, -1.1139, -1.1139, -1.1139,\n",
      "         -1.1139, -1.1139],\n",
      "        [-1.1139, -1.1139, -1.1139, -1.1139, -1.1139, -1.1139, -1.1139, -1.1139,\n",
      "         -1.1139, -1.1139],\n",
      "        [-1.1139, -1.1139, -1.1139, -1.1139, -1.1139, -1.1139, -1.1139, -1.1139,\n",
      "         -1.1139, -1.1139]], grad_fn=<SelectBackward>)\n",
      "tensor([[0.9967, 0.9967, 0.9967, 0.9967, 0.9967, 0.9967, 0.9967, 0.9967, 0.9967,\n",
      "         0.9967],\n",
      "        [0.9967, 0.9967, 0.9967, 0.9967, 0.9967, 0.9967, 0.9967, 0.9967, 0.9967,\n",
      "         0.9967],\n",
      "        [0.9967, 0.9967, 0.9967, 0.9967, 0.9967, 0.9967, 0.9967, 0.9967, 0.9967,\n",
      "         0.9967],\n",
      "        [0.9967, 0.9967, 0.9967, 0.9967, 0.9967, 0.9967, 0.9967, 0.9967, 0.9967,\n",
      "         0.9967],\n",
      "        [0.9967, 0.9967, 0.9967, 0.9967, 0.9967, 0.9967, 0.9967, 0.9967, 0.9967,\n",
      "         0.9967],\n",
      "        [0.9967, 0.9967, 0.9967, 0.9967, 0.9967, 0.9967, 0.9967, 0.9967, 0.9967,\n",
      "         0.9967],\n",
      "        [0.9967, 0.9967, 0.9967, 0.9967, 0.9967, 0.9967, 0.9967, 0.9967, 0.9967,\n",
      "         0.9967],\n",
      "        [0.9967, 0.9967, 0.9967, 0.9967, 0.9967, 0.9967, 0.9967, 0.9967, 0.9967,\n",
      "         0.9967],\n",
      "        [0.9967, 0.9967, 0.9967, 0.9967, 0.9967, 0.9967, 0.9967, 0.9967, 0.9967,\n",
      "         0.9967],\n",
      "        [0.9967, 0.9967, 0.9967, 0.9967, 0.9967, 0.9967, 0.9967, 0.9967, 0.9967,\n",
      "         0.9967]], grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "zeros = torch.ones((10,10), dtype=int)\n",
    "y_zeros = embed(zeros)\n",
    "print(\"y_zeros.shape: \", y_zeros.shape)\n",
    "print(y_zeros[:,:,0])\n",
    "print(y_zeros[:,:,1])\n",
    "print(y_zeros[:,:,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same integer values get mapped to same vectors, as it should be. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f1cd3426d90>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "eye = torch.eye(10, dtype=int)\n",
    "y_eye = embed(eye).detach()\n",
    "plt.imshow(y_eye.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again the embedding works, since each integer value is associated to a particular vector (in this case 3D vector, that can be represented as RGB color once clipped)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for state in states:\n",
    "    x = state\n",
    "    if len(x.shape) <= 3:\n",
    "        x = x.unsqueeze(0)\n",
    "    #print(\"x.shape (before embed): \", x.shape)\n",
    "    x = embed(x)\n",
    "    #print(\"x.shape (after embed): \", x.shape)\n",
    "    #print(\"x.sum in slices: \", sum_slices(x,(0,1)))\n",
    "    x = x.transpose(-1,-3)\n",
    "    #print(\"x.shape: \", x.shape)\n",
    "    x = x.transpose(-1,-2).reshape(x.shape[0],-1,x.shape[-2],x.shape[-1]).squeeze()\n",
    "    #print(\"x.shape: \", x.shape)\n",
    "    ### This part is just for correct visualization after embedding ###\n",
    "    x = x.transpose(-1,0)\n",
    "    x = x.transpose(1,0)\n",
    "    #print(\"x.shape: \", x.shape)\n",
    "    x = x.detach().numpy()\n",
    "    M = x.max(axis=(0,1))\n",
    "    m= x.min(axis=(0,1))\n",
    "    #print(\"M: \", M.shape )\n",
    "    #print(\"m: \", m.shape)\n",
    "    x = (x - m)/(M-m)\n",
    "    plt.imshow(x)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given this simple rendering (after embedding) from naked eye we can only see that:\n",
    "1. keys have different colors from the boxes they should open\n",
    "2. distractors are identical to the right boxes\n",
    "\n",
    "Then we also have to notice that in this representation colors are biased because I had to normalize in [0,1] the \"RGB\" channels given by the embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_state = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_in = 1\n",
    "k_out = 24\n",
    "kernel_size = 2\n",
    "stride = 1\n",
    "padding = 0\n",
    "\n",
    "layers = []\n",
    "layers.append(nn.Conv2d(n_dim*k_in, k_out//2, kernel_size, stride, padding))\n",
    "layers.append(nn.ReLU())\n",
    "layers.append(nn.Conv2d(k_out//2, k_out, kernel_size, stride, padding))\n",
    "#layers.append(nn.ReLU())\n",
    "net = nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_slices(x, axes):\n",
    "    return x.squeeze().sum(axis=axes).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if use_state:\n",
    "    x = states[0]\n",
    "else:\n",
    "    x = torch.zeros((1,14,14), dtype=int) # this is the structure of the state retrieved by the game\n",
    "if len(x.shape) <= 3:\n",
    "    x = x.unsqueeze(0)\n",
    "print(\"x.shape (before embed): \", x.shape)\n",
    "x = embed(x)\n",
    "print(\"x.shape (after embed): \", x.shape)\n",
    "print(\"x.sum in slices: \", sum_slices(x,(0,1)))\n",
    "x = x.transpose(-1,-3)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = x.transpose(-1,-2).reshape(x.shape[0],-1,x.shape[-2],x.shape[-1])\n",
    "print(\"x.shape: \", x.shape)\n",
    "y = net(x)\n",
    "print(\"y.shape: \", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y = y.squeeze().detach()\n",
    "print(sum_slices(y,(1,2)))\n",
    "print(\"y[0,:,:]: \", y[0,:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So basically each layer is uniform thanks to the input and how convolution works (each slice is the result of the convolution from the same kernel of the same input). All the zeros that can be seen are due to the ReLU activations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_encoding2D(x):\n",
    "    x_ax = x.shape[-2]\n",
    "    y_ax = x.shape[-1]\n",
    "\n",
    "    x_lin = torch.linspace(-1,1,x_ax)\n",
    "    xx = x_lin.repeat(x.shape[0],y_ax,1).view(-1, 1, y_ax, x_ax).transpose(3,2)\n",
    "\n",
    "    y_lin = torch.linspace(-1,1,y_ax).view(-1,1)\n",
    "    yy = y_lin.repeat(x.shape[0],1,x_ax).view(-1, 1, y_ax, x_ax).transpose(3,2)\n",
    "\n",
    "    x = torch.cat((x,xx,yy), axis=1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_state:\n",
    "    x = states[0]\n",
    "else:\n",
    "    x = torch.zeros((1,14,14), dtype=int) # this is the structure of the state retrieved by the game\n",
    "if len(x.shape) <= 3:\n",
    "    x = x.unsqueeze(0)\n",
    "print(\"x.shape (before embed): \", x.shape)\n",
    "x = embed(x)\n",
    "print(\"x.shape (after embed): \", x.shape)\n",
    "print(\"x.sum in slices: \", sum_slices(x,(0,1)))\n",
    "x = x.transpose(-1,-3)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = x.transpose(-1,-2).reshape(x.shape[0],-1,x.shape[-2],x.shape[-1])\n",
    "print(\"x.shape: \", x.shape)\n",
    "y = net(x)\n",
    "print(\"y.shape: \", y.shape)\n",
    "y_enc = add_encoding2D(y)\n",
    "print(\"y_enc.shape: \", y_enc.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify that the last 2 layers have a positional encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tmp = y_enc.squeeze().detach()\n",
    "print(\"y_tmp.shape: \", y_tmp.shape)\n",
    "plt.imshow(y_tmp[-2])\n",
    "plt.show()\n",
    "plt.imshow(y_tmp[-1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A different approach would be to sum these two layers pixel-wise to all other features. Probably it would amplify the importance of the position, at the risk that if the magnitude is too high we would lose data.\n",
    "\n",
    "Also more complicated encodings are possible; this one is the one I think they used in the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projection from 26 to n_features (default 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 256\n",
    "projection = nn.Linear(k_out + 2, n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_state:\n",
    "    x = states[0]\n",
    "else:\n",
    "    x = torch.zeros((1,14,14), dtype=int) # this is the structure of the state retrieved by the game\n",
    "if len(x.shape) <= 3:\n",
    "    x = x.unsqueeze(0)\n",
    "print(\"x.shape (before embed): \", x.shape)\n",
    "x = embed(x)\n",
    "print(\"x.shape (after embed): \", x.shape)\n",
    "print(\"x.sum in slices: \", sum_slices(x,(0,1)))\n",
    "x = x.transpose(-1,-3)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = x.transpose(-1,-2).reshape(x.shape[0],-1,x.shape[-2],x.shape[-1])\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = net(x)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x= add_encoding2D(x)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = x.view(x.shape[0], x.shape[1],-1)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = x.transpose(2,1)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = projection(x)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = x.transpose(1,0)\n",
    "print(\"x.shape: \", x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here everything seems clean. Let's see if there is some trace of the positional encoding left. Ideally thanks to the projection now each feature potentially has a positional encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tmp = x.squeeze().detach().view(12,12,256)\n",
    "plt.imshow(x_tmp[:,:,0])\n",
    "plt.show()\n",
    "plt.imshow(x_tmp[:,:,128])\n",
    "plt.show()\n",
    "plt.imshow(x_tmp[:,:,-2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again everything seems fine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relational Block \n",
    "Implements the relational block, composed by a Multi-Headed Dot-Product Attention layer followed by a Position-wise Feed-Forward layer. I implement here the former one, whereas I just import the latter from the module, since it's very basic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 64\n",
    "dropout = 0\n",
    "n_heads = 4\n",
    "\n",
    "norm = nn.LayerNorm(n_features)\n",
    "drop = nn.Dropout(dropout) # disabled\n",
    "attn = nn.MultiheadAttention(n_features, n_heads, dropout)\n",
    "ff = rnet.PositionwiseFeedForward(n_features, hidden_dim, dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if use_state:\n",
    "    x = states[0]\n",
    "else:\n",
    "    x = torch.zeros((1,14,14), dtype=int) # this is the structure of the state retrieved by the game\n",
    "if len(x.shape) <= 3:\n",
    "    x = x.unsqueeze(0)\n",
    "print(\"x.shape (before embed): \", x.shape)\n",
    "x = embed(x)\n",
    "print(\"x.shape (after embed): \", x.shape)\n",
    "print(\"x.sum in slices: \", sum_slices(x,(0,1)))\n",
    "x = x.transpose(-1,-3)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = x.transpose(-1,-2).reshape(x.shape[0],-1,x.shape[-2],x.shape[-1])\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = net(x)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x= add_encoding2D(x)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = x.view(x.shape[0], x.shape[1],-1)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = x.transpose(2,1)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = projection(x)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = x.transpose(1,0)\n",
    "print(\"x.shape: \", x.shape)\n",
    "\n",
    "x_tmp = x # save it for plotting\n",
    "\n",
    "# From here it has always the same shape\n",
    "attn_output, attn_output_weights =  attn(x,x,x, key_padding_mask=None) # MHA step\n",
    "print(\"attn_output.shape: \", attn_output.shape)\n",
    "x_add = attn_output + x\n",
    "print(\"x_add.shape: \", x_add.shape)\n",
    "x_norm = drop(norm(x_add))\n",
    "print(\"x_norm.shape: \", x_norm.shape)\n",
    "x_ff = ff(x_norm)\n",
    "print(\"x_ff.shape: \", x_ff.shape)\n",
    "out = drop(norm(x_ff))\n",
    "print(\"out.shape: \", out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_layer(x, layer=0):\n",
    "    x = x.squeeze().detach()[:,layer]\n",
    "    plt.imshow(x.view(12,12))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Input\")\n",
    "plot_layer(x_tmp)\n",
    "print(\"Attention output\")\n",
    "plot_layer(attn_output)\n",
    "print(\"Input + attention\")\n",
    "plot_layer(x_add)\n",
    "print(\"After LayerNorm\")\n",
    "plot_layer(x_norm)\n",
    "print(\"After position-wise FF\")\n",
    "plot_layer(x_ff)\n",
    "print(\"After LayerNorm\")\n",
    "plot_layer(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we didn't see anything suspicious, with the attention layer not doing much and the only real change happening during the positionwise feed forward, in which we make a convolution of the 256 features to obtain new ones, so of course after that we are looking at a different feature plane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see if the attention layer is working correctly, we can play with the pixel and batch asix and see if the result is affected by those changes. Since the attention mechanism works taking into account relations between pixels, masking some of them should change the output for the others. Instead the batch dimension shouldn't intefrere with the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if batch size changes the output\n",
    "if use_state:\n",
    "    x = states[0]\n",
    "else:\n",
    "    x = torch.zeros((1,14,14), dtype=int) # this is the structure of the state retrieved by the game\n",
    "if len(x.shape) <= 3:\n",
    "    x = x.unsqueeze(0)\n",
    "print(\"x.shape (before embed): \", x.shape)\n",
    "x = embed(x)\n",
    "print(\"x.shape (after embed): \", x.shape)\n",
    "print(\"x.sum in slices: \", sum_slices(x,(0,1)))\n",
    "x = x.transpose(-1,-3)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = x.transpose(-1,-2).reshape(x.shape[0],-1,x.shape[-2],x.shape[-1])\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = net(x)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x= add_encoding2D(x)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = x.view(x.shape[0], x.shape[1],-1)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = x.transpose(2,1)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = projection(x)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = x.transpose(1,0)\n",
    "print(\"x.shape: \", x.shape)\n",
    "\n",
    "x_tmp2 = torch.cat((x,x), axis=1)\n",
    "attn_output, attn_output_weights =  attn(x_tmp2,x_tmp2,x_tmp2, key_padding_mask=None) # MHA step\n",
    "print(\"attn_output.shape: \", attn_output.shape)\n",
    "x_add = attn_output + x\n",
    "print(\"x_add.shape: \", x_add.shape)\n",
    "x_norm = drop(norm(x_add))\n",
    "print(\"x_norm.shape: \", x_norm.shape)\n",
    "x_ff = ff(x_norm)\n",
    "print(\"x_ff.shape: \", x_ff.shape)\n",
    "out2 = drop(norm(x_ff))\n",
    "out3 = out2[:,1,:].unsqueeze(1)\n",
    "out2 = out2[:,0,:].unsqueeze(1)\n",
    "\n",
    "print(\"out2.shape: \", out2.shape)\n",
    "print(\"out2: \", out2)\n",
    "print(\"out: \", out)\n",
    "print(\"Element sum of the difference 2-3: \", torch.sum(out2- out3).item())\n",
    "print(\"Element sum of the difference 2-0: \", torch.sum(out2- out).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "out2 = out3 means that the batch dimension correctly hasn't changed the result, because the same input has been concatenated along that axes and the same two outputs have been obtained on the other end. out2 = out shows that different samples are handled independently, as it should happen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if pixel sequence changes the output\n",
    "if use_state:\n",
    "    x = states[0]\n",
    "else:\n",
    "    x = torch.zeros((1,14,14), dtype=int) # this is the structure of the state retrieved by the game\n",
    "if len(x.shape) <= 3:\n",
    "    x = x.unsqueeze(0)\n",
    "print(\"x.shape (before embed): \", x.shape)\n",
    "x = embed(x)\n",
    "print(\"x.shape (after embed): \", x.shape)\n",
    "print(\"x.sum in slices: \", sum_slices(x,(0,1)))\n",
    "x = x.transpose(-1,-3)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = x.transpose(-1,-2).reshape(x.shape[0],-1,x.shape[-2],x.shape[-1])\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = net(x)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x= add_encoding2D(x)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = x.view(x.shape[0], x.shape[1],-1)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = x.transpose(2,1)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = projection(x)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = x.transpose(1,0)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x_tmp1 = x\n",
    "x_tmp1[100:] = 0. # mask last 44 positions\n",
    "attn_output, attn_output_weights =  attn(x_tmp1,x_tmp1,x_tmp1, key_padding_mask=None) # MHA step\n",
    "print(\"attn_output.shape: \", attn_output.shape)\n",
    "x_add = attn_output + x\n",
    "print(\"x_add.shape: \", x_add.shape)\n",
    "x_norm = drop(norm(x_add))\n",
    "print(\"x_norm.shape: \", x_norm.shape)\n",
    "x_ff = ff(x_norm)\n",
    "print(\"x_ff.shape: \", x_ff.shape)\n",
    "out1 = drop(norm(x_ff))\n",
    "\n",
    "print(\"out1: \", out1)\n",
    "print(\"out: \", out)\n",
    "print(\"Element sum of the difference: \", torch.sum(out1 - out).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, the new output has changed, as it should."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LayerNorm formula**\n",
    "$$y = \\frac{x - \\mathrm{E}[x]}{ \\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma + \\beta$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# What LayerNorm does\n",
    "\n",
    "E = x_add[0,0,:].mean()\n",
    "print(\"E: \", E)\n",
    "V = x_add[0,0,:].var()\n",
    "print(\"V: \", V)\n",
    "y = (x_add[0,0,:]-E)/torch.sqrt(V+1e-5)\n",
    "print(\"LayerNorm by hand: \\n\", y)\n",
    "print(\"LayerNorm: \\n\", x_norm[0,0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Featurewise MaxPooling\n",
    "\n",
    "For each feature, take the maximum value among the pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.zeros((1,14,14), dtype=int) # this is the structure of the state retrieved by the game\n",
    "if len(x.shape) <= 3:\n",
    "    x = x.unsqueeze(0)\n",
    "print(\"x.shape (before embed): \", x.shape)\n",
    "x = embed(x)\n",
    "print(\"x.shape (after embed): \", x.shape)\n",
    "print(\"x.sum in slices: \", sum_slices(x,(0,1)))\n",
    "x = x.transpose(-1,-3)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = x.transpose(-1,-2).reshape(x.shape[0],-1,x.shape[-2],x.shape[-1])\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = net(x)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x= add_encoding2D(x)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = x.view(x.shape[0], x.shape[1],-1)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = x.transpose(2,1)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = projection(x)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = x.transpose(1,0)\n",
    "print(\"x.shape: \", x.shape)\n",
    "\n",
    "x_tmp = x # save it for plotting\n",
    "\n",
    "# From here it has always the same shape\n",
    "attn_output, attn_output_weights =  attn(x,x,x, key_padding_mask=None) # MHA step\n",
    "print(\"attn_output.shape: \", attn_output.shape)\n",
    "x_add = attn_output + x\n",
    "print(\"x_add.shape: \", x_add.shape)\n",
    "x_norm = drop(norm(x_add))\n",
    "print(\"x_norm.shape: \", x_norm.shape)\n",
    "x_ff = ff(x_norm)\n",
    "print(\"x_ff.shape: \", x_ff.shape)\n",
    "x = drop(norm(x_ff))\n",
    "print(\"x.shape: \", x.shape)\n",
    "# Max pooling feature-wise\n",
    "x, _ = torch.max(x, axis=0)\n",
    "print(\"x.shape: \", x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing much to control here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternative to max pooling - Linear projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(rnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_proj = nn.Linear(144,1) # needs to know how many pixels there are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.zeros((1,1,14,14), dtype=int) # this is the structure of the state retrieved by the game\n",
    "if len(x.shape) <= 3:\n",
    "    x = x.unsqueeze(0)\n",
    "print(\"x.shape (before embed): \", x.shape)\n",
    "x = embed(x)\n",
    "print(\"x.shape (after embed): \", x.shape)\n",
    "print(\"x.sum in slices: \", sum_slices(x,(0,1)))\n",
    "x = x.transpose(-1,-3)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = x.transpose(-1,-2).reshape(x.shape[0],-1,x.shape[-2],x.shape[-1])\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = net(x)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x= add_encoding2D(x)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = x.view(x.shape[0], x.shape[1],-1)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = x.transpose(2,1)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = projection(x)\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = x.transpose(1,0)\n",
    "print(\"x.shape: \", x.shape)\n",
    "\n",
    "x_tmp = x # save it for plotting\n",
    "\n",
    "# From here it has always the same shape\n",
    "attn_output, attn_output_weights =  attn(x,x,x, key_padding_mask=None) # MHA step\n",
    "print(\"attn_output.shape: \", attn_output.shape)\n",
    "x_add = attn_output + x\n",
    "print(\"x_add.shape: \", x_add.shape)\n",
    "x_norm = drop(norm(x_add))\n",
    "print(\"x_norm.shape: \", x_norm.shape)\n",
    "x_ff = ff(x_norm)\n",
    "print(\"x_ff.shape: \", x_ff.shape)\n",
    "x = drop(norm(x_ff))\n",
    "print(\"x.shape: \", x.shape)\n",
    "\n",
    "# Feature-wise projection\n",
    "x = x.transpose(-1,0)\n",
    "print(\"x.shape (before linear): \", x.shape)\n",
    "shape = x.shape\n",
    "x = linear_proj(x).reshape(shape[0],shape[1])\n",
    "print(\"x.shape: \", x.shape)\n",
    "x = x.transpose(-1,0)\n",
    "print(\"x.shape: \", x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual Layer\n",
    "\n",
    "Here the original paper uses just a Multi-Layer Perceptron, but I thought it would be nice to have sone skip connections in order to make the architecture more flexible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(rnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hiddens = 256 \n",
    "residual_layer = rnet.ResidualLayer(n_features, n_hiddens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "residual_layer(x) - x # residual after ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'RelationalModule.RelationalNetworks' from '/home/nicola/Nicola_unipd/MasterThesis/RelationalDeepRL/RelationalModule/RelationalNetworks.py'>"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(rnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): RelationalModule(\n",
      "    (net): Sequential(\n",
      "      (0): PositionalEncoding(\n",
      "        (projection): Linear(in_features=26, out_features=256, bias=True)\n",
      "      )\n",
      "      (1): AttentionBlock(\n",
      "        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (ff): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=64, bias=True)\n",
      "          (w_2): Linear(in_features=64, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (2): AttentionBlock(\n",
      "        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (ff): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=64, bias=True)\n",
      "          (w_2): Linear(in_features=64, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (1): FeaturewiseMaxPool()\n",
      "  (2): ResidualLayer(\n",
      "    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (w1): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (w2): Linear(in_features=256, out_features=256, bias=True)\n",
      "  )\n",
      "  (3): ResidualLayer(\n",
      "    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (w1): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (w2): Linear(in_features=256, out_features=256, bias=True)\n",
      "  )\n",
      "  (4): ResidualLayer(\n",
      "    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (w1): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (w2): Linear(in_features=256, out_features=256, bias=True)\n",
      "  )\n",
      "  (5): ResidualLayer(\n",
      "    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (w1): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (w2): Linear(in_features=256, out_features=256, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "box_net = rnet.BoxWorldNet(in_channels=3, n_kernels=24, vocab_size=6, n_dim=3,\n",
    "                              n_features=256, n_attn_modules=2, n_linears=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): RelationalModule(\n",
      "    (net): Sequential(\n",
      "      (0): PositionalEncoding(\n",
      "        (projection): Linear(in_features=26, out_features=256, bias=True)\n",
      "      )\n",
      "      (1): AttentionBlock(\n",
      "        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (ff): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=64, bias=True)\n",
      "          (w_2): Linear(in_features=64, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (2): AttentionBlock(\n",
      "        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (ff): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=64, bias=True)\n",
      "          (w_2): Linear(in_features=64, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (1): FeaturewiseProjection(\n",
      "    (norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "    (projection): Linear(in_features=144, out_features=1, bias=True)\n",
      "  )\n",
      "  (2): ResidualLayer(\n",
      "    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (w1): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (w2): Linear(in_features=256, out_features=256, bias=True)\n",
      "  )\n",
      "  (3): ResidualLayer(\n",
      "    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (w1): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (w2): Linear(in_features=256, out_features=256, bias=True)\n",
      "  )\n",
      "  (4): ResidualLayer(\n",
      "    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (w1): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (w2): Linear(in_features=256, out_features=256, bias=True)\n",
      "  )\n",
      "  (5): ResidualLayer(\n",
      "    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (w1): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (w2): Linear(in_features=256, out_features=256, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "box_net_v1 = rnet.BoxWorldNet(in_channels=3, n_kernels=24, vocab_size=6, n_dim=3,\n",
    "                              n_features=256, n_attn_modules=2, n_linears=4, max_pool=False,\n",
    "                              linear_size=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([14, 14, 1])\n",
      "torch.Size([14, 14, 3])\n"
     ]
    }
   ],
   "source": [
    "x1 = torch.LongTensor(state[0])\n",
    "print(x1.shape)\n",
    "x2 = torch.tensor(state[1]).float()\n",
    "print(x2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1.shape (before ExtractEntities):  torch.Size([14, 14, 1])\n",
      "x2.shape (before ExtractEntities):  torch.Size([14, 14, 3])\n",
      "x.shape (after Embedding and reshape):  torch.Size([1, 6, 14, 14])\n",
      "x.shape (ExtractEntities):  torch.Size([1, 24, 12, 12])\n",
      "x.shape (After encoding):  torch.Size([1, 26, 12, 12])\n",
      "x.shape (Before transposing and projection):  torch.Size([1, 26, 144])\n",
      "x.shape (PositionalEncoding):  torch.Size([144, 1, 256])\n",
      "x.shape (RelationalModule):  torch.Size([144, 1, 256])\n",
      "x.shape (FeaturewiseProjection):  torch.Size([144, 1, 256])\n",
      "x.shape (FeaturewiseProjection):  torch.Size([256, 1, 144])\n",
      "x.shape (FeaturewiseProjection):  torch.Size([256, 1])\n",
      "x.shape (FeaturewiseProjection):  torch.Size([1, 256])\n",
      "x.shape (BoxWorldNet):  torch.Size([1, 256])\n",
      "y.shape:  torch.Size([1, 256])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0594,  1.1729,  0.9001,  0.4614, -1.3175,  0.0731,  0.9347, -0.9794,\n",
       "         -1.5173, -0.7390, -0.9998,  2.4710, -1.1093, -0.6915, -3.4846,  2.2579,\n",
       "         -1.9974, -0.5731, -0.4970, -0.1387,  1.4172,  0.6031,  0.1381,  1.5487,\n",
       "          0.4061, -0.1736, -0.5689, -0.6867, -1.1547,  0.6612, -0.0434,  0.1895,\n",
       "         -0.1609,  0.4308,  2.0075,  0.1477,  1.8875,  1.4404, -0.5629, -0.2460,\n",
       "          1.6132, -1.5495,  1.3293, -1.8583, -0.4135,  0.2229,  0.1548,  2.1788,\n",
       "         -0.4053,  1.1293,  0.6658,  0.8162,  3.4881,  0.3799,  2.8879,  0.2408,\n",
       "         -0.2668, -0.1129,  0.8771,  0.1817,  1.6333,  0.5355,  0.5702, -0.9400,\n",
       "         -0.8138, -0.2316,  0.2152, -2.4184, -1.0042, -0.3276, -0.2822,  1.0378,\n",
       "          1.4559, -0.2632,  2.6049,  1.5024,  1.4799, -1.2181,  0.9351, -1.7353,\n",
       "         -1.2603, -0.4981,  0.5729, -0.5147, -1.8516,  0.3851,  0.4077, -1.3375,\n",
       "          1.4091,  1.5234, -0.9242,  0.9547, -0.4658,  0.6823, -0.6142, -1.1469,\n",
       "         -1.4122,  0.5281,  1.3724, -1.7548, -0.2391, -0.8516,  1.8060, -0.1137,\n",
       "         -1.3639, -0.6103, -1.1044, -0.4112,  0.6626,  2.4084,  0.3815, -0.3091,\n",
       "          1.5008,  0.8576, -0.7875, -1.0062, -0.5292,  0.9780,  1.4716,  0.4395,\n",
       "         -0.4712,  1.3198, -0.3525, -0.9911,  0.9307, -1.2999,  0.8449, -1.2089,\n",
       "         -0.8410, -0.5941, -0.0397,  2.0397,  1.0871, -1.7894,  0.8869,  0.4383,\n",
       "         -0.1737,  1.1713,  0.8410,  0.9965,  0.3011, -2.1499,  0.2293,  0.6582,\n",
       "          0.8354,  0.6355,  0.5543,  0.0048, -0.3656,  0.2970,  0.7668, -1.9987,\n",
       "          0.0722, -0.4991,  1.5351,  0.4473,  0.0422,  0.0940, -1.8532, -1.4366,\n",
       "         -0.4296, -1.6004, -1.3151,  0.4036,  1.7561, -1.2755,  0.6525,  0.2279,\n",
       "         -0.1960, -0.3997,  0.5094,  0.7819,  0.4383,  0.5715,  0.5258, -0.3637,\n",
       "         -0.1222, -1.5624, -0.3792, -1.4540,  1.4918, -0.5330,  0.5449,  1.3594,\n",
       "          1.0556, -1.0754, -0.5877,  1.1049, -0.4777,  1.2080, -0.1692,  0.1146,\n",
       "         -2.6222, -0.4799, -1.5950, -0.2370, -0.8800,  1.4404,  1.2603,  0.8302,\n",
       "          0.1233,  0.1661,  0.0079, -0.3477,  0.7621,  0.3748,  2.6161, -0.2069,\n",
       "          0.4668, -1.4139, -1.2182,  1.1051,  0.5597, -2.0778,  1.4008, -0.2809,\n",
       "         -0.0312, -1.7374, -0.6511,  0.7491,  0.8382,  2.6533,  0.8734, -1.0396,\n",
       "         -1.0296, -0.7004, -2.3923,  0.7202,  0.0964,  1.2664, -0.0616,  0.8752,\n",
       "         -0.1115, -1.0939, -1.9377,  1.2493,  0.8829, -1.4044, -0.5779, -1.6190,\n",
       "         -0.1982, -0.6651, -0.4949,  2.7224, -0.4127,  0.8885, -0.6328, -1.8119,\n",
       "          0.7735, -1.6129,  1.9822, -0.5384,  1.0313,  0.0839,  0.0766,  1.3489]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = box_net_v1((x1, x2))\n",
    "print(\"y.shape: \", y.shape)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
