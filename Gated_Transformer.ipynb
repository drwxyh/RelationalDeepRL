{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gated transformer\n",
    "\n",
    "<img src='Supplementary material/gated_transformer.png'>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    Applies 2 linear layers with ReLU and dropout layers\n",
    "    only after the first layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(F.relu(self.w_1(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Original Transformer Block \n",
    "\n",
    "This is the original Transformer block, with the NormLayer applied after the summation of the input with the output of the submodule. Notice that my implementation did not have the skip connection also for the second layer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, n_features, n_heads, n_hidden=64, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          n_features: Number of input and output features. (d_model)\n",
    "          n_heads: Number of attention heads in the Multi-Head Attention.\n",
    "          n_hidden: Number of hidden units in the Feedforward (MLP) block. (d_k)\n",
    "          dropout: Dropout rate after the first layer of the MLP and the two skip connections.\n",
    "        \"\"\"\n",
    "        super(AttentionBlock, self).__init__()\n",
    "        self.norm = nn.LayerNorm(n_features)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.attn = nn.MultiheadAttention(n_features, n_heads, dropout)\n",
    "        self.ff = PositionwiseFeedForward(n_features, n_hidden, dropout)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          x of shape (n_pixels**2, batch_size, n_features): Input sequences.\n",
    "          mask of shape (batch_size, max_seq_length): Boolean tensor indicating which elements of the input\n",
    "              sequences should be ignored.\n",
    "        \n",
    "        Returns:\n",
    "          z of shape (max_seq_length, batch_size, n_features): Encoded input sequence.\n",
    "\n",
    "        Note: All intermediate signals should be of shape (n_pixels**2, batch_size, n_features).\n",
    "        \"\"\"\n",
    "\n",
    "        attn_output, attn_output_weights = self.attn(x,x,x, key_padding_mask=mask) # MHA step\n",
    "        x_norm = self.dropout(self.norm(attn_output + x)) # add and norm\n",
    "        z = self.ff(x_norm) # FF step\n",
    "        return self.dropout(self.norm(z)) # add and norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 32\n",
    "n_heads = 4\n",
    "\n",
    "Tr = AttentionBlock(n_features, n_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(10, 1, n_features)\n",
    "\n",
    "y = TrI(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Block with Identity Map Reordering \n",
    "\n",
    "Now this one is the one that applies the LayerNorm only to the input of the submodule, but leaving untached the initial signal, so that the identity transformation can be easily implemented.\n",
    "\n",
    "\"Because the layer norm reordering causes a path where 2 linear layers are applied in sequence, we apply a ReLU activation to each sub-module output before the residual connection\" (not 100% sure of which is the path and why the addition of just a single ReLU doesn't solve the problem, but 2 are required; not a big deal anyway)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerIdentityBlock(nn.Module):\n",
    "    def __init__(self, n_features, n_heads, n_hidden=64, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          n_features: Number of input and output features. (d_model)\n",
    "          n_heads: Number of attention heads in the Multi-Head Attention.\n",
    "          n_hidden: Number of hidden units in the Feedforward (MLP) block. (d_k)\n",
    "          dropout: Dropout rate after the first layer of the MLP and the two skip connections.\n",
    "        \"\"\"\n",
    "        super(TransformerIdentityBlock, self).__init__()\n",
    "        self.norm = nn.LayerNorm(n_features)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.attn = nn.MultiheadAttention(n_features, n_heads, dropout)\n",
    "        self.ff = PositionwiseFeedForward(n_features, n_hidden, dropout)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          x of shape (n_pixels**2, batch_size, n_features): Input sequences.\n",
    "          mask of shape (batch_size, max_seq_length): Boolean tensor indicating which elements of the input\n",
    "              sequences should be ignored.\n",
    "        \n",
    "        Returns:\n",
    "          z of shape (max_seq_length, batch_size, n_features): Encoded input sequence.\n",
    "\n",
    "        Note: All intermediate signals should be of shape (n_pixels**2, batch_size, n_features).\n",
    "        \"\"\"\n",
    "        \n",
    "        # First submodule\n",
    "        x_norm = self.norm(x) # LayerNorm to the input before entering submodule\n",
    "        attn_output, attn_output_weights = self.attn(x_norm, x_norm, x_norm, key_padding_mask=mask) # MHA step\n",
    "        x = self.dropout(F.relu(attn_output) + x) # skip connection added\n",
    "        \n",
    "        # Second submodule\n",
    "        x_norm = self.norm(x) # LayerNorm to the input before entering submodule\n",
    "        z = F.relu(self.ff(x_norm)) # FF step\n",
    "        return self.dropout(z+x) # skip connection added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 32\n",
    "n_heads = 4\n",
    "\n",
    "TrI = TransformerIdentityBlock(n_features, n_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(10, 1, n_features)\n",
    "\n",
    "y = TrI(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gated Transformer with IMR \n",
    "\n",
    "\n",
    "<img src='Supplementary material/GRU_gating.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = True\n",
    "\n",
    "class GRU_gating(nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super(GRU_gating, self).__init__()\n",
    "        self.Wr = nn.Linear(n_features*2, n_features, bias=False)\n",
    "        self.Wz = nn.Linear(n_features*2, n_features, bias=True)\n",
    "        self.Wg = nn.Linear(n_features*2, n_features, bias=False)\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        xy = torch.cat([x, y], axis=-1)\n",
    "        if debug: print(\"xy.shape: \", xy.shape)\n",
    "            \n",
    "        r = torch.sigmoid(self.Wr(xy))\n",
    "        if debug: print(\"r.shape: \", r.shape)\n",
    "            \n",
    "        z = torch.sigmoid(self.Wz(xy))\n",
    "        if debug: print(\"z.shape: \", z.shape)\n",
    "            \n",
    "        rx = r*x\n",
    "        if debug: print(\"rx.shape: \", rx.shape)\n",
    "            \n",
    "        h = torch.tanh(self.Wg(torch.cat([rx, y], axis=-1)))\n",
    "        if debug: print(\"h.shape: \", h.shape)\n",
    "            \n",
    "        g = (1-z)*x + z*h\n",
    "        if debug: print(\"g.shape: \", g.shape)\n",
    "            \n",
    "        return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape:  torch.Size([1, 10])\n",
      "y.shape:  torch.Size([1, 10])\n",
      "xy.shape:  torch.Size([1, 20])\n",
      "r.shape:  torch.Size([1, 10])\n",
      "z.shape:  torch.Size([1, 10])\n",
      "rx.shape:  torch.Size([1, 10])\n",
      "h.shape:  torch.Size([1, 10])\n",
      "g.shape:  torch.Size([1, 10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.6947, 0.6898, 0.5084, 0.3647, 0.5615, 0.4226, 0.2960, 0.5226, 0.7250,\n",
       "         0.2281]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_features = 10\n",
    "GRU_gate = GRU_gating(n_features)\n",
    "\n",
    "x = torch.ones(1, n_features)\n",
    "print(\"x.shape: \", x.shape)\n",
    "\n",
    "y = torch.rand(1, n_features)\n",
    "print(\"y.shape: \", y.shape)\n",
    "\n",
    "g = GRU_gate(x,y)\n",
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GatedTransformerBlock(nn.Module):\n",
    "    def __init__(self, n_features, n_heads, n_hidden=64, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          n_features: Number of input and output features. (d_model)\n",
    "          n_heads: Number of attention heads in the Multi-Head Attention.\n",
    "          n_hidden: Number of hidden units in the Feedforward (MLP) block. (d_k)\n",
    "          dropout: Dropout rate after the first layer of the MLP and the two skip connections.\n",
    "        \"\"\"\n",
    "        super(GatedTransformerBlock, self).__init__()\n",
    "        self.norm = nn.LayerNorm(n_features)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.attn = nn.MultiheadAttention(n_features, n_heads, dropout)\n",
    "        self.GRU_gate1 = GRU_gating(n_features)\n",
    "        self.ff = PositionwiseFeedForward(n_features, n_hidden, dropout)\n",
    "        self.GRU_gate2 = GRU_gating(n_features)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          x of shape (n_pixels**2, batch_size, n_features): Input sequences.\n",
    "          mask of shape (batch_size, max_seq_length): Boolean tensor indicating which elements of the input\n",
    "              sequences should be ignored.\n",
    "        \n",
    "        Returns:\n",
    "          z of shape (max_seq_length, batch_size, n_features): Encoded input sequence.\n",
    "\n",
    "        Note: All intermediate signals should be of shape (n_pixels**2, batch_size, n_features).\n",
    "        \"\"\"\n",
    "        \n",
    "        # First submodule\n",
    "        x_norm = self.norm(x) # LayerNorm to the input before entering submodule\n",
    "        attn_output, attn_output_weights = self.attn(x_norm, x_norm, x_norm, key_padding_mask=mask) # MHA step\n",
    "        x = self.dropout(self.GRU_gate1(x, attn_output)) # skip connection added\n",
    "        \n",
    "        # Second submodule\n",
    "        x_norm = self.norm(x) # LayerNorm to the input before entering submodule\n",
    "        z = self.ff(x_norm) # FF step\n",
    "        return self.dropout(self.GRU_gate2(x, z)) # skip connection added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 32\n",
    "n_heads = 4\n",
    "\n",
    "GTr = GatedTransformerBlock(n_features, n_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xy.shape:  torch.Size([10, 1, 64])\n",
      "r.shape:  torch.Size([10, 1, 32])\n",
      "z.shape:  torch.Size([10, 1, 32])\n",
      "rx.shape:  torch.Size([10, 1, 32])\n",
      "h.shape:  torch.Size([10, 1, 32])\n",
      "g.shape:  torch.Size([10, 1, 32])\n",
      "xy.shape:  torch.Size([10, 1, 64])\n",
      "r.shape:  torch.Size([10, 1, 32])\n",
      "z.shape:  torch.Size([10, 1, 32])\n",
      "rx.shape:  torch.Size([10, 1, 32])\n",
      "h.shape:  torch.Size([10, 1, 32])\n",
      "g.shape:  torch.Size([10, 1, 32])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 3.2434e-01,  1.9966e-02,  2.5949e-01,  7.8418e-02,  3.7675e-01,\n",
       "           7.6138e-02,  4.2284e-01,  0.0000e+00,  4.6148e-01,  3.7017e-01,\n",
       "           0.0000e+00,  2.4634e-01,  4.0325e-01,  2.3206e-01,  3.2800e-01,\n",
       "           2.6801e-01,  3.6624e-01,  3.2993e-01,  3.5932e-01,  0.0000e+00,\n",
       "           2.7202e-01,  2.0279e-01,  2.0578e-01,  2.8484e-01,  3.7115e-02,\n",
       "           3.7038e-01,  2.2633e-01,  4.1369e-01,  2.7697e-01,  1.1816e-01,\n",
       "           4.0359e-01,  2.6289e-01]],\n",
       "\n",
       "        [[ 3.6654e-01,  1.1305e-01,  0.0000e+00,  1.8434e-01,  4.7564e-01,\n",
       "           8.2172e-02,  4.4056e-01,  3.9164e-01,  0.0000e+00,  3.8793e-01,\n",
       "           4.6810e-01,  2.1911e-01,  4.4695e-01,  1.3583e-01,  3.7940e-01,\n",
       "           2.2348e-01,  4.3927e-01,  1.9926e-01,  2.9011e-01,  3.1536e-01,\n",
       "           1.8956e-01,  2.2109e-01,  1.4870e-01,  3.7345e-01,  3.3986e-01,\n",
       "           3.4751e-01,  3.5846e-01,  3.6325e-01,  3.1484e-01, -4.1760e-02,\n",
       "           4.7626e-01,  2.5629e-01]],\n",
       "\n",
       "        [[ 3.9131e-01,  9.9254e-02,  2.9460e-01,  1.2068e-01,  4.1197e-01,\n",
       "           1.3925e-01,  4.2575e-01,  4.2210e-01,  4.8953e-01,  3.5234e-01,\n",
       "           4.5619e-01,  3.0051e-01,  3.1176e-01,  3.0840e-01,  3.9713e-01,\n",
       "           1.7971e-01,  0.0000e+00,  3.0040e-01,  3.3964e-01,  0.0000e+00,\n",
       "           1.9918e-01,  2.3622e-01,  1.2385e-01,  3.0705e-01,  3.2184e-01,\n",
       "           3.3374e-01,  3.4647e-01,  7.0852e-02,  2.7629e-01,  1.6197e-01,\n",
       "           4.6952e-01, -2.9461e-04]],\n",
       "\n",
       "        [[ 5.9959e-02,  8.7894e-02,  2.8237e-01,  6.6255e-02,  3.7899e-01,\n",
       "           1.8909e-01,  3.4110e-01,  3.8700e-01,  4.3458e-01,  0.0000e+00,\n",
       "           1.1474e-01,  2.0355e-01,  4.2142e-01,  1.0212e-01,  0.0000e+00,\n",
       "           1.5318e-01,  5.9117e-02,  1.5721e-01,  3.5224e-01,  2.9949e-01,\n",
       "           2.3916e-01,  2.3444e-01,  1.6836e-01,  2.2860e-02,  3.3897e-01,\n",
       "           4.0303e-01,  1.9489e-01,  4.1998e-01,  4.3375e-01, -1.6642e-01,\n",
       "           3.3880e-01,  1.7584e-02]],\n",
       "\n",
       "        [[ 4.0181e-01,  1.5684e-01,  3.9306e-01,  1.7628e-01,  4.0921e-01,\n",
       "           9.9194e-02,  4.2779e-01,  4.2915e-01,  5.0107e-01,  3.3396e-01,\n",
       "           4.9303e-01,  2.0918e-01,  8.5773e-02,  1.9635e-01,  4.1375e-01,\n",
       "           1.8285e-01,  4.2106e-01,  2.8363e-01,  2.7012e-01,  0.0000e+00,\n",
       "           2.2930e-01,  2.7873e-01,  0.0000e+00,  3.3739e-01,  2.8918e-01,\n",
       "           3.9300e-01,  3.7624e-01,  3.5449e-01,  0.0000e+00, -5.7506e-02,\n",
       "           4.5807e-01,  2.7654e-01]],\n",
       "\n",
       "        [[ 3.9733e-01,  1.5602e-01,  3.8033e-01,  1.3146e-01,  4.2990e-01,\n",
       "           1.1629e-01,  3.9684e-01,  3.7606e-01,  4.9327e-01,  3.2478e-01,\n",
       "           1.1900e-01,  2.0922e-01,  1.2692e-01,  7.9856e-02,  3.9127e-01,\n",
       "           0.0000e+00,  4.5575e-01,  1.8869e-01,  2.6398e-01,  3.4722e-01,\n",
       "           2.0709e-01,  1.9797e-01,  1.9658e-01,  3.3561e-01,  3.5392e-01,\n",
       "           3.6567e-01,  3.0741e-01,  3.2586e-01,  3.7020e-01, -4.0165e-02,\n",
       "           4.7775e-01,  3.2523e-01]],\n",
       "\n",
       "        [[ 0.0000e+00, -3.4528e-03,  2.9859e-01,  0.0000e+00,  4.2970e-01,\n",
       "           7.6602e-02,  0.0000e+00,  3.2770e-01,  3.6459e-01,  3.2842e-01,\n",
       "           4.6496e-01,  2.4678e-01, -0.0000e+00,  0.0000e+00,  4.2429e-01,\n",
       "           1.4014e-01,  3.4136e-01, -3.2025e-02,  7.0090e-02,  2.5488e-01,\n",
       "           1.7981e-01,  3.1511e-01,  1.6971e-01,  3.3557e-01,  2.9916e-01,\n",
       "           2.8641e-01,  4.3572e-01,  3.7034e-01, -7.1491e-02,  1.8884e-01,\n",
       "           3.7853e-01, -1.8697e-02]],\n",
       "\n",
       "        [[ 3.3198e-01, -1.3154e-02,  3.7025e-01,  8.2973e-02,  4.9308e-01,\n",
       "           9.8878e-02,  3.8837e-01,  1.3483e-02,  4.6748e-01,  4.0209e-01,\n",
       "           5.2982e-01,  1.8807e-01,  4.7722e-01,  1.9387e-01,  3.3880e-01,\n",
       "           2.5733e-01,  3.8068e-01,  3.3539e-01,  2.9498e-01,  3.5405e-01,\n",
       "           2.9168e-01,  0.0000e+00,  2.6447e-01,  3.2413e-01,  0.0000e+00,\n",
       "           4.2849e-01,  0.0000e+00,  4.2260e-01,  3.2181e-01,  2.0637e-01,\n",
       "           4.7841e-01,  2.2296e-01]],\n",
       "\n",
       "        [[ 2.9608e-01,  1.1467e-01,  3.4861e-01, -4.1313e-02,  3.9657e-02,\n",
       "           1.0232e-01, -4.5869e-03,  4.1984e-01,  5.0710e-01,  3.4233e-01,\n",
       "           4.4875e-02,  5.6855e-02,  4.1163e-01,  2.2793e-01,  0.0000e+00,\n",
       "           1.8199e-01,  0.0000e+00,  2.7252e-01,  3.0131e-01,  3.9010e-01,\n",
       "           2.2302e-01, -7.7358e-02,  1.5771e-01,  3.3160e-01,  2.9994e-01,\n",
       "           4.3466e-01,  2.4920e-01,  4.2092e-01,  3.3098e-01,  1.9998e-01,\n",
       "           3.9244e-01,  2.9784e-01]],\n",
       "\n",
       "        [[ 3.3842e-01,  6.2112e-02,  2.2883e-01,  1.8618e-01,  3.8179e-01,\n",
       "           8.5269e-02,  3.6840e-01,  0.0000e+00,  4.1324e-01,  3.0447e-01,\n",
       "           0.0000e+00,  2.1198e-01,  4.4650e-01,  2.1179e-01,  3.1586e-01,\n",
       "           1.7956e-01, -5.9851e-02, -1.5420e-01,  3.9449e-01,  3.9934e-01,\n",
       "           2.4536e-01, -2.3921e-02,  1.9896e-01,  2.6051e-01,  3.7069e-01,\n",
       "           4.0915e-01,  1.9804e-01,  3.8527e-01,  4.1303e-01,  1.6309e-01,\n",
       "          -0.0000e+00,  6.5087e-03]]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.ones(10, 1, n_features)\n",
    "y = GTr(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
